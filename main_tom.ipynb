{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions used in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import time\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Windy gridworld\n",
    "Gives a reward of -1 for each step taken, while the final state is not reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "env = WindyGridworldEnv()\n",
    "env??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "### Target policy (choose greedy vs non-greedy)\n",
    "Greedy policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def get_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        Takes a state and an action and returns the probability of taking that action from \n",
    "        that state, under Q and a greedy policy\n",
    "        \"\"\"   \n",
    "        # for state and action only:\n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        \n",
    "        if action in max_indices:\n",
    "            prob = 1/len(max_indices)\n",
    "        else:\n",
    "            prob = 0\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "\n",
    "        best_actions = [i for i, j in enumerate([self.Q[obs][i] for i in range(4)]) \n",
    "                   if j == max([self.Q[obs][i] for i in range(4)])] \n",
    "\n",
    "        best_action = np.random.choice(best_actions)\n",
    "        \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def get_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        Takes a state and an action and returns the probability of taking that action from \n",
    "        that state, under Q and a epsilon greedy policy\n",
    "        \"\"\"\n",
    "\n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        \n",
    "        # all probs are equal, give all equal probabilities\n",
    "        if len(max_indices) == len(action_probs):\n",
    "            return 1/len(max_indices)\n",
    "            \n",
    "        if action in max_indices:\n",
    "            prob = (1-self.epsilon)/len(max_indices)\n",
    "        else:\n",
    "            prob = epsilon / (len(action_probs) - len(max_indices))\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "\n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "         \n",
    "        best_actions = [i for i, j in enumerate(self.Q[obs])\n",
    "                   if j == np.max(self.Q[obs])] \n",
    "\n",
    "        p = np.random.uniform()\n",
    "        if p > self.epsilon:\n",
    "            # choose one of the best actions\n",
    "            action = np.random.choice(best_actions)\n",
    "        else:\n",
    "            # return a random action\n",
    "            action = np.random.randint(0,4)\n",
    "                \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function given an env and policy\n",
    "Function to sample an episode from the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n",
    "        state after the termination is not included in the list of states.\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    # get a starting state\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    \n",
    "    # keep looping until done, don's save the terminal state\n",
    "    while not d:\n",
    "        states.append(s)\n",
    "        a = policy.sample_action(s)\n",
    "        s, r, d, _ = env.step(a)\n",
    "        \n",
    "        # save                \n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(d)\n",
    "        \n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 3311\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for eps greedy policy\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = EpsilonGreedyPolicy(Q, epsilon=0.1)\n",
    "\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, bp)\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Importance Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_importance_sampling(env, behavior_policy, target_policy, num_episodes, weighted=False, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05, seed=42):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the Q function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        weighted: Boolean flag to use weighted or ordinary importance sampling.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from (state, action) -> value.\n",
    "    \"\"\"\n",
    "\n",
    "    # set the current Q to a large negative value\n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    if weighted:\n",
    "        C = np.zeros((env.nS, env.nA))\n",
    "    else:\n",
    "        returns_count = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral and target policy\n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q        \n",
    "    \n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save the episode length\n",
    "        episode_lens.append(len(states)) \n",
    "\n",
    "        G = 0        \n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)):\n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            if weighted:\n",
    "                # add W to the sum of weights C\n",
    "                C[s][a] += W\n",
    "                Q[s][a] += W/C[s][a] * (G - Q[s][a])\n",
    "            else:\n",
    "                returns_count[s][a] += 1 \n",
    "                # use every visit incremental method\n",
    "                Q[s][a] += 1/returns_count[s][a] * (W * G - Q[s][a])\n",
    "            \n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))        \n",
    "\n",
    "            if W == 0:\n",
    "                break\n",
    "        \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Plot the episode length over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (1000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 326.66it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using weighted importance sampling (1000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 345.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "seed = 10\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.1\n",
    "gamma = 1.0\n",
    "num_episodes = 1000\n",
    "Q = np.ones((env.nS, env.nA)) * -100\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_ordinary, mc_ordinary_epslengths = mc_importance_sampling(env, \n",
    "                                                               behavioral_policy, target_policy, \n",
    "                                                               num_episodes, weighted=False,discount_factor=gamma, \n",
    "                                                               epsilon=epsilon, seed=seed)\n",
    "print(f\"Updating Q using weighted importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_weighted, mc_weighted_epslengths = mc_importance_sampling(env, \n",
    "                                                               behavioral_policy, target_policy,\n",
    "                                                               num_episodes, weighted=True, discount_factor=gamma, \n",
    "                                                               epsilon=epsilon, seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting episode length ordinary: 16\n",
      "resulting episode length weighted: 17\n"
     ]
    }
   ],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_mc_ordinary)\n",
    "greedy_weighted = GreedyPolicy(Q_mc_weighted)\n",
    "\n",
    "ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "weighted_episode = sample_episode(env, greedy_weighted)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])}\")\n",
    "print(f\"resulting episode length weighted: {len(weighted_episode[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting episode lengths during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6870, 1863, 128, 93, 28, 78, 20, 84, 60, 28, 23, 23, 20, 27, 24, 31, 27, 35, 20, 23, 18, 30, 28, 30, 22, 24, 126, 20, 39, 21, 25, 33, 22, 24, 41, 22, 19, 32, 19, 19, 27, 20, 98, 25, 23, 41, 29, 41, 58, 24, 91, 21, 26, 47, 50, 51, 19, 28, 38, 18, 27, 27, 46, 21, 24, 37, 19, 17, 17, 20, 20, 24, 18, 17, 17, 19, 23, 17, 19, 18, 18, 17, 20, 20, 17, 20, 20, 25, 18, 17, 17, 23, 17, 18, 19, 18, 18, 30, 19, 25, 17, 18, 19, 18, 27, 21, 27, 25, 18, 20, 19, 29, 18, 17, 17, 20, 18, 17, 18, 17, 18, 19, 17, 19, 20, 21, 18, 17, 21, 17, 17, 19, 17, 16, 17, 18, 17, 21, 18, 18, 18, 17, 22, 34, 24, 16, 18, 16, 18, 16, 18, 21, 18, 17, 19, 17, 16, 23, 18, 16, 17, 19, 19, 18, 19, 16, 16, 16, 22, 21, 21, 21, 17, 22, 17, 16, 20, 18, 22, 16, 21, 16, 28, 19, 16, 19, 16, 17, 19, 16, 16, 22, 19, 17, 16, 17, 20, 19, 17, 20, 20, 21, 16, 18, 16, 23, 18, 20, 16, 17, 18, 17, 16, 16, 16, 16, 21, 17, 19, 16, 16, 16, 22, 16, 21, 16, 19, 16, 16, 19, 20, 16, 16, 20, 19, 29, 20, 22, 18, 16, 16, 16, 16, 16, 17, 20, 16, 17, 17, 18, 16, 21, 18, 19, 17, 18, 29, 20, 19, 16, 19, 16, 20, 19, 19, 27, 16, 17, 21, 16, 16, 21, 21, 16, 16, 17, 22, 18, 23, 25, 18, 24, 21, 16, 19, 16, 23, 20, 16, 18, 18, 16, 19, 17, 18, 16, 18, 20, 16, 20, 21, 16, 18, 16, 17, 20, 16, 18, 16, 20, 22, 16, 16, 22, 17, 23, 17, 16, 17, 24, 17, 16, 20, 17, 18, 17, 16, 19, 16, 16, 19, 22, 16, 17, 16, 17, 16, 19, 21, 19, 26, 17, 22, 16, 16, 19, 17, 17, 17, 20, 22, 19, 16, 19, 17, 17, 18, 17, 20, 16, 18, 16, 19, 17, 16, 16, 18, 16, 16, 16, 16, 23, 20, 16, 17, 18, 17, 22, 16, 18, 18, 18, 17, 20, 18, 16, 19, 16, 19, 23, 16, 20, 16, 22, 18, 16, 19, 17, 18, 18, 16, 18, 16, 18, 18, 16, 19, 19, 17, 20, 17, 17, 17, 18, 16, 18, 21, 19, 18, 17, 19, 19, 16, 17, 19, 18, 18, 21, 18, 18, 18, 20, 16, 16, 16, 21, 32, 16, 17, 17, 18, 16, 17, 17, 17, 16, 18, 17, 19, 16, 16, 16, 19, 19, 19, 16, 17, 16, 20, 20, 18, 16, 18, 16, 19, 19, 16, 18, 21, 21, 19, 19, 17, 20, 23, 22, 20, 20, 18, 19, 17, 19, 16, 20, 18, 19, 16, 19, 16, 16, 18, 16, 20, 25, 16, 19, 20, 16, 18, 17, 21, 20, 19, 21, 16, 17, 18, 17, 20, 16, 18, 20, 21, 16, 18, 18, 22, 21, 25, 21, 17, 21, 16, 18, 29, 16, 20, 16, 23, 17, 18, 22, 17, 16, 16, 18, 18, 24, 19, 22, 21, 18, 16, 20, 17, 22, 21, 18, 16, 17, 20, 18, 21, 16, 20, 16, 18, 17, 16, 17, 18, 16, 16, 20, 16, 24, 24, 18, 23, 18, 16, 16, 16, 19, 19, 16, 17, 21, 21, 18, 20, 17, 18, 24, 20, 18, 17, 20, 16, 18, 22, 18, 16, 16, 16, 26, 16, 18, 16, 20, 18, 16, 18, 16, 16, 16, 18, 19, 19, 18, 25, 16, 18, 18, 16, 16, 16, 17, 16, 16, 16, 19, 17, 16, 19, 16, 21, 21, 27, 21, 20, 18, 23, 18, 26, 19, 16, 19, 18, 18, 16, 20, 21, 16, 17, 16, 16, 18, 19, 18, 17, 19, 17, 18, 18, 23, 17, 17, 18, 16, 29, 16, 19, 21, 17, 16, 19, 18, 20, 27, 18, 16, 22, 18, 16, 16, 16, 16, 17, 16, 16, 19, 22, 21, 16, 16, 18, 21, 19, 16, 16, 16, 19, 16, 17, 20, 17, 16, 24, 16, 17, 22, 18, 21, 17, 21, 16, 18, 18, 20, 19, 16, 20, 22, 16, 26, 16, 16, 17, 16, 21, 17, 16, 22, 19, 18, 16, 17, 17, 31, 16, 17, 22, 20, 19, 16, 22, 18, 19, 19, 16, 19, 16, 18, 16, 16, 19, 19, 16, 16, 16, 20, 16, 16, 17, 17, 20, 25, 16, 21, 28, 16, 17, 23, 18, 19, 19, 19, 16, 17, 18, 18, 17, 16, 27, 17, 16, 17, 16, 18, 16, 21, 18, 21, 18, 27, 19, 17, 16, 18, 16, 22, 18, 16, 17, 18, 16, 16, 16, 17, 17, 26, 16, 17, 17, 21, 22, 17, 16, 16, 21, 19, 16, 17, 19, 17, 16, 16, 18, 17, 18, 19, 16, 16, 20, 16, 18, 16, 16, 16, 22, 18, 16, 17, 19, 16, 18, 20, 33, 23, 16, 19, 17, 16, 18, 23, 19, 17, 16, 16, 20, 20, 19, 23, 16, 19, 20, 16, 19, 16, 17, 24, 18, 19, 20, 19, 20, 16, 19, 16, 16, 19, 16, 18, 17, 19, 18, 20, 19, 16, 17, 16, 16, 18, 16, 16, 18, 18, 16, 19, 16, 16, 26, 16, 19, 19, 21, 18, 16, 16, 21, 19, 18, 16, 20, 19, 21, 19, 18, 17, 26, 21, 29, 20, 16, 16, 18, 18, 18, 28, 17, 22, 17, 19, 16, 17, 18, 20, 18, 16, 18, 16, 16, 16, 16, 29, 18, 16, 16, 19, 22, 21, 19, 16, 19, 17, 19, 24, 19, 18, 18, 16, 19, 26, 16, 16, 18, 18, 16, 17, 18, 16, 20, 32, 16, 18, 16, 20, 21, 20, 16, 22, 16, 16, 18, 18, 17, 16, 16, 22, 19, 17, 26, 19, 35, 28, 19, 19, 17, 16, 23, 16, 18, 20, 17, 16, 16, 17, 17, 16]\n",
      "[6870, 1863, 128, 93, 28, 78, 20, 27, 31, 30, 33, 43, 26, 24, 41, 21, 19, 26, 23, 20, 21, 20, 21, 17, 25, 19, 19, 17, 19, 22, 17, 23, 19, 18, 17, 19, 17, 17, 18, 18, 17, 17, 21, 19, 23, 23, 17, 18, 32, 24, 19, 18, 22, 20, 19, 20, 17, 18, 17, 21, 22, 20, 32, 17, 27, 21, 17, 29, 23, 17, 18, 20, 33, 19, 18, 21, 22, 17, 17, 26, 17, 17, 24, 17, 17, 25, 25, 27, 17, 19, 24, 22, 18, 17, 21, 19, 18, 21, 17, 29, 17, 20, 26, 17, 18, 20, 18, 19, 20, 21, 21, 17, 21, 17, 20, 20, 17, 19, 17, 20, 21, 23, 17, 18, 21, 17, 26, 19, 21, 23, 22, 18, 17, 17, 23, 17, 18, 22, 18, 17, 26, 19, 27, 17, 18, 19, 18, 27, 23, 25, 25, 18, 20, 19, 28, 19, 17, 17, 20, 18, 17, 18, 17, 18, 19, 17, 20, 20, 21, 18, 17, 18, 20, 19, 23, 18, 19, 18, 20, 21, 19, 19, 19, 17, 19, 37, 31, 18, 17, 17, 19, 17, 18, 20, 19, 17, 23, 23, 20, 23, 17, 19, 24, 19, 22, 17, 17, 18, 25, 19, 18, 19, 17, 21, 18, 17, 20, 22, 20, 20, 19, 17, 20, 22, 21, 17, 19, 19, 17, 17, 18, 22, 19, 17, 20, 25, 20, 22, 23, 20, 17, 22, 19, 24, 19, 19, 17, 18, 17, 17, 22, 17, 19, 20, 17, 23, 17, 17, 17, 20, 17, 21, 17, 21, 17, 31, 27, 17, 19, 19, 17, 30, 32, 18, 17, 17, 17, 17, 17, 21, 17, 18, 18, 17, 17, 18, 22, 20, 18, 18, 18, 25, 19, 17, 20, 17, 18, 18, 21, 25, 20, 19, 19, 17, 17, 26, 23, 17, 20, 23, 19, 17, 31, 22, 27, 19, 19, 19, 17, 23, 20, 17, 17, 18, 17, 18, 17, 19, 17, 19, 21, 22, 27, 17, 19, 17, 19, 18, 17, 18, 17, 22, 22, 18, 28, 21, 21, 18, 17, 18, 22, 18, 17, 25, 17, 18, 19, 25, 17, 17, 20, 21, 17, 17, 18, 20, 20, 20, 22, 22, 19, 18, 20, 17, 17, 21, 22, 18, 21, 23, 19, 19, 17, 17, 17, 23, 21, 18, 24, 20, 21, 19, 17, 19, 17, 18, 17, 18, 28, 17, 18, 19, 20, 28, 17, 19, 19, 19, 17, 19, 18, 17, 19, 17, 18, 20, 17, 22, 17, 21, 18, 19, 19, 22, 17, 17, 19, 17, 23, 21, 17, 21, 19, 20, 19, 17, 17, 19, 20, 19, 24, 21, 18, 17, 19, 23, 17, 21, 25, 21, 27, 17, 17, 23, 17, 18, 20, 23, 22, 20, 17, 20, 18, 17, 18, 18, 24, 18, 19, 17, 21, 19, 17, 19, 18, 20, 17, 17, 17, 20, 27, 17, 24, 17, 23, 17, 17, 19, 30, 19, 23, 18, 18, 24, 22, 17, 21, 23, 22, 17, 18, 23, 18, 18, 19, 23, 17, 19, 20, 17, 22, 20, 17, 24, 22, 17, 22, 19, 25, 20, 19, 17, 18, 23, 20, 19, 19, 18, 27, 17, 19, 22, 22, 23, 27, 20, 21, 19, 20, 20, 20, 17, 18, 17, 22, 18, 17, 25, 19, 17, 19, 17, 17, 28, 17, 22, 21, 18, 21, 18, 35, 22, 17, 18, 19, 22, 22, 17, 21, 24, 19, 18, 17, 19, 18, 17, 22, 20, 19, 23, 17, 25, 19, 17, 17, 17, 20, 18, 17, 18, 21, 21, 25, 17, 18, 17, 19, 19, 17, 19, 22, 17, 17, 18, 19, 18, 20, 21, 20, 19, 17, 20, 24, 17, 18, 23, 17, 19, 20, 22, 19, 25, 17, 18, 19, 17, 17, 17, 21, 18, 17, 19, 20, 17, 20, 17, 22, 22, 24, 22, 20, 22, 18, 20, 26, 19, 17, 22, 22, 17, 20, 20, 19, 17, 19, 17, 19, 21, 18, 18, 21, 18, 28, 24, 18, 18, 19, 17, 23, 17, 23, 20, 19, 17, 23, 25, 20, 28, 17, 25, 20, 17, 17, 18, 19, 20, 17, 20, 24, 23, 17, 26, 25, 20, 17, 17, 17, 19, 18, 17, 17, 19, 17, 20, 17, 18, 23, 17, 21, 17, 21, 17, 19, 17, 29, 17, 24, 23, 17, 24, 17, 17, 18, 17, 22, 17, 18, 19, 20, 17, 17, 17, 19, 24, 17, 18, 23, 21, 20, 18, 21, 22, 24, 17, 18, 17, 17, 19, 17, 19, 19, 17, 17, 17, 20, 19, 17, 18, 23, 27, 19, 24, 18, 19, 17, 29, 20, 18, 22, 21, 17, 19, 21, 17, 17, 19, 26, 18, 17, 17, 23, 20, 26, 17, 21, 21, 29, 22, 17, 18, 17, 22, 17, 17, 18, 19, 18, 18, 19, 23, 34, 19, 18, 17, 18, 20, 19, 18, 32, 21, 17, 18, 17, 17, 18, 18, 19, 19, 17, 18, 19, 21, 17, 19, 17, 17, 17, 17, 20, 17, 20, 18, 17, 25, 18, 24, 27, 17, 19, 18, 17, 21, 19, 17, 19, 20, 20, 17, 19, 24, 17, 17, 22, 18, 25, 17, 17, 21, 24, 24, 20, 18, 17, 19, 20, 17, 19, 17, 21, 18, 18, 20, 20, 18, 17, 17, 17, 18, 17, 18, 18, 17, 18, 17, 19, 19, 32, 17, 20, 19, 30, 17, 18, 24, 20, 19, 17, 20, 36, 20, 19, 18, 24, 22, 29, 21, 18, 17, 21, 17, 19, 30, 19, 25, 20, 17, 18, 19, 19, 19, 17, 19, 20, 17, 17, 20, 20, 19, 17, 17, 30, 22, 23, 17, 28, 19, 28, 20, 19, 21, 22, 20, 19, 17, 22, 17, 17, 21, 19, 17, 27, 26, 17, 19, 17, 19, 18, 22, 18, 20, 17, 17, 18, 18, 17, 18, 17, 24, 19, 19, 19, 19, 24, 17, 21, 19, 20, 20, 21, 20, 17, 25, 20, 17, 17, 18, 19, 17, 17, 19, 20, 24, 17, 19, 26, 27, 20, 17, 26, 18]\n"
     ]
    }
   ],
   "source": [
    "print(mc_ordinary_epslengths)\n",
    "\n",
    "print(mc_weighted_epslengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def running_mean(vals, n=1):\n",
    "    assert n < len(vals)\n",
    "    cumvals = np.array(vals).cumsum()\n",
    "    return (cumvals[n:] - cumvals[:-n]) / n \n",
    "\n",
    "# set smoothing factor\n",
    "n = 3\n",
    "\n",
    "plt.plot(running_mean(mc_ordinary_epslengths, n), label=\"ordinary\")\n",
    "plt.plot(running_mean(mc_weighted_epslengths, n), label=\"weighted\")\n",
    "# plt.hlines(num_episodes)\n",
    "plt.title('Episode lengths MC')\n",
    "# plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "# plt.gca().set_ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference\n",
    "\n",
    "### TO-DO: TD Ordinary Importance Sampling (make it work for gridworld)\n",
    "Copied from TD_lab. Currently on-policy, needs to be off-policy.\n",
    "\n",
    "Confused: do we need value functions instead of q-values? Do we even use importance weights in off-policy TD? Are there more off-policy TD methods besides SARSA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, policy, Q, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        # initial state is 3,0 in the grid (according to source code)\n",
    "        s = env.reset()\n",
    "        a = policy.sample_action(s)\n",
    "        final_state_reached = False\n",
    "        \n",
    "        while True:\n",
    "            # new actions\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # keep track of stats\n",
    "            R += r\n",
    "            i += 1    \n",
    "            \n",
    "            # sample action at state s_prime\n",
    "            a_prime = policy.sample_action(s_prime)\n",
    "\n",
    "            # update Q \n",
    "            Q[s][a] += alpha * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])    \n",
    "    \n",
    "            # update policy\n",
    "            policy.Q = Q\n",
    "            \n",
    "            # if final state, terminate loop\n",
    "            if final_state:\n",
    "                break\n",
    "        \n",
    "            # update current s and a for next iteration\n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: TD Weighted Importance Sampling (same as above but weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD weighted importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

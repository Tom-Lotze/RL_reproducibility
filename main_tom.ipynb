{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions used in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import time\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Windy gridworld\n",
    "Gives a reward of -1 for each step taken, while the final state is not reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "env = WindyGridworldEnv()\n",
    "env??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "### Target policy (choose greedy vs non-greedy)\n",
    "Greedy policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def get_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        Takes a state and an action and returns the probability of taking that action from \n",
    "        that state, under Q and a greedy policy\n",
    "        \"\"\"   \n",
    "        # for state and action only:\n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        \n",
    "        if action in max_indices:\n",
    "            prob = 1/len(max_indices)\n",
    "        else:\n",
    "            prob = 0\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "\n",
    "        best_actions = [i for i, j in enumerate([self.Q[obs][i] for i in range(4)]) \n",
    "                   if j == max([self.Q[obs][i] for i in range(4)])] \n",
    "\n",
    "        best_action = np.random.choice(best_actions)\n",
    "        \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def get_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        Takes a state and an action and returns the probability of taking that action from \n",
    "        that state, under Q and a epsilon greedy policy\n",
    "        \"\"\"\n",
    "\n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        \n",
    "        # all probs are equal, give all equal probabilities\n",
    "        if len(max_indices) == len(action_probs):\n",
    "            return 1/len(max_indices)\n",
    "            \n",
    "        if action in max_indices:\n",
    "            prob = (1-self.epsilon)/len(max_indices)\n",
    "        else:\n",
    "            prob = epsilon / (len(action_probs) - len(max_indices))\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "\n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "         \n",
    "        best_actions = [i for i, j in enumerate(self.Q[obs])\n",
    "                   if j == np.max(self.Q[obs])] \n",
    "\n",
    "        p = np.random.uniform()\n",
    "        if p > self.epsilon:\n",
    "            # choose one of the best actions\n",
    "            action = np.random.choice(best_actions)\n",
    "        else:\n",
    "            # return a random action\n",
    "            action = np.random.randint(0,4)\n",
    "                \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function given an env and policy\n",
    "Function to sample an episode from the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n",
    "        state after the termination is not included in the list of states.\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    # get a starting state\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    \n",
    "    # keep looping until done, don's save the terminal state\n",
    "    while not d:\n",
    "        states.append(s)\n",
    "        a = policy.sample_action(s)\n",
    "        s, r, d, _ = env.step(a)\n",
    "        \n",
    "        # save                \n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(d)\n",
    "        \n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 3311\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for eps greedy policy\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = EpsilonGreedyPolicy(Q, epsilon=0.1)\n",
    "\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, bp)\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Ordinary Importance Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, weighted=False, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05, seed=42):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the Q function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from (state, action) -> value.\n",
    "    \"\"\"\n",
    "\n",
    "    # set the current Q to a large negative value\n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    returns_count = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral and target policy\n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q        \n",
    "    \n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save the episode length\n",
    "        episode_lens.append(len(states)) \n",
    "\n",
    "        G = 0        \n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)):\n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            returns_count[s][a] += 1 \n",
    "            \n",
    "            # use every visit incremental method\n",
    "            Q[s][a] += 1/returns_count[s][a] * (W * G - Q[s][a])\n",
    "            \n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))        \n",
    "            if W == 0:\n",
    "                break\n",
    "        \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC: Weighted Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_weighted_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05, seed=42):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the Q function\n",
    "    for a given target policy using behavior policy and weighted importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from (state, action) -> value.\n",
    "        the lengths of the episodes generated throughout training.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a matrix defaultdict for the Q function and the sum of weights C\n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    C = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral and target policy\n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q\n",
    "\n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save episode lengths\n",
    "        episode_lens.append(len(states))\n",
    "            \n",
    "        # initialize the return and the weight\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)): \n",
    "            # extract info of current timestep from trajectory    \n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            # add W to the sum of weights C\n",
    "            C[s][a] += W\n",
    "            \n",
    "            # update Q function incrementally\n",
    "            Q[s][a] += W/C[s][a] * (G - Q[s][a])\n",
    "            \n",
    "            # update the weight\n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))\n",
    "            \n",
    "            # break out of the loop if the weights are 0\n",
    "            if W == 0:\n",
    "                break  \n",
    "                \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Plot the episode length over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (1000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 292.21it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using weighted importance sampling (1000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 331.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "seed = 10\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.1\n",
    "discount_factor = 1.0\n",
    "num_episodes = 1000\n",
    "Q = np.ones((env.nS, env.nA)) * -100\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_ordinary, mc_ordinary_epslengths = mc_ordinary_importance_sampling(env, behavioral_policy, target_policy, \n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)\n",
    "print(f\"Updating Q using weighted importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_weighted, mc_weighted_epslengths = mc_weighted_importance_sampling(env, behavioral_policy, target_policy,\n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting episode length ordinary: 16\n",
      "resulting episode length weighted: 17\n"
     ]
    }
   ],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_mc_ordinary)\n",
    "greedy_weighted = GreedyPolicy(Q_mc_weighted)\n",
    "\n",
    "ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "weighted_episode = sample_episode(env, greedy_weighted)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])}\")\n",
    "print(f\"resulting episode length weighted: {len(weighted_episode[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting episode lengths during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6870, 1863, 128, 93, 28, 78, 20, 84, 60, 28, 23, 23, 20, 27, 24, 31, 27, 35, 20, 23, 18, 30, 28, 30, 22, 24, 126, 20, 39, 21, 25, 33, 22, 24, 41, 22, 19, 32, 19, 19, 27, 20, 98, 25, 23, 41, 29, 41, 58, 24, 91, 21, 26, 47, 50, 51, 19, 28, 38, 18, 27, 27, 46, 21, 24, 37, 19, 17, 17, 20, 20, 24, 18, 17, 17, 19, 23, 17, 19, 18, 18, 17, 20, 20, 17, 20, 20, 25, 18, 17, 17, 23, 17, 18, 19, 18, 18, 30, 19, 25, 17, 18, 19, 18, 27, 21, 27, 25, 18, 20, 19, 29, 18, 17, 17, 20, 18, 17, 18, 17, 18, 19, 17, 19, 20, 21, 18, 17, 21, 17, 17, 19, 17, 16, 17, 18, 17, 21, 18, 18, 18, 17, 22, 34, 24, 16, 18, 16, 18, 16, 18, 21, 18, 17, 19, 17, 16, 23, 18, 16, 17, 19, 19, 18, 19, 16, 16, 16, 22, 21, 21, 21, 17, 22, 17, 16, 20, 18, 22, 16, 21, 16, 28, 19, 16, 19, 16, 17, 19, 16, 16, 22, 19, 17, 16, 17, 20, 19, 17, 20, 20, 21, 16, 18, 16, 23, 18, 20, 16, 17, 18, 17, 16, 16, 16, 16, 21, 17, 19, 16, 16, 16, 22, 16, 21, 16, 19, 16, 16, 19, 20, 16, 16, 20, 19, 29, 20, 22, 18, 16, 16, 16, 16, 16, 17, 20, 16, 17, 17, 18, 16, 21, 18, 19, 17, 18, 29, 20, 19, 16, 19, 16, 20, 19, 19, 27, 16, 17, 21, 16, 16, 21, 21, 16, 16, 17, 22, 18, 23, 25, 18, 24, 21, 16, 19, 16, 23, 20, 16, 18, 18, 16, 19, 17, 18, 16, 18, 20, 16, 20, 21, 16, 18, 16, 17, 20, 16, 18, 16, 20, 22, 16, 16, 22, 17, 23, 17, 16, 17, 24, 17, 16, 20, 17, 18, 17, 16, 19, 16, 16, 19, 22, 16, 17, 16, 17, 16, 19, 21, 19, 26, 17, 22, 16, 16, 19, 17, 17, 17, 20, 22, 19, 16, 19, 17, 17, 18, 17, 20, 16, 18, 16, 19, 17, 16, 16, 18, 16, 16, 16, 16, 23, 20, 16, 17, 18, 17, 22, 16, 18, 18, 18, 17, 20, 18, 16, 19, 16, 19, 23, 16, 20, 16, 22, 18, 16, 19, 17, 18, 18, 16, 18, 16, 18, 18, 16, 19, 19, 17, 20, 17, 17, 17, 18, 16, 18, 21, 19, 18, 17, 19, 19, 16, 17, 19, 18, 18, 21, 18, 18, 18, 20, 16, 16, 16, 21, 32, 16, 17, 17, 18, 16, 17, 17, 17, 16, 18, 17, 19, 16, 16, 16, 19, 19, 19, 16, 17, 16, 20, 20, 18, 16, 18, 16, 19, 19, 16, 18, 21, 21, 19, 19, 17, 20, 23, 22, 20, 20, 18, 19, 17, 19, 16, 20, 18, 19, 16, 19, 16, 16, 18, 16, 20, 25, 16, 19, 20, 16, 18, 17, 21, 20, 19, 21, 16, 17, 18, 17, 20, 16, 18, 20, 21, 16, 18, 18, 22, 21, 25, 21, 17, 21, 16, 18, 29, 16, 20, 16, 23, 17, 18, 22, 17, 16, 16, 18, 18, 24, 19, 22, 21, 18, 16, 20, 17, 22, 21, 18, 16, 17, 20, 18, 21, 16, 20, 16, 18, 17, 16, 17, 18, 16, 16, 20, 16, 24, 24, 18, 23, 18, 16, 16, 16, 19, 19, 16, 17, 21, 21, 18, 20, 17, 18, 24, 20, 18, 17, 20, 16, 18, 22, 18, 16, 16, 16, 26, 16, 18, 16, 20, 18, 16, 18, 16, 16, 16, 18, 19, 19, 18, 25, 16, 18, 18, 16, 16, 16, 17, 16, 16, 16, 19, 17, 16, 19, 16, 21, 21, 27, 21, 20, 18, 23, 18, 26, 19, 16, 19, 18, 18, 16, 20, 21, 16, 17, 16, 16, 18, 19, 18, 17, 19, 17, 18, 18, 23, 17, 17, 18, 16, 29, 16, 19, 21, 17, 16, 19, 18, 20, 27, 18, 16, 22, 18, 16, 16, 16, 16, 17, 16, 16, 19, 22, 21, 16, 16, 18, 21, 19, 16, 16, 16, 19, 16, 17, 20, 17, 16, 24, 16, 17, 22, 18, 21, 17, 21, 16, 18, 18, 20, 19, 16, 20, 22, 16, 26, 16, 16, 17, 16, 21, 17, 16, 22, 19, 18, 16, 17, 17, 31, 16, 17, 22, 20, 19, 16, 22, 18, 19, 19, 16, 19, 16, 18, 16, 16, 19, 19, 16, 16, 16, 20, 16, 16, 17, 17, 20, 25, 16, 21, 28, 16, 17, 23, 18, 19, 19, 19, 16, 17, 18, 18, 17, 16, 27, 17, 16, 17, 16, 18, 16, 21, 18, 21, 18, 27, 19, 17, 16, 18, 16, 22, 18, 16, 17, 18, 16, 16, 16, 17, 17, 26, 16, 17, 17, 21, 22, 17, 16, 16, 21, 19, 16, 17, 19, 17, 16, 16, 18, 17, 18, 19, 16, 16, 20, 16, 18, 16, 16, 16, 22, 18, 16, 17, 19, 16, 18, 20, 33, 23, 16, 19, 17, 16, 18, 23, 19, 17, 16, 16, 20, 20, 19, 23, 16, 19, 20, 16, 19, 16, 17, 24, 18, 19, 20, 19, 20, 16, 19, 16, 16, 19, 16, 18, 17, 19, 18, 20, 19, 16, 17, 16, 16, 18, 16, 16, 18, 18, 16, 19, 16, 16, 26, 16, 19, 19, 21, 18, 16, 16, 21, 19, 18, 16, 20, 19, 21, 19, 18, 17, 26, 21, 29, 20, 16, 16, 18, 18, 18, 28, 17, 22, 17, 19, 16, 17, 18, 20, 18, 16, 18, 16, 16, 16, 16, 29, 18, 16, 16, 19, 22, 21, 19, 16, 19, 17, 19, 24, 19, 18, 18, 16, 19, 26, 16, 16, 18, 18, 16, 17, 18, 16, 20, 32, 16, 18, 16, 20, 21, 20, 16, 22, 16, 16, 18, 18, 17, 16, 16, 22, 19, 17, 26, 19, 35, 28, 19, 19, 17, 16, 23, 16, 18, 20, 17, 16, 16, 17, 17, 16]\n",
      "[6870, 1863, 128, 93, 28, 78, 20, 27, 31, 30, 33, 43, 26, 24, 41, 21, 19, 26, 23, 20, 21, 20, 21, 17, 25, 19, 19, 17, 19, 22, 17, 23, 19, 18, 17, 19, 17, 17, 18, 18, 17, 17, 21, 19, 23, 23, 17, 18, 32, 24, 19, 18, 22, 20, 19, 20, 17, 18, 17, 21, 22, 20, 32, 17, 27, 21, 17, 29, 23, 17, 18, 20, 33, 19, 18, 21, 22, 17, 17, 26, 17, 17, 24, 17, 17, 25, 25, 27, 17, 19, 24, 22, 18, 17, 21, 19, 18, 21, 17, 29, 17, 20, 26, 17, 18, 20, 18, 19, 20, 21, 21, 17, 21, 17, 20, 20, 17, 19, 17, 20, 21, 23, 17, 18, 21, 17, 26, 19, 21, 23, 22, 18, 17, 17, 23, 17, 18, 22, 18, 17, 26, 19, 27, 17, 18, 19, 18, 27, 23, 25, 25, 18, 20, 19, 28, 19, 17, 17, 20, 18, 17, 18, 17, 18, 19, 17, 20, 20, 21, 18, 17, 18, 20, 19, 23, 18, 19, 18, 20, 21, 19, 19, 19, 17, 19, 37, 31, 18, 17, 17, 19, 17, 18, 20, 19, 17, 23, 23, 20, 23, 17, 19, 24, 19, 22, 17, 17, 18, 25, 19, 18, 19, 17, 21, 18, 17, 20, 22, 20, 20, 19, 17, 20, 22, 21, 17, 19, 19, 17, 17, 18, 22, 19, 17, 20, 25, 20, 22, 23, 20, 17, 22, 19, 24, 19, 19, 17, 18, 17, 17, 22, 17, 19, 20, 17, 23, 17, 17, 17, 20, 17, 21, 17, 21, 17, 31, 27, 17, 19, 19, 17, 30, 32, 18, 17, 17, 17, 17, 17, 21, 17, 18, 18, 17, 17, 18, 22, 20, 18, 18, 18, 25, 19, 17, 20, 17, 18, 18, 21, 25, 20, 19, 19, 17, 17, 26, 23, 17, 20, 23, 19, 17, 31, 22, 27, 19, 19, 19, 17, 23, 20, 17, 17, 18, 17, 18, 17, 19, 17, 19, 21, 22, 27, 17, 19, 17, 19, 18, 17, 18, 17, 22, 22, 18, 28, 21, 21, 18, 17, 18, 22, 18, 17, 25, 17, 18, 19, 25, 17, 17, 20, 21, 17, 17, 18, 20, 20, 20, 22, 22, 19, 18, 20, 17, 17, 21, 22, 18, 21, 23, 19, 19, 17, 17, 17, 23, 21, 18, 24, 20, 21, 19, 17, 19, 17, 18, 17, 18, 28, 17, 18, 19, 20, 28, 17, 19, 19, 19, 17, 19, 18, 17, 19, 17, 18, 20, 17, 22, 17, 21, 18, 19, 19, 22, 17, 17, 19, 17, 23, 21, 17, 21, 19, 20, 19, 17, 17, 19, 20, 19, 24, 21, 18, 17, 19, 23, 17, 21, 25, 21, 27, 17, 17, 23, 17, 18, 20, 23, 22, 20, 17, 20, 18, 17, 18, 18, 24, 18, 19, 17, 21, 19, 17, 19, 18, 20, 17, 17, 17, 20, 27, 17, 24, 17, 23, 17, 17, 19, 30, 19, 23, 18, 18, 24, 22, 17, 21, 23, 22, 17, 18, 23, 18, 18, 19, 23, 17, 19, 20, 17, 22, 20, 17, 24, 22, 17, 22, 19, 25, 20, 19, 17, 18, 23, 20, 19, 19, 18, 27, 17, 19, 22, 22, 23, 27, 20, 21, 19, 20, 20, 20, 17, 18, 17, 22, 18, 17, 25, 19, 17, 19, 17, 17, 28, 17, 22, 21, 18, 21, 18, 35, 22, 17, 18, 19, 22, 22, 17, 21, 24, 19, 18, 17, 19, 18, 17, 22, 20, 19, 23, 17, 25, 19, 17, 17, 17, 20, 18, 17, 18, 21, 21, 25, 17, 18, 17, 19, 19, 17, 19, 22, 17, 17, 18, 19, 18, 20, 21, 20, 19, 17, 20, 24, 17, 18, 23, 17, 19, 20, 22, 19, 25, 17, 18, 19, 17, 17, 17, 21, 18, 17, 19, 20, 17, 20, 17, 22, 22, 24, 22, 20, 22, 18, 20, 26, 19, 17, 22, 22, 17, 20, 20, 19, 17, 19, 17, 19, 21, 18, 18, 21, 18, 28, 24, 18, 18, 19, 17, 23, 17, 23, 20, 19, 17, 23, 25, 20, 28, 17, 25, 20, 17, 17, 18, 19, 20, 17, 20, 24, 23, 17, 26, 25, 20, 17, 17, 17, 19, 18, 17, 17, 19, 17, 20, 17, 18, 23, 17, 21, 17, 21, 17, 19, 17, 29, 17, 24, 23, 17, 24, 17, 17, 18, 17, 22, 17, 18, 19, 20, 17, 17, 17, 19, 24, 17, 18, 23, 21, 20, 18, 21, 22, 24, 17, 18, 17, 17, 19, 17, 19, 19, 17, 17, 17, 20, 19, 17, 18, 23, 27, 19, 24, 18, 19, 17, 29, 20, 18, 22, 21, 17, 19, 21, 17, 17, 19, 26, 18, 17, 17, 23, 20, 26, 17, 21, 21, 29, 22, 17, 18, 17, 22, 17, 17, 18, 19, 18, 18, 19, 23, 34, 19, 18, 17, 18, 20, 19, 18, 32, 21, 17, 18, 17, 17, 18, 18, 19, 19, 17, 18, 19, 21, 17, 19, 17, 17, 17, 17, 20, 17, 20, 18, 17, 25, 18, 24, 27, 17, 19, 18, 17, 21, 19, 17, 19, 20, 20, 17, 19, 24, 17, 17, 22, 18, 25, 17, 17, 21, 24, 24, 20, 18, 17, 19, 20, 17, 19, 17, 21, 18, 18, 20, 20, 18, 17, 17, 17, 18, 17, 18, 18, 17, 18, 17, 19, 19, 32, 17, 20, 19, 30, 17, 18, 24, 20, 19, 17, 20, 36, 20, 19, 18, 24, 22, 29, 21, 18, 17, 21, 17, 19, 30, 19, 25, 20, 17, 18, 19, 19, 19, 17, 19, 20, 17, 17, 20, 20, 19, 17, 17, 30, 22, 23, 17, 28, 19, 28, 20, 19, 21, 22, 20, 19, 17, 22, 17, 17, 21, 19, 17, 27, 26, 17, 19, 17, 19, 18, 22, 18, 20, 17, 17, 18, 18, 17, 18, 17, 24, 19, 19, 19, 19, 24, 17, 21, 19, 20, 20, 21, 20, 17, 25, 20, 17, 17, 18, 19, 17, 17, 19, 20, 24, 17, 19, 26, 27, 20, 17, 26, 18]\n"
     ]
    }
   ],
   "source": [
    "print(mc_ordinary_epslengths)\n",
    "\n",
    "print(mc_weighted_epslengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def running_mean(vals, n=1):\n",
    "    assert n < len(vals)\n",
    "    cumvals = np.array(vals).cumsum()\n",
    "    return (cumvals[n:] - cumvals[:-n]) / n \n",
    "\n",
    "# set smoothing factor\n",
    "n = 3\n",
    "\n",
    "plt.plot(running_mean(mc_ordinary_epslengths, n), label=\"ordinary\")\n",
    "plt.plot(running_mean(mc_weighted_epslengths, n), label=\"weighted\")\n",
    "# plt.hlines(num_episodes)\n",
    "plt.title('Episode lengths MC')\n",
    "# plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "# plt.gca().set_ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference\n",
    "\n",
    "### TO-DO: TD Ordinary Importance Sampling (make it work for gridworld)\n",
    "Copied from TD_lab. Currently on-policy, needs to be off-policy.\n",
    "\n",
    "Confused: do we need value functions instead of q-values? Do we even use importance weights in off-policy TD? Are there more off-policy TD methods besides SARSA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, policy, Q, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        # initial state is 3,0 in the grid (according to source code)\n",
    "        s = env.reset()\n",
    "        a = policy.sample_action(s)\n",
    "        final_state_reached = False\n",
    "        \n",
    "        while True:\n",
    "            # new actions\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # keep track of stats\n",
    "            R += r\n",
    "            i += 1    \n",
    "            \n",
    "            # sample action at state s_prime\n",
    "            a_prime = policy.sample_action(s_prime)\n",
    "\n",
    "            # update Q \n",
    "            Q[s][a] += alpha * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])    \n",
    "    \n",
    "            # update policy\n",
    "            policy.Q = Q\n",
    "            \n",
    "            # if final state, terminate loop\n",
    "            if final_state:\n",
    "                break\n",
    "        \n",
    "            # update current s and a for next iteration\n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: TD Weighted Importance Sampling (same as above but weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD weighted importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

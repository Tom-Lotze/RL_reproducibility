{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions used in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import time\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Windy gridworld\n",
    "Gives a reward of -1 for each step taken, while the final state is not reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "env = WindyGridworldEnv()\n",
    "env??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "### Target policy (choose greedy vs non-greedy)\n",
    "Greedy policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def get_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"   \n",
    "#         # Inefficient but kept same structure as below if we change policy later\n",
    "#         probs = [1/(len(list(np.where(self.Q[s] == max(self.Q[s]))[0]))) \n",
    "#                  if self.Q[s][a] == np.max(self.Q[s]) else 0 for s,a in zip(states, actions)]\n",
    "        \n",
    "        # for state and action only:\n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        \n",
    "        if action in max_indices:\n",
    "            prob = 1/len(max_indices)\n",
    "        else:\n",
    "            prob = 0\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "\n",
    "        best_actions = [i for i, j in enumerate([self.Q[obs][i] for i in range(4)]) \n",
    "                   if j == max([self.Q[obs][i] for i in range(4)])] \n",
    "\n",
    "        best_action = np.random.choice(best_actions)\n",
    "        \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def get_probs(self, state, action):\n",
    "        # loop over the state action lists and compute probabilities according to eps greedy\n",
    "#         probs = [(1-epsilon)/(len(list(np.where(self.Q[s] == max(self.Q[s]))[0])))\n",
    "#                  if Q[s][a] == np.max(self.Q[s]) else \n",
    "#                  epsilon / (4-len(list(np.where(self.Q[s] == max(self.Q[s]))[0])))\n",
    "#                  for s,a in zip(states, actions)]\n",
    "        \n",
    "        \n",
    "        # for one state and action \n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        # all probs are equal, give all equal probabilities\n",
    "        if len(max_indices) == len(action_probs):\n",
    "            return 1/len(max_indices)\n",
    "            \n",
    "        if action in max_indices:\n",
    "            prob = (1-self.epsilon)/len(max_indices)\n",
    "        else:\n",
    "            prob = epsilon / (len(action_probs) - len(max_indices))\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "        \n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "         \n",
    "        best_actions = [i for i, j in enumerate(self.Q[obs])\n",
    "                   if j == np.max(self.Q[obs])] \n",
    "#         if len(best_actions) == 0:\n",
    "#             print(\"WRONG\")\n",
    "#             print(self.Q)\n",
    "#             print(obs)\n",
    "            \n",
    "        p = np.random.uniform()\n",
    "        if p > self.epsilon:\n",
    "            # choose one of the best actions\n",
    "            action = np.random.choice(best_actions)\n",
    "        else:\n",
    "            # return a random action\n",
    "            action = np.random.randint(0,4)\n",
    "                \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy(object):\n",
    "    \"\"\"\n",
    "    A behavioural policy\n",
    "    \"\"\"\n",
    "    def __init__(self, nS, nA):\n",
    "        self.probs = np.ones((nS, nA)) * 1/nA\n",
    "        \n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"        \n",
    "        probs = [self.probs[s,a] for s,a in zip(states, actions)]\n",
    "        \n",
    "        return probs\n",
    "\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            state: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        p_s = self.probs[state]\n",
    "        \n",
    "        return np.random.choice(range(0,self.probs.shape[1]), p=p_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = RandomPolicy(env.nS, env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function given an env and policy\n",
    "Function to sample an episode from the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n",
    "        state after the termination is not included in the list of states.\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    # get a starting state\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    \n",
    "    # keep looping until done, don's save the terminal state\n",
    "    while not d:\n",
    "        states.append(s)\n",
    "        a = policy.sample_action(s)\n",
    "        s, r, d, _ = env.step(a)\n",
    "        \n",
    "        # save                \n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(d)\n",
    "        \n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 2923\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for random policy\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, random_policy)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 6622\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for eps greedy policy\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = EpsilonGreedyPolicy(Q, epsilon=0.1)\n",
    "\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, bp)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Ordinary Importance Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qdefaultdict2array(Q, nA, nS):\n",
    "    \"\"\" Helper function to convert a defaultdict matrix Q to a NumPy matrix\"\"\"\n",
    "    Q_np = np.zeros((nS, nA))\n",
    "    for S in range(nS):\n",
    "        for A in range(nA):\n",
    "            Q_np[S][A] = Q[S][A]\n",
    "    return Q_np\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and weighted importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "#     Q = defaultdict(lambda: defaultdict(float))\n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    returns_count = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral function:\n",
    "#         behavior_policy = EpsilonGreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS), epsilon)\n",
    "        behavior_policy.Q = Q\n",
    "#         target_policy = GreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "        target_policy.Q = Q        \n",
    "    \n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save the episode length\n",
    "        episode_lens.append(len(states))\n",
    "        \n",
    "        # extract target and behavioral probabilities\n",
    "#         target_probs = target_policy.get_probs(states, actions)\n",
    "#         behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "#         target_probs = [target_policy.get_probs(states[t], actions[t]) for t in range(len(states))]\n",
    "#         behavioral_probs = [behavioral_policy.get_probs(states[t], actions[t]) for t in range(len(states))]\n",
    "            \n",
    "\n",
    "        G = 0        \n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)):\n",
    "#             print(i)\n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            returns_count[s][a] += 1 \n",
    "\n",
    "            # compute the ratio using the two probability lists\n",
    "#             ratio = np.prod([t/b for t, b in zip(target_probs[timestep:], behavioral_probs[timestep:])])\n",
    "            \n",
    "            # use every visit incremental method\n",
    "            Q[s][a] += 1/returns_count[s][a] * (W * G - Q[s][a])\n",
    "            \n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))\n",
    "            \n",
    "            if W == 0:\n",
    "                break\n",
    "    \n",
    "#     Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "    \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC: Weighted Importance Sampling\n",
    "\n",
    "##### (TODO: Eventually: merge the two functions into one with a weighted flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_weighted_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a matrix defaultdict for the Q function and the sum of weights C\n",
    "#     Q = defaultdict(lambda: defaultdict(float))\n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    C = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral function:\n",
    "#         behavior_policy = EpsilonGreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS), epsilon)\n",
    "        behavior_policy.Q = Q\n",
    "#         target_policy = GreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "        target_policy.Q = Q\n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save episode lengths\n",
    "        episode_lens.append(len(states))\n",
    "        \n",
    "        # extract target and behavioral probabilities OLD\n",
    "#         target_probs = target_policy.get_probs(states, actions)\n",
    "#         behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "       \n",
    "        # print(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "#         print(target_probs)\n",
    "        \n",
    "        \n",
    "        # initialize the return and the weight\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)): \n",
    "#             print(i)\n",
    "            # extract info of current timestep from trajectory    \n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            # add W to the sum of weights C\n",
    "            C[s][a] += W\n",
    "            \n",
    "            # update Q function incrementally\n",
    "            Q[s][a] += W/C[s][a] * (G - Q[s][a])\n",
    "            \n",
    "            # update the weight\n",
    "            # W *= (target_probs[timestep])/(behavioral_probs[timestep])\n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))\n",
    "            \n",
    "            # break out of the loop if the weights are 0\n",
    "            if W == 0:\n",
    "                break\n",
    "    \n",
    "#     Q = Qdefaultdict2array(Q, env.nA, env.nS)     \n",
    "    \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Plot the episode length over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (5000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:06<00:00, 723.69it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using weighted importance sampling (5000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:06<00:00, 753.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "np.random.seed(42)\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.1\n",
    "discount_factor = 1.0\n",
    "num_episodes = 5000\n",
    "Q = np.ones((env.nS, env.nA)) * -100\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_ordinary, mc_ordinary_epslengths = mc_ordinary_importance_sampling(env, behavioral_policy, target_policy, \n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)\n",
    "print(f\"Updating Q using weighted importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_weighted, mc_weighted_epslengths = mc_weighted_importance_sampling(env, behavioral_policy, target_policy,\n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting episode length ordinary: 16\n",
      "resulting episode length weighted: 15\n"
     ]
    }
   ],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_mc_ordinary)\n",
    "greedy_weighted = GreedyPolicy(Q_mc_weighted)\n",
    "\n",
    "ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "weighted_episode = sample_episode(env, greedy_weighted)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])}\")\n",
    "print(f\"resulting episode length weighted: {len(weighted_episode[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting episode lengths during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-100., -100., -100., -100.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_mc_weighted[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def running_mean(vals, n=1):\n",
    "    assert n < len(vals)\n",
    "    cumvals = np.array(vals).cumsum()\n",
    "    return (cumvals[n:] - cumvals[:-n]) / n \n",
    "\n",
    "# set smoothing factor\n",
    "n = 10\n",
    "\n",
    "plt.plot(running_mean(mc_ordinary_epslengths, n), label=\"ordinary\")\n",
    "plt.plot(running_mean(mc_weighted_epslengths, n), label=\"weighted\")\n",
    "plt.title('Episode lengths MC')\n",
    "plt.legend()\n",
    "# plt.gca().set_ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference\n",
    "\n",
    "### TO-DO: TD Ordinary Importance Sampling (make it work for gridworld)\n",
    "Copied from TD_lab. Currently on-policy, needs to be off-policy.\n",
    "\n",
    "Confused: do we need value functions instead of q-values? Do we even use importance weights in off-policy TD? Are there more off-policy TD methods besides SARSA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, policy, Q, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        # initial state is 3,0 in the grid (according to source code)\n",
    "        s = env.reset()\n",
    "        a = policy.sample_action(s)\n",
    "        final_state_reached = False\n",
    "        \n",
    "        while True:\n",
    "            # new actions\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # keep track of stats\n",
    "            R += r\n",
    "            i += 1    \n",
    "            \n",
    "            # sample action at state s_prime\n",
    "            a_prime = policy.sample_action(s_prime)\n",
    "\n",
    "            # update Q \n",
    "            Q[s][a] += alpha * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])    \n",
    "    \n",
    "            # update policy\n",
    "            policy.Q = Q\n",
    "            \n",
    "            # if final state, terminate loop\n",
    "            if final_state:\n",
    "                break\n",
    "        \n",
    "            # update current s and a for next iteration\n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: TD Weighted Importance Sampling (same as above but weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD weighted importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions used in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import time\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Windy gridworld\n",
    "Gives a reward of -1 for each step taken, while the final state is not reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "env = WindyGridworldEnv()\n",
    "env??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "### Target policy (choose greedy vs non-greedy)\n",
    "Greedy policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"   \n",
    "        \n",
    "        # Inefficient but kept same structure as below if we change policy later\n",
    "        probs = [1 if a == np.argmax(self.Q[s]) else 0 for s,a in zip(states, actions)]\n",
    "        \n",
    "        return probs\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "\n",
    "        # find out what the max action is\n",
    "        best_action = np.argmax(self.Q[obs])\n",
    "        \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def get_probs(self, states, actions):\n",
    "        # loop over the state action lists and compute probabilities according to eps greedy\n",
    "        probs = [1-self.epsilon if a == np.argmax(Q[s]) else self.epsilon/self.Q.shape[0] for s, a in zip(states, actions)]\n",
    "                \n",
    "        return probs\n",
    "        \n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "         \n",
    "        best_actions = [i for i, j in enumerate([self.Q[obs][i] for i in range(4)]) \n",
    "                   if j == max([self.Q[obs][i] for i in range(4)])] \n",
    "        p = np.random.uniform()\n",
    "        if p > self.epsilon:\n",
    "            # choose one of the best actions\n",
    "            action = np.random.choice(best_actions)\n",
    "        else:\n",
    "            # return a random action\n",
    "            action = np.random.randint(0,4)\n",
    "                \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioural policy\n",
    "Random policy from blackjack lab. \n",
    "TODO: experiment with behavioural policies to check which yield interesting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy(object):\n",
    "    \"\"\"\n",
    "    A behavioural policy\n",
    "    \"\"\"\n",
    "    def __init__(self, nS, nA):\n",
    "        self.probs = np.ones((nS, nA)) * 1/nA\n",
    "        \n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"        \n",
    "        probs = [self.probs[s,a] for s,a in zip(states, actions)]\n",
    "        \n",
    "        return probs\n",
    "\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            state: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        p_s = self.probs[state]\n",
    "        \n",
    "        return np.random.choice(range(0,self.probs.shape[1]), p=p_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = RandomPolicy(env.nS, env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function given an env and policy\n",
    "Function to sample an episode from the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n",
    "        state after the termination is not included in the list of states.\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    # get a starting state\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    \n",
    "    # keep looping until done, don's save the terminal state\n",
    "    while not d:\n",
    "        states.append(s)\n",
    "        a = policy.sample_action(s)\n",
    "        s, r, d, _ = env.step(a)\n",
    "        \n",
    "        # save                \n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(d)\n",
    "        \n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 7393\n",
      "length of episode 1: 9911\n",
      "length of episode 2: 170\n",
      "length of episode 3: 12086\n",
      "length of episode 4: 509\n",
      "length of episode 5: 10901\n",
      "length of episode 6: 3895\n",
      "length of episode 7: 4957\n",
      "length of episode 8: 9287\n",
      "length of episode 9: 6468\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for random policy\n",
    "for episode in range(10):\n",
    "    trajectory_data = sample_episode(env, random_policy)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 7401\n",
      "length of episode 1: 5997\n",
      "length of episode 2: 8393\n",
      "length of episode 3: 499\n",
      "length of episode 4: 3313\n",
      "length of episode 5: 6857\n",
      "length of episode 6: 2296\n",
      "length of episode 7: 2545\n",
      "length of episode 8: 26983\n",
      "length of episode 9: 5788\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for eps greedy policy\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = EpsilonGreedyPolicy(Q, epsilon=0.1)\n",
    "\n",
    "for episode in range(10):\n",
    "    trajectory_data = sample_episode(env, bp)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: MC Ordinary Importance Sampling (make it work for windy gridworld)\n",
    "Status: updated to update Q instead of V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and weighted importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "    Q = defaultdict(lambda: defaultdict(float))\n",
    "    returns_count = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # extract target and behavioral probabilities\n",
    "        target_probs = target_policy.get_probs(states, actions)\n",
    "        behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "\n",
    "        G = 0        \n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for timestep in range(len(states)-1, -1, -1):\n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            returns_count[s][a] += 1 \n",
    "\n",
    "            # compute the ratio using the two probability lists\n",
    "            ratio = np.prod([t/b for t, b in zip(target_probs[timestep:], behavioral_probs[timestep:])])\n",
    "\n",
    "            # use every visit incremental method\n",
    "            Q[s][a] += 1/returns_count[s][a] * (ratio * G - Q[s][a])\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4460d04b7616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpsilonGreedyPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGreedyPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mQ_10k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc_ordinary_importance_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-a2ac12f4f11b>\u001b[0m in \u001b[0;36mmc_ordinary_importance_sampling\u001b[0;34m(env, behavior_policy, target_policy, num_episodes, discount_factor, sampling_function)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# sample episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehavior_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# extract target and behavioral probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a1d0e87ee118>\u001b[0m in \u001b[0;36msample_episode\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-abc4dc8006e4>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = EpsilonGreedyPolicy(Q, epsilon=0.05)\n",
    "gp = GreedyPolicy(Q)\n",
    "Q_10k = mc_ordinary_importance_sampling(env, bp, gp, num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q_10k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC: Weighted Importance Sampling\n",
    "\n",
    "##### (TODO: Eventually: merge the two functions into one with a weighted flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_weighted_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a matrix defaultdict for the Q function and the sum of weights C\n",
    "    Q = defaultdict(lambda: defaultdict(float))\n",
    "    C = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # extract target and behavioral probabilities\n",
    "        target_probs = target_policy.get_probs(states, actions)\n",
    "        behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "        \n",
    "        # initialize the return and the weight\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for timestep in range(len(states)-1, -1, -1):            \n",
    "            # extract info of current timestep from trajectory    \n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            # add W to the sum of weights C\n",
    "            C[s][a] += W\n",
    "            \n",
    "            # update Q function incrementally\n",
    "            Q[s][a] += W/C[s][a] * (G - Q[s][a])\n",
    "            \n",
    "            # update the weight\n",
    "            W *= (target_probs[timestep])/(behavioral_probs[timestep])\n",
    "            \n",
    "            # break out of the loop if the weights are 0\n",
    "            if W == 0:\n",
    "                break\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:45<00:00,  1.90it/s]"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = EpsilonGreedyPolicy(Q, epsilon=0.05)\n",
    "gp = GreedyPolicy(Q)\n",
    "Q_10k = mc_weighted_importance_sampling(env, bp, gp, num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{48: defaultdict(<class 'float'>, {3: -1.0, 2: -2.0005725004522805, 1: -5.012202043274311, 0: -39.0}), 68: defaultdict(<class 'float'>, {0: -2.0006118702835, 3: -3.0, 1: -4.0028652595420615, 2: -4.0214810627473145}), 69: defaultdict(<class 'float'>, {3: -3.000613189564369, 1: -4.006833008299926, 2: -4.655265492176622, 0: -4.00000108038424}), 59: defaultdict(<class 'float'>, {2: -4.007392831869315, 1: -4.024237261368661, 3: -3.001838149294274, 0: -3.0094529119628386}), 49: defaultdict(<class 'float'>, {2: -4.013772815655844, 1: -3.0021101369959573, 3: -2.0005413288157397, 0: -4.013838303570058}), 39: defaultdict(<class 'float'>, {2: -3.003545659157099, 3: -6.004279394477442, 1: -4.004442904649776, 0: -5.003703219690444}), 29: defaultdict(<class 'float'>, {2: -4.006719726445993, 0: -6.007699012048301, 3: -7.000047812834603, 1: -5.02377780148853}), 19: defaultdict(<class 'float'>, {2: -5.005950520848916, 3: -10.575218191794857, 0: -8.743976508556646, 1: -6.132756369026636}), 9: defaultdict(<class 'float'>, {2: -6.0450942345441145, 3: -10.576462078345545, 0: -7.59740133649666, 1: -8.42717808202784}), 8: defaultdict(<class 'float'>, {1: -7.67767112495348, 3: -10.021871900318898, 0: -12.636232199970694, 2: -9.899064077700062}), 7: defaultdict(<class 'float'>, {1: -8.674971369559842, 3: -11.000018592115325, 2: -11.002661203249227, 0: -9.794539094893532}), 6: defaultdict(<class 'float'>, {1: -9.119024149258664, 0: -10.000034127567208, 2: -14.24609816726652, 3: -17.000109391473245}), 5: defaultdict(<class 'float'>, {1: -13.00215022361696, 0: -19.777295112879084, 3: -18.99995897482175, 2: -14.04772059031669}), 4: defaultdict(<class 'float'>, {1: -16.108514054224134, 2: -20.021501199297653, 0: -17.79459417528983, 3: -20.000000910540173}), 18: defaultdict(<class 'float'>, {0: -10.792138587103445, 3: -14.000000000001908, 1: -9.559029087801846, 2: -11.791869429200124}), 58: defaultdict(<class 'float'>, {1: -3.0017042422733997, 3: -2.0, 2: -3.0009569557711973, 0: -9.000093058337782}), 17: defaultdict(<class 'float'>, {3: -36.0, 1: -13.000000000357302, 2: -18.0, 0: -26.0}), 27: defaultdict(<class 'float'>, {2: -42.0, 1: -35.0, 0: -14.000000069970843, 3: -27.0}), 38: defaultdict(<class 'float'>, {3: -15.000000069970843, 0: -11.000093058337782, 1: -5.004280276794573, 2: -11.021572462446574}), 3: defaultdict(<class 'float'>, {1: -20.14768360762279, 2: -24.00000000577795, 0: -25.81060662020593, 3: -27.002236989516426}), 13: defaultdict(<class 'float'>, {1: -23.0, 0: -28.0, 3: -27.000000000000064, 2: -29.0}), 12: defaultdict(<class 'float'>, {1: -27.980908160625162, 0: -45.949249640885135, 2: -30.0, 3: -35.000000000001144}), 2: defaultdict(<class 'float'>, {2: -29.161686951450562, 1: -25.00000000000027, 0: -26.0, 3: -32.99980217757185}), 22: defaultdict(<class 'float'>, {0: -29.0, 2: -42.04498628597113, 3: -30.000016326397336, 1: -28.000000111161203}), 33: defaultdict(<class 'float'>, {3: -72.0, 2: -56.00288025921222, 1: -48.04498675544208, 0: -40.000000001528264}), 32: defaultdict(<class 'float'>, {1: -41.000000001528264, 0: -44.86668329603106, 3: -59.00009305833778, 2: -43.99999999997676}), 31: defaultdict(<class 'float'>, {1: -42.000016326397336, 2: -30.000000001282725, 3: -62.0, 0: -51.0}), 30: defaultdict(<class 'float'>, {1: -48.00000035451892, 2: -60.0, 0: -54.94620631821019, 3: -53.000093058285174}), 40: defaultdict(<class 'float'>, {0: -59.00000000000004, 1: -84.99998874432848, 2: -33.0, 3: -86.00000000162561}), 41: defaultdict(<class 'float'>, {3: -34.0, 1: -29.000000000016588, 0: -43.0, 2: -60.0}), 42: defaultdict(<class 'float'>, {3: -63.002849002849004, 2: -46.0, 0: -45.0, 1: -28.00000000134814}), 51: defaultdict(<class 'float'>, {0: -77.0, 1: -65.00000000053427, 2: -31.0, 3: -67.0}), 52: defaultdict(<class 'float'>, {3: -68.0, 0: -83.0, 1: -28.0, 2: -82.0000000255974}), 61: defaultdict(<class 'float'>, {0: -77.9999999999196, 3: -57.00000000000095, 2: -58.0, 1: -30.0}), 60: defaultdict(<class 'float'>, {1: -55.00000000026655, 3: -56.0, 0: -73.0, 2: -60.002849002849004}), 62: defaultdict(<class 'float'>, {3: -53.0, 1: -51.54211701476022, 2: -56.002849002849004, 0: -29.0}), 28: defaultdict(<class 'float'>, {0: -10.000000425436962, 2: -10.791666666711993, 1: -6.000093144757169, 3: -14.00000168432175}), 15: defaultdict(<class 'float'>, {0: -45.0, 3: -80.0, 2: -133.0, 1: -138.0}), 24: defaultdict(<class 'float'>, {1: -46.0, 2: -47.0, 3: -54.000016326397336, 0: -59.0}), 21: defaultdict(<class 'float'>, {2: -31.000000001266137, 3: -50.0, 1: -29.000016326664156, 0: -39.870499090722916}), 20: defaultdict(<class 'float'>, {1: -39.021481074765326, 3: -42.00284900320631, 0: -44.00000179464225, 2: -49.0000000012662}), 10: defaultdict(<class 'float'>, {2: -40.000000366795454, 3: -38.000036163820035, 0: -37.000034044137294, 1: -37.0}), 11: defaultdict(<class 'float'>, {3: -39.00003439738969, 0: -36.94659115284049, 1: -38.0, 2: -32.00001632766346}), 1: defaultdict(<class 'float'>, {2: -34.87226896479171, 0: -39.00288293491642, 3: -42.000007691034305, 1: -32.00028987984498}), 14: defaultdict(<class 'float'>, {3: -68.0, 2: -32.00286525954205, 0: -31.0000000000002, 1: -26.0}), 23: defaultdict(<class 'float'>, {1: -27.000000111161203, 0: -35.00000000577473, 2: -36.002849002850134, 3: -55.0}), 43: defaultdict(<class 'float'>, {0: -42.0, 1: -27.00000000134814, 3: -52.0, 2: -53.00000000000114}), 54: defaultdict(<class 'float'>, {3: -81.0, 1: -53.0, 0: -79.0, 2: -82.002849002849}), 63: defaultdict(<class 'float'>, {1: -54.0, 0: -121.0, 2: -122.0, 3: -50.0}), 50: defaultdict(<class 'float'>, {2: -71.0, 3: -72.002849002849, 1: -32.0, 0: -75.00000000001924}), 0: defaultdict(<class 'float'>, {1: -36.86677276318907, 2: -40.79166666666746, 3: -45.00286537663212, 0: -37.866683282197606}), 47: defaultdict(<class 'float'>, {2: -1.0, 3: -106.0, 0: -33.0, 1: -152.0}), 35: defaultdict(<class 'float'>, {3: -111.0, 0: -133.0, 1: -22.0, 2: -23.0}), 44: defaultdict(<class 'float'>, {1: -24.0, 2: -25.0, 3: -62.0, 0: -100.0}), 53: defaultdict(<class 'float'>, {1: -26.0, 2: -27.0, 3: -44.000000069970845, 0: -89.0}), 16: defaultdict(<class 'float'>, {1: -22.0, 2: -68.0, 0: -117.0}), 25: defaultdict(<class 'float'>, {1: -23.0, 2: -24.002849002849004, 0: -40.0, 3: -74.0}), 34: defaultdict(<class 'float'>, {1: -26.00000000134814, 0: -61.00002934344565, 2: -79.0, 3: -38.0}), 26: defaultdict(<class 'float'>, {0: -21.0, 2: -69.0, 1: -105.0}), 57: defaultdict(<class 'float'>, {2: -2.0}), 45: defaultdict(<class 'float'>, {0: -52.0, 3: -87.0})}\n"
     ]
    }
   ],
   "source": [
    "print(dict(Q_10k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference\n",
    "\n",
    "### TO-DO: TD Ordinary Importance Sampling (make it work for gridworld)\n",
    "Copied from TD_lab. Currently on-policy, needs to be off-policy.\n",
    "\n",
    "Confused: do we need value functions instead of q-values? Do we even use importance weights in off-policy TD? Are there more off-policy TD methods besides SARSA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, policy, Q, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        # initial state is 3,0 in the grid (according to source code)\n",
    "        s = env.reset()\n",
    "        a = policy.sample_action(s)\n",
    "        final_state_reached = False\n",
    "        \n",
    "        while True:\n",
    "            # new actions\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # keep track of stats\n",
    "            R += r\n",
    "            i += 1    \n",
    "            \n",
    "            # sample action at state s_prime\n",
    "            a_prime = policy.sample_action(s_prime)\n",
    "\n",
    "            # update Q \n",
    "            Q[s][a] += alpha * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])    \n",
    "    \n",
    "            # update policy\n",
    "            policy.Q = Q\n",
    "            \n",
    "            # if final state, terminate loop\n",
    "            if final_state:\n",
    "                break\n",
    "        \n",
    "            # update current s and a for next iteration\n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: TD Weighted Importance Sampling (same as above but weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD weighted importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

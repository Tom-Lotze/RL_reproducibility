{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions used in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import time\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Windy gridworld\n",
    "Gives a reward of -1 for each step taken, while the final state is not reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "env = WindyGridworldEnv()\n",
    "env??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "### Target policy (choose greedy vs non-greedy)\n",
    "Greedy policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def get_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"   \n",
    "#         # Inefficient but kept same structure as below if we change policy later\n",
    "#         probs = [1/(len(list(np.where(self.Q[s] == max(self.Q[s]))[0]))) \n",
    "#                  if self.Q[s][a] == np.max(self.Q[s]) else 0 for s,a in zip(states, actions)]\n",
    "        \n",
    "        # for state and action only:\n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        \n",
    "        if action in max_indices:\n",
    "            prob = 1/len(max_indices)\n",
    "        else:\n",
    "            prob = 0\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "\n",
    "        best_actions = [i for i, j in enumerate([self.Q[obs][i] for i in range(4)]) \n",
    "                   if j == max([self.Q[obs][i] for i in range(4)])] \n",
    "\n",
    "        best_action = np.random.choice(best_actions)\n",
    "        \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def get_probs(self, state, action):\n",
    "        # loop over the state action lists and compute probabilities according to eps greedy\n",
    "#         probs = [(1-epsilon)/(len(list(np.where(self.Q[s] == max(self.Q[s]))[0])))\n",
    "#                  if Q[s][a] == np.max(self.Q[s]) else \n",
    "#                  epsilon / (4-len(list(np.where(self.Q[s] == max(self.Q[s]))[0])))\n",
    "#                  for s,a in zip(states, actions)]\n",
    "        \n",
    "        \n",
    "        # for one state and action \n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        # all probs are equal, give all equal probabilities\n",
    "        if len(max_indices) == len(action_probs):\n",
    "            return 1/len(max_indices)\n",
    "            \n",
    "        if action in max_indices:\n",
    "            prob = (1-self.epsilon)/len(max_indices)\n",
    "        else:\n",
    "            prob = epsilon / (len(action_probs) - len(max_indices))\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "        \n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "         \n",
    "        best_actions = [i for i, j in enumerate(self.Q[obs])\n",
    "                   if j == np.max(self.Q[obs])] \n",
    "#         if len(best_actions) == 0:\n",
    "#             print(\"WRONG\")\n",
    "#             print(self.Q)\n",
    "#             print(obs)\n",
    "            \n",
    "        p = np.random.uniform()\n",
    "        if p > self.epsilon:\n",
    "            # choose one of the best actions\n",
    "            action = np.random.choice(best_actions)\n",
    "        else:\n",
    "            # return a random action\n",
    "            action = np.random.randint(0,4)\n",
    "                \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy(object):\n",
    "    \"\"\"\n",
    "    A behavioural policy\n",
    "    \"\"\"\n",
    "    def __init__(self, nS, nA):\n",
    "        self.probs = np.ones((nS, nA)) * 1/nA\n",
    "        \n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"        \n",
    "        probs = [self.probs[s,a] for s,a in zip(states, actions)]\n",
    "        \n",
    "        return probs\n",
    "\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            state: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        p_s = self.probs[state]\n",
    "        \n",
    "        return np.random.choice(range(0,self.probs.shape[1]), p=p_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = RandomPolicy(env.nS, env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function given an env and policy\n",
    "Function to sample an episode from the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n",
    "        state after the termination is not included in the list of states.\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    # get a starting state\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    \n",
    "    # keep looping until done, don's save the terminal state\n",
    "    while not d:\n",
    "        states.append(s)\n",
    "        a = policy.sample_action(s)\n",
    "        s, r, d, _ = env.step(a)\n",
    "        \n",
    "        # save                \n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(d)\n",
    "        \n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 1441\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for random policy\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, random_policy)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 8173\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for eps greedy policy\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = EpsilonGreedyPolicy(Q, epsilon=0.1)\n",
    "\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, bp)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Ordinary Importance Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qdefaultdict2array(Q, nA, nS):\n",
    "    \"\"\" Helper function to convert a defaultdict matrix Q to a NumPy matrix\"\"\"\n",
    "    Q_np = np.zeros((nS, nA))\n",
    "    for S in range(nS):\n",
    "        for A in range(nA):\n",
    "            Q_np[S][A] = Q[S][A]\n",
    "    return Q_np\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and weighted importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "    Q = defaultdict(lambda: defaultdict(float))\n",
    "    returns_count = defaultdict(lambda: defaultdict(float))\n",
    "    episode_lens = []\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral function:\n",
    "        behavior_policy = EpsilonGreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS), epsilon)\n",
    "        \n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save the episode length\n",
    "        episode_lens.append(len(states))\n",
    "        \n",
    "        # extract target and behavioral probabilities\n",
    "#         target_probs = target_policy.get_probs(states, actions)\n",
    "#         behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "        target_probs = [target_policy.get_probs(states[t], actions[t]) for t in range(len(states))]\n",
    "        behavioral_probs = [behavioral_policy.get_probs(states[t], actions[t]) for t in range(len(states))]\n",
    "            \n",
    "\n",
    "        G = 0        \n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)):\n",
    "#             print(i)\n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            returns_count[s][a] += 1 \n",
    "\n",
    "            # compute the ratio using the two probability lists\n",
    "            ratio = np.prod([t/b for t, b in zip(target_probs[timestep:], behavioral_probs[timestep:])])\n",
    "    \n",
    "            # use every visit incremental method\n",
    "            Q[s][a] += 1/returns_count[s][a] * (ratio * G - Q[s][a])\n",
    "    \n",
    "    Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "    \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC: Weighted Importance Sampling\n",
    "\n",
    "##### (TODO: Eventually: merge the two functions into one with a weighted flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_weighted_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a matrix defaultdict for the Q function and the sum of weights C\n",
    "    Q = defaultdict(lambda: defaultdict(float))\n",
    "    C = defaultdict(lambda: defaultdict(float))\n",
    "    episode_lens = []\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral function:\n",
    "        behavior_policy = EpsilonGreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS), epsilon)\n",
    "        \n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save episode lengths\n",
    "        episode_lens.append(len(states))\n",
    "        \n",
    "        # extract target and behavioral probabilities OLD\n",
    "#         target_probs = target_policy.get_probs(states, actions)\n",
    "#         behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "       \n",
    "        # print(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "#         print(target_probs)\n",
    "        \n",
    "        \n",
    "        # initialize the return and the weight\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)): \n",
    "#             print(i)\n",
    "            # extract info of current timestep from trajectory    \n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            # add W to the sum of weights C\n",
    "            C[s][a] += W\n",
    "            \n",
    "            # update Q function incrementally\n",
    "            Q[s][a] += W/C[s][a] * (G - Q[s][a])\n",
    "            \n",
    "            # update the weight\n",
    "            # W *= (target_probs[timestep])/(behavioral_probs[timestep])\n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))\n",
    "            \n",
    "            # break out of the loop if the weights are 0\n",
    "            if W == 0:\n",
    "                break\n",
    "    \n",
    "    Q = Qdefaultdict2array(Q, env.nA, env.nS)     \n",
    "    \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Plot the episode length over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (500 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:18<00:00, 26.68it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using weighted importance sampling (500 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:46<00:00, 10.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "np.random.seed(42)\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.4\n",
    "discount_factor = 1.0\n",
    "num_episodes = 500\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_ordinary, mc_ordinary_epslengths = mc_ordinary_importance_sampling(env, behavioral_policy, target_policy, \n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)\n",
    "print(f\"Updating Q using weighted importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_weighted, mc_weighted_epslengths = mc_weighted_importance_sampling(env, behavioral_policy, target_policy,\n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-b68f33d007c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ordinary_episode = sample_episode(env, greedy_ordinary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mweighted_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy_weighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"resulting episode length ordinary: {len(ordinary_episode[0])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a1d0e87ee118>\u001b[0m in \u001b[0;36msample_episode\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-dd61e986015c>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     47\u001b[0m                    if j == max([self.Q[obs][i] for i in range(4)])] \n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbest_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_mc_ordinary)\n",
    "greedy_weighted = GreedyPolicy(Q_mc_weighted)\n",
    "\n",
    "# ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "weighted_episode = sample_episode(env, greedy_weighted)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])}\")\n",
    "print(f\"resulting episode length weighted: {len(weighted_episode[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting episode lengths during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.06936275e+03, -6.48180723e+02, -7.98000000e+02,\n",
       "        -2.88733205e+02],\n",
       "       [-7.79181818e+02, -6.24418605e+02, -2.12009313e+02,\n",
       "        -6.36450000e+02],\n",
       "       [-1.14472619e+03, -4.30554687e+02, -1.17909302e+03,\n",
       "        -1.22703488e+03],\n",
       "       [-6.89100000e+02, -2.29898357e+02, -6.89588921e+02,\n",
       "        -6.42595611e+02],\n",
       "       [-1.65138178e+02, -2.70990566e+02, -2.66016028e+02,\n",
       "        -2.60214779e+02],\n",
       "       [-3.00856667e+02, -2.84316681e+02, -1.51421440e+02,\n",
       "        -3.26141880e+02],\n",
       "       [-1.50172789e+02, -4.00376238e+02, -3.87697236e+02,\n",
       "        -4.04309840e+02],\n",
       "       [-8.12430605e+02, -2.23748581e+02, -7.79924915e+02,\n",
       "        -9.19522436e+02],\n",
       "       [-3.86125761e+02, -1.37347384e+02, -3.73810573e+02,\n",
       "        -4.11685897e+02],\n",
       "       [-1.50272676e+02, -9.61706743e+01, -1.32360646e+02,\n",
       "        -1.46519308e+02],\n",
       "       [-1.05353488e+03, -1.13634615e+03, -3.49479381e+02,\n",
       "        -8.36850000e+02],\n",
       "       [-2.01997914e+02, -4.11946667e+02, -4.31235294e+02,\n",
       "        -4.18260274e+02],\n",
       "       [-1.48876923e+03, -1.01840625e+03, -3.77464646e+02,\n",
       "        -9.96323529e+02],\n",
       "       [-2.07871795e+02, -2.67930556e+02, -1.56770686e+02,\n",
       "        -4.24637681e+02],\n",
       "       [-2.39157895e+02, -2.84234043e+02, -1.41475352e+02,\n",
       "        -2.19888889e+02],\n",
       "       [-9.26000000e+02, -1.36433333e+03, -1.84000000e+02,\n",
       "        -2.65666667e+02],\n",
       "       [-1.09896552e+02, -7.40869565e+01, -1.14687500e+02,\n",
       "        -1.18060606e+02],\n",
       "       [-9.31400000e+01, -1.47800000e+02, -4.30750000e+02,\n",
       "        -2.46166667e+02],\n",
       "       [-1.88611429e+02, -1.79562814e+02, -1.28539484e+02,\n",
       "        -2.05248649e+02],\n",
       "       [-1.68288201e+02, -1.16671600e+02, -6.95011710e+01,\n",
       "        -1.93924214e+02],\n",
       "       [-7.36139535e+02, -1.96239496e+02, -1.00735556e+03,\n",
       "        -1.30742553e+03],\n",
       "       [-2.27650224e+02, -5.89439394e+02, -5.62941860e+02,\n",
       "        -3.85901961e+02],\n",
       "       [-4.94553571e+02, -6.03660377e+02, -2.51267956e+02,\n",
       "        -5.53417910e+02],\n",
       "       [-1.19450000e+03, -2.70000000e+02, -1.01872727e+03,\n",
       "        -7.88909091e+02],\n",
       "       [-7.19666667e+02, -1.25625000e+03, -2.44625000e+02,\n",
       "        -1.73852941e+02],\n",
       "       [-1.33285714e+02, -1.07854167e+02, -1.63444444e+02,\n",
       "        -1.46190476e+02],\n",
       "       [-1.41500000e+02, -1.67000000e+02, -7.38000000e+01,\n",
       "        -9.38333333e+01],\n",
       "       [-3.87000000e+02, -1.72500000e+02, -3.28666667e+02,\n",
       "        -9.44545455e+01],\n",
       "       [-1.51279412e+02, -9.13768116e+01, -7.84789030e+01,\n",
       "        -1.45571429e+02],\n",
       "       [-1.84872727e+02, -1.46395522e+02, -4.04297924e+01,\n",
       "        -1.58879032e+02],\n",
       "       [-4.51641975e+02, -5.50773196e+02, -2.18570136e+02,\n",
       "        -5.17132743e+02],\n",
       "       [-3.20894231e+02, -2.11686756e+02, -3.00679803e+02,\n",
       "        -2.97588832e+02],\n",
       "       [-2.37120000e+02, -2.68606218e+02, -1.99188448e+02,\n",
       "        -2.82620536e+02],\n",
       "       [-1.76124183e+02, -4.45433333e+02, -2.79111111e+02,\n",
       "        -6.39333333e+02],\n",
       "       [-2.99083333e+02, -1.18042553e+02, -2.27733333e+02,\n",
       "        -2.77478261e+02],\n",
       "       [-2.67000000e+02, -7.63333333e+01, -1.88500000e+02,\n",
       "        -2.13000000e+02],\n",
       "       [ 0.00000000e+00, -6.90000000e+01, -1.68000000e+02,\n",
       "        -2.70000000e+02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-2.02000000e+02, -4.19782609e+01, -4.04692308e+02,\n",
       "        -1.74894737e+02],\n",
       "       [-1.03925234e+02, -5.48807339e+01, -2.61938089e+01,\n",
       "        -8.10165289e+01],\n",
       "       [-3.69484848e+02, -2.19803545e+02, -3.01600000e+02,\n",
       "        -3.70850000e+02],\n",
       "       [-1.99173463e+02, -2.31495098e+02, -2.38878049e+02,\n",
       "        -2.72145078e+02],\n",
       "       [-2.71754286e+02, -2.20018750e+02, -2.30272727e+02,\n",
       "        -1.91299049e+02],\n",
       "       [-3.72148148e+02, -4.90588235e+02, -4.27913043e+02,\n",
       "        -1.71538012e+02],\n",
       "       [-1.26416667e+02, -1.27846154e+02, -9.42564103e+01,\n",
       "        -1.30666667e+02],\n",
       "       [-1.66500000e+02, -1.70000000e+02, -1.75500000e+02,\n",
       "        -1.31200000e+02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-8.10000000e+01, -1.29000000e+02, -1.00000000e+00,\n",
       "        -1.35250000e+02],\n",
       "       [-8.75405405e+01, -3.58281250e+01, -1.98363636e+01,\n",
       "        -1.00000000e+00],\n",
       "       [-6.88636364e+01, -6.97959184e+01, -6.58500000e+01,\n",
       "        -1.58124006e+01],\n",
       "       [-6.75000000e+02, -1.92541436e+02, -6.29000000e+02,\n",
       "        -8.25500000e+02],\n",
       "       [-2.49190909e+02, -2.26804598e+02, -1.86034091e+02,\n",
       "        -2.40416667e+02],\n",
       "       [-2.84906977e+02, -2.54380952e+02, -2.83852459e+02,\n",
       "        -1.86834758e+02],\n",
       "       [-1.23500000e+03, -1.31405405e+02, -5.81200000e+02,\n",
       "        -1.10166667e+03],\n",
       "       [-1.28935897e+02, -1.44000000e+02, -1.74789474e+02,\n",
       "        -1.52150000e+02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-1.86000000e+02, -5.20000000e+02,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-9.14285714e+01, -1.34222222e+01, -1.93571429e+01,\n",
       "        -6.55333333e+01],\n",
       "       [-2.33666667e+02, -2.50300000e+02, -2.78000000e+02,\n",
       "        -3.44477612e+01],\n",
       "       [-5.56766667e+02, -2.23225806e+02, -7.16771429e+02,\n",
       "        -4.07315789e+02],\n",
       "       [-2.11220930e+02, -1.70122963e+02, -2.33452713e+02,\n",
       "        -2.23856250e+02],\n",
       "       [-2.00144068e+02, -1.62837121e+02, -1.58992018e+02,\n",
       "        -1.67097458e+02],\n",
       "       [-2.77514286e+02, -1.34050505e+02, -3.03794872e+02,\n",
       "        -2.07451613e+02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-2.00000000e+00, -5.26000000e+02, -5.27000000e+02,\n",
       "        -3.54000000e+02],\n",
       "       [-3.33000000e+02, -4.77500000e+01, -2.70800000e+02,\n",
       "        -2.48800000e+02]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_mc_ordinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def running_mean(vals, n=1):\n",
    "    assert n < len(vals)\n",
    "    cumvals = np.array(vals).cumsum()\n",
    "    return (cumvals[n:] - cumvals[:-n]) / n \n",
    "\n",
    "# set smoothing factor\n",
    "n = 10\n",
    "\n",
    "plt.plot(running_mean(mc_ordinary_epslengths, n), label=\"ordinary\")\n",
    "plt.plot(running_mean(mc_weighted_epslengths, n), label=\"weighted\")\n",
    "plt.title('Episode lengths MC')\n",
    "plt.legend()\n",
    "# plt.gca().set_ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference\n",
    "\n",
    "### TO-DO: TD Ordinary Importance Sampling (make it work for gridworld)\n",
    "Copied from TD_lab. Currently on-policy, needs to be off-policy.\n",
    "\n",
    "Confused: do we need value functions instead of q-values? Do we even use importance weights in off-policy TD? Are there more off-policy TD methods besides SARSA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, policy, Q, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        # initial state is 3,0 in the grid (according to source code)\n",
    "        s = env.reset()\n",
    "        a = policy.sample_action(s)\n",
    "        final_state_reached = False\n",
    "        \n",
    "        while True:\n",
    "            # new actions\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # keep track of stats\n",
    "            R += r\n",
    "            i += 1    \n",
    "            \n",
    "            # sample action at state s_prime\n",
    "            a_prime = policy.sample_action(s_prime)\n",
    "\n",
    "            # update Q \n",
    "            Q[s][a] += alpha * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])    \n",
    "    \n",
    "            # update policy\n",
    "            policy.Q = Q\n",
    "            \n",
    "            # if final state, terminate loop\n",
    "            if final_state:\n",
    "                break\n",
    "        \n",
    "            # update current s and a for next iteration\n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: TD Weighted Importance Sampling (same as above but weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD weighted importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

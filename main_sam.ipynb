{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions used in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import time\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Windy gridworld\n",
    "Gives a reward of -1 for each step taken, while the final state is not reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m            WindyGridworldEnv\n",
       "\u001b[0;31mString form:\u001b[0m     <WindyGridworldEnv instance>\n",
       "\u001b[0;31mFile:\u001b[0m            ~/Documents/Master AI/Reinforcement Learning/RL_reproducibility/windy_gridworld.py\n",
       "\u001b[0;31mSource:\u001b[0m         \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mWindyGridworldEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscreteEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'render.modes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ansi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_limit_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnew_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnew_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_limit_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel_multi_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mis_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_position\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Wind strength\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwinds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwinds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwinds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Calculate transition probabilities\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUP\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDOWN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLEFT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# We always start in state (3, 0)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0misd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0misd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel_multi_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindyGridworldEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0moutfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ansi'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# print(self.s)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" x \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" T \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" o \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mClass docstring:\u001b[0m\n",
       "Has the following members\n",
       "- nS: number of states\n",
       "- nA: number of actions\n",
       "- P: transitions (*)\n",
       "- isd: initial state distribution (**)\n",
       "\n",
       "(*) dictionary dict of dicts of lists, where\n",
       "  P[s][a] == [(probability, nextstate, reward, done), ...]\n",
       "(**) list or array of length nS\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "env = WindyGridworldEnv()\n",
    "env??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "### Target policy (choose greedy vs non-greedy)\n",
    "Greedy policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def get_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"   \n",
    "        # for state and action only:\n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        \n",
    "        if action in max_indices:\n",
    "            prob = 1/len(max_indices)\n",
    "        else:\n",
    "            prob = 0\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        best_action = np.random.choice([i for i, j in enumerate(self.Q[obs]) if j == np.max(self.Q[obs])])\n",
    "        \n",
    "        return best_action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def get_probs(self, state, action):\n",
    "        # for one state and action \n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        # all probs are equal, give all equal probabilities\n",
    "        if len(max_indices) == len(action_probs):\n",
    "            return 1/len(max_indices)\n",
    "            \n",
    "        if action in max_indices:\n",
    "            prob = (1-self.epsilon)/len(max_indices)\n",
    "        else:\n",
    "            prob = epsilon / (len(action_probs) - len(max_indices))\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        p = np.random.uniform()\n",
    "        if p > self.epsilon:\n",
    "            # choose one of the best actions\n",
    "            action = np.random.choice([i for i, j in enumerate(self.Q[obs]) if j == np.max(self.Q[obs])])\n",
    "        else:\n",
    "            # return a random action\n",
    "            action = np.random.randint(0,4)\n",
    "                \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioural policy\n",
    "Random policy from blackjack lab. \n",
    "TODO: experiment with behavioural policies to check which yield interesting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy(object):\n",
    "    \"\"\"\n",
    "    A behavioural policy\n",
    "    \"\"\"\n",
    "    def __init__(self, nS, nA):\n",
    "        self.probs = np.ones((nS, nA)) * 1/nA\n",
    "        \n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"        \n",
    "        probs = [self.probs[s,a] for s,a in zip(states, actions)]\n",
    "        \n",
    "        return probs\n",
    "\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            state: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        p_s = self.probs[state]\n",
    "        \n",
    "        return np.random.choice(range(0,self.probs.shape[1]), p=p_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = RandomPolicy(env.nS, env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function given an env and policy\n",
    "Function to sample an episode from the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n",
    "        state after the termination is not included in the list of states.\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    # get a starting state\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    \n",
    "    # keep looping until done, don's save the terminal state\n",
    "    while not d:\n",
    "        states.append(s)\n",
    "        a = policy.sample_action(s)\n",
    "        s, r, d, _ = env.step(a)\n",
    "        \n",
    "        # save                \n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(d)\n",
    "        \n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 19551\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for random policy\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, random_policy)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 865\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for eps greedy policy\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = EpsilonGreedyPolicy(Q, epsilon=0.1)\n",
    "\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, bp)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Ordinary Importance Sampling (make it work for windy gridworld)\n",
    "Status: updated to update Q instead of V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qdefaultdict2array(Q, nA, nS):\n",
    "    Q_np = np.zeros((nS, nA))\n",
    "    for S in range(nS):\n",
    "        for A in range(nA):\n",
    "            Q_np[S][A] = Q[S][A]\n",
    "    return Q_np\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and weighted importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "#     Q = defaultdict(lambda: defaultdict(float))\n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    returns_count = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral function:\n",
    "#         behavior_policy = EpsilonGreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS), epsilon)\n",
    "        behavior_policy.Q = Q\n",
    "#         target_policy = GreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "        target_policy.Q = Q        \n",
    "    \n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save the episode length\n",
    "        episode_lens.append(len(states))\n",
    "        \n",
    "        # extract target and behavioral probabilities\n",
    "#         target_probs = target_policy.get_probs(states, actions)\n",
    "#         behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "#         target_probs = [target_policy.get_probs(states[t], actions[t]) for t in range(len(states))]\n",
    "#         behavioral_probs = [behavioral_policy.get_probs(states[t], actions[t]) for t in range(len(states))]\n",
    "            \n",
    "\n",
    "        G = 0        \n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)):\n",
    "#             print(i)\n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            returns_count[s][a] += 1 \n",
    "\n",
    "            # compute the ratio using the two probability lists\n",
    "#             ratio = np.prod([t/b for t, b in zip(target_probs[timestep:], behavioral_probs[timestep:])])\n",
    "            \n",
    "            # use every visit incremental method\n",
    "            Q[s][a] += 1/returns_count[s][a] * (W * G - Q[s][a])\n",
    "            \n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))\n",
    "            \n",
    "            if W == 0:\n",
    "                break\n",
    "    \n",
    "#     Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "    \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC: Weighted Importance Sampling\n",
    "\n",
    "##### (TODO: Eventually: merge the two functions into one with a weighted flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_weighted_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a matrix defaultdict for the Q function and the sum of weights C\n",
    "#     Q = defaultdict(lambda: defaultdict(float))\n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    C = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral function:\n",
    "#         behavior_policy = EpsilonGreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS), epsilon)\n",
    "        behavior_policy.Q = Q\n",
    "#         target_policy = GreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "        target_policy.Q = Q\n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save episode lengths\n",
    "        episode_lens.append(len(states))\n",
    "        \n",
    "        # extract target and behavioral probabilities OLD\n",
    "#         target_probs = target_policy.get_probs(states, actions)\n",
    "#         behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "       \n",
    "        # print(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "#         print(target_probs)\n",
    "        \n",
    "        \n",
    "        # initialize the return and the weight\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)): \n",
    "#             print(i)\n",
    "            # extract info of current timestep from trajectory    \n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            # add W to the sum of weights C\n",
    "            C[s][a] += W\n",
    "            \n",
    "            # update Q function incrementally\n",
    "            Q[s][a] += W/C[s][a] * (G - Q[s][a])\n",
    "            \n",
    "            # update the weight\n",
    "            # W *= (target_probs[timestep])/(behavioral_probs[timestep])\n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))\n",
    "            \n",
    "            # break out of the loop if the weights are 0\n",
    "            if W == 0:\n",
    "                break\n",
    "    \n",
    "#     Q = Qdefaultdict2array(Q, env.nA, env.nS)     \n",
    "    \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Plot the episode length over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (5000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:07<00:00, 668.46it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using weighted importance sampling (5000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:07<00:00, 680.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "np.random.seed(42)\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.1\n",
    "discount_factor = 1.0\n",
    "num_episodes = 5000\n",
    "Q = np.ones((env.nS, env.nA)) * -100\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_ordinary, mc_ordinary_epslengths = mc_ordinary_importance_sampling(env, behavioral_policy, target_policy, \n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)\n",
    "print(f\"Updating Q using weighted importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_weighted, mc_weighted_epslengths = mc_weighted_importance_sampling(env, behavioral_policy, target_policy,\n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def running_mean(vals, n=1):\n",
    "    assert n < len(vals)\n",
    "    cumvals = np.array(vals).cumsum()\n",
    "    return (cumvals[n:] - cumvals[:-n]) / n \n",
    "\n",
    "n = 5\n",
    "\n",
    "plt.plot(running_mean(mc_ordinary_epslengths, n), label=\"ordinary\")\n",
    "plt.plot(running_mean(mc_weighted_epslengths, n), label=\"weighted\")\n",
    "plt.title('Episode lengths MC')\n",
    "plt.legend()\n",
    "# plt.gca().set_ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting episode length ordinary: 16\n",
      "resulting episode length weighted: 15\n"
     ]
    }
   ],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_mc_ordinary)\n",
    "greedy_weighted = GreedyPolicy(Q_mc_weighted)\n",
    "\n",
    "ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "weighted_episode = sample_episode(env, greedy_weighted)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])}\")\n",
    "print(f\"resulting episode length weighted: {len(weighted_episode[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference\n",
    "\n",
    "TO-DO: Make N-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: Off-policy TD control. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q\n",
    "            \n",
    "        s = env.reset()\n",
    "        a = behavior_policy.sample_action(s)\n",
    "        \n",
    "        while True:\n",
    "            # Take action\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # Sample action at from next state\n",
    "            a_prime = behavior_policy.sample_action(s_prime)\n",
    "            \n",
    "            # Update weight\n",
    "            W = (target_policy.get_probs([s_prime],[a_prime]))/(behavior_policy.get_probs([s_prime],[a_prime]))\n",
    "\n",
    "            # Update Q \n",
    "            Q[s][a] += alpha * W * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])    \n",
    "            \n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "            R += r\n",
    "            i += 1 \n",
    "            \n",
    "            if final_state:\n",
    "                break\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "    Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (5000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 215/5000 [00:01<00:22, 214.83it/s]\u001b[A\n",
      " 13%|█▎        | 663/5000 [00:02<00:17, 254.54it/s]\u001b[A\n",
      " 23%|██▎       | 1166/5000 [00:03<00:12, 298.81it/s]\u001b[A\n",
      " 33%|███▎      | 1674/5000 [00:04<00:09, 340.92it/s]\u001b[A\n",
      " 44%|████▍     | 2189/5000 [00:05<00:07, 379.32it/s]\u001b[A\n",
      " 54%|█████▍    | 2703/5000 [00:06<00:05, 411.66it/s]\u001b[A\n",
      " 64%|██████▎   | 3175/5000 [00:07<00:04, 428.07it/s]\u001b[A\n",
      " 73%|███████▎  | 3645/5000 [00:08<00:03, 439.76it/s]\u001b[A\n",
      " 83%|████████▎ | 4131/5000 [00:09<00:01, 452.55it/s]\u001b[A\n",
      " 92%|█████████▏| 4600/5000 [00:10<00:00, 457.12it/s]\u001b[A\n",
      "100%|██████████| 5000/5000 [00:11<00:00, 454.00it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "np.random.seed(42)\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.05\n",
    "discount_factor = 1.0\n",
    "num_episodes = 5000\n",
    "alpha=0.5\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_td_ordinary, td_ordinary_epsstats = sarsa_ordinary_importance_sampling(env, behavioral_policy, target_policy, \n",
    "                                                                        num_episodes, discount_factor, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def running_mean(vals, n=1):\n",
    "    assert n < len(vals)\n",
    "    cumvals = np.array(vals).cumsum()\n",
    "    return (cumvals[n:] - cumvals[:-n]) / n \n",
    "\n",
    "n = 5\n",
    "\n",
    "plt.plot(running_mean(td_ordinary_epsstats[0], n), label=\"ordinary\")\n",
    "plt.title('Episode lengths TD')\n",
    "plt.legend()\n",
    "# plt.gca().set_ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting episode length ordinary: 15\n"
     ]
    }
   ],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_td_ordinary)\n",
    "\n",
    "ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: TD Weighted Importance Sampling (same as above but weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD weighted importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

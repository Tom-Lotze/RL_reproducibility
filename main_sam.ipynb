{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions used in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import time\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Windy gridworld\n",
    "Gives a reward of -1 for each step taken, while the final state is not reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m            WindyGridworldEnv\n",
       "\u001b[0;31mString form:\u001b[0m     <WindyGridworldEnv instance>\n",
       "\u001b[0;31mFile:\u001b[0m            ~/Documents/Master AI/Reinforcement Learning/RL_reproducibility/windy_gridworld.py\n",
       "\u001b[0;31mSource:\u001b[0m         \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mWindyGridworldEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscreteEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'render.modes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ansi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_limit_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnew_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnew_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_limit_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel_multi_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mis_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_position\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Wind strength\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwinds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwinds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwinds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Calculate transition probabilities\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUP\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDOWN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLEFT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# We always start in state (3, 0)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0misd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0misd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel_multi_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindyGridworldEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0moutfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ansi'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# print(self.s)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" x \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" T \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" o \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mClass docstring:\u001b[0m\n",
       "Has the following members\n",
       "- nS: number of states\n",
       "- nA: number of actions\n",
       "- P: transitions (*)\n",
       "- isd: initial state distribution (**)\n",
       "\n",
       "(*) dictionary dict of dicts of lists, where\n",
       "  P[s][a] == [(probability, nextstate, reward, done), ...]\n",
       "(**) list or array of length nS\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "env = WindyGridworldEnv()\n",
    "env??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "### Target policy (choose greedy vs non-greedy)\n",
    "Greedy policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def get_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"   \n",
    "        # for state and action only:\n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        \n",
    "        if action in max_indices:\n",
    "            prob = 1/len(max_indices)\n",
    "        else:\n",
    "            prob = 0\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        best_action = np.random.choice([i for i, j in enumerate(self.Q[obs]) if j == np.max(self.Q[obs])])\n",
    "        \n",
    "        return best_action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def get_probs(self, state, action):\n",
    "        # for one state and action \n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        # all probs are equal, give all equal probabilities\n",
    "        if len(max_indices) == len(action_probs):\n",
    "            return 1/len(max_indices)\n",
    "            \n",
    "        if action in max_indices:\n",
    "            prob = (1-self.epsilon)/len(max_indices)\n",
    "        else:\n",
    "            prob = epsilon / (len(action_probs) - len(max_indices))\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        p = np.random.uniform()\n",
    "        if p > self.epsilon:\n",
    "            # choose one of the best actions\n",
    "            action = np.random.choice([i for i, j in enumerate(self.Q[obs]) if j == np.max(self.Q[obs])])\n",
    "        else:\n",
    "            # return a random action\n",
    "            action = np.random.randint(0,4)\n",
    "                \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioural policy\n",
    "Random policy from blackjack lab. \n",
    "TODO: experiment with behavioural policies to check which yield interesting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy(object):\n",
    "    \"\"\"\n",
    "    A behavioural policy\n",
    "    \"\"\"\n",
    "    def __init__(self, nS, nA):\n",
    "        self.probs = np.ones((nS, nA)) * 1/nA\n",
    "        \n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"        \n",
    "        probs = [self.probs[s,a] for s,a in zip(states, actions)]\n",
    "        \n",
    "        return probs\n",
    "\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            state: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        p_s = self.probs[state]\n",
    "        \n",
    "        return np.random.choice(range(0,self.probs.shape[1]), p=p_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = RandomPolicy(env.nS, env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function given an env and policy\n",
    "Function to sample an episode from the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n",
    "        state after the termination is not included in the list of states.\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    # get a starting state\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    \n",
    "    # keep looping until done, don's save the terminal state\n",
    "    while not d:\n",
    "        states.append(s)\n",
    "        a = policy.sample_action(s)\n",
    "        s, r, d, _ = env.step(a)\n",
    "        \n",
    "        # save                \n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(d)\n",
    "        \n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 19551\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for random policy\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, random_policy)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 865\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for eps greedy policy\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = EpsilonGreedyPolicy(Q, epsilon=0.1)\n",
    "\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, bp)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Ordinary Importance Sampling (make it work for windy gridworld)\n",
    "Status: updated to update Q instead of V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qdefaultdict2array(Q, nA, nS):\n",
    "    Q_np = np.zeros((nS, nA))\n",
    "    for S in range(nS):\n",
    "        for A in range(nA):\n",
    "            Q_np[S][A] = Q[S][A]\n",
    "    return Q_np\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "#     Q = defaultdict(lambda: defaultdict(float))\n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    returns_count = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral function:\n",
    "#         behavior_policy = EpsilonGreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS), epsilon)\n",
    "        behavior_policy.Q = Q\n",
    "#         target_policy = GreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "        target_policy.Q = Q        \n",
    "    \n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save the episode length\n",
    "        episode_lens.append(len(states))\n",
    "        \n",
    "        # extract target and behavioral probabilities\n",
    "#         target_probs = target_policy.get_probs(states, actions)\n",
    "#         behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "#         target_probs = [target_policy.get_probs(states[t], actions[t]) for t in range(len(states))]\n",
    "#         behavioral_probs = [behavioral_policy.get_probs(states[t], actions[t]) for t in range(len(states))]\n",
    "            \n",
    "\n",
    "        G = 0        \n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)):\n",
    "#             print(i)\n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            returns_count[s][a] += 1 \n",
    "\n",
    "            # compute the ratio using the two probability lists\n",
    "#             ratio = np.prod([t/b for t, b in zip(target_probs[timestep:], behavioral_probs[timestep:])])\n",
    "            \n",
    "            # use every visit incremental method\n",
    "            Q[s][a] += 1/returns_count[s][a] * (W * G - Q[s][a])\n",
    "            \n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))\n",
    "            \n",
    "            if W == 0:\n",
    "                break\n",
    "    \n",
    "#     Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "    \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC: Weighted Importance Sampling\n",
    "\n",
    "##### (TODO: Eventually: merge the two functions into one with a weighted flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_weighted_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and weighted importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a matrix defaultdict for the Q function and the sum of weights C\n",
    "#     Q = defaultdict(lambda: defaultdict(float))\n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    C = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral function:\n",
    "#         behavior_policy = EpsilonGreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS), epsilon)\n",
    "        behavior_policy.Q = Q\n",
    "#         target_policy = GreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "        target_policy.Q = Q\n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save episode lengths\n",
    "        episode_lens.append(len(states))\n",
    "        \n",
    "        # extract target and behavioral probabilities OLD\n",
    "#         target_probs = target_policy.get_probs(states, actions)\n",
    "#         behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "       \n",
    "        # print(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "#         print(target_probs)\n",
    "        \n",
    "        \n",
    "        # initialize the return and the weight\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)): \n",
    "#             print(i)\n",
    "            # extract info of current timestep from trajectory    \n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            # add W to the sum of weights C\n",
    "            C[s][a] += W\n",
    "            \n",
    "            # update Q function incrementally\n",
    "            Q[s][a] += W/C[s][a] * (G - Q[s][a])\n",
    "            \n",
    "            # update the weight\n",
    "            # W *= (target_probs[timestep])/(behavioral_probs[timestep])\n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))\n",
    "            \n",
    "            # break out of the loop if the weights are 0\n",
    "            if W == 0:\n",
    "                break\n",
    "    \n",
    "#     Q = Qdefaultdict2array(Q, env.nA, env.nS)     \n",
    "    \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Plot the episode length over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (5000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:07<00:00, 668.46it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using weighted importance sampling (5000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:07<00:00, 680.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "np.random.seed(42)\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.1\n",
    "discount_factor = 1.0\n",
    "num_episodes = 5000\n",
    "Q = np.ones((env.nS, env.nA)) * -100\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_ordinary, mc_ordinary_epslengths = mc_ordinary_importance_sampling(env, behavioral_policy, target_policy, \n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)\n",
    "print(f\"Updating Q using weighted importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_weighted, mc_weighted_epslengths = mc_weighted_importance_sampling(env, behavioral_policy, target_policy,\n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de3xV5Z3v8c9v5wrhDiEGghAVrBEUEbmIIGpbrLYqtkyhY6vHqjOjnjo9M22xTqtO9dR2GK21Ry2jVbRVx0u1atURUQregKBSuQpIgEiEEG6B3LN/54+9oDvZAUISkrD29/16xb32s5611vPE8N1rP+tm7o6IiCSHSEc3QERE2o9CX0QkiSj0RUSSiEJfRCSJKPRFRJKIQl9EJIko9KVTMbNXzezKNl7nbWb2+xYuW2RmX2zL9jRzu0PMzM0stb23LeGm0Jc2FwRlpZntjfv5TXOWdfevuPuco93GzuZofbiY2eTgw+OPjcpPD8rnx5WZmX3PzJab2T4zKzazZ8xsRFu3SzqO9iLkaPmau7/R0Y0QAEqBs82sr7uXBWVXAp80qncvcDFwLfAOkAJMDco+bqe2ylGmPX1pV2Z2lZm9Y2b3mdluM1ttZhfEzZ9vZtcE0yeZ2V+CetvN7L/j6p1tZkuCeUvM7Oy4efnBcuVmNhfo16gN48zsXTPbZWbLzGxyM9seMbOZZrbezMrM7Gkz6xPM2z8cc6WZbQrae0vcsl3MbI6Z7TSzVWb2QzMrDuY9DhwPvBR8K/ph3Gb//iDrG2NmhWa2x8y2mtndh2h6DfACMD1YNgX4O+APcesbCtwAzHD3N9292t0r3P0P7n5Xc34/cmxQ6EtHGAt8SiyMbwX+uD88G/kZ8DrQG8gD7gMI6v4Z+DXQF7gb+LOZ9Q2WewJYGqz/Z8T2agmWHRgsewfQB/hX4Dkzy25Gu78HXAacCwwAdgL/r1Gdc4CTgQuAn5rZKUH5rcAQ4ATgS8AV+xdw928Dm4h9O+rm7r9sxvruBe519x7AicDTh2n7Y8B3gukpwApgS9z8C4Bid198mPXIMU6hL0fLC8Ge9P6fa+PmbQN+5e617v7fwBpiQwiN1QKDgQHuXuXubwflFwNr3f1xd69z9yeB1cDXzOx44CzgJ8He6gLgpbh1XgG84u6vuHvU3ecChcBFzejTPwC3uHuxu1cDtwHfaHSw9XZ3r3T3ZcAy4PSg/O+A/+vuO929mNgHVnMcbH21wElm1s/d97r7+4daibu/C/Qxs5OJhf9jjar0BUqa2SY5hin05Wi5zN17xf38V9y8z7zhnf42EttzbuyHgAGLzWyFmV0dlA8Ilom3ERgYzNvp7vsazdtvMDAt/gOJ2N50bjP6NBh4Pm65VUA9kBNX5/O46QqgW1ybN8fNi58+lIOt77vAMGB1MLz11Was63HgRuA84PlG88po3u9AjnEKfekIA83M4t4fT8OhBgDc/XN3v9bdBxDby77fzE4K6g5uVP144DNie6u9zSyr0bz9NgOPN/pAymrmuPVm4CuNls1098+asWwJsSGq/QY1mn9Et7t197XuPgPoD/wCeLZRn5vyOHA9sW86FY3mzQPyzGz0kbRDjj0KfekI/YHvmVmamU0DTgFeaVzJzKaZ2f6g3EksGOuDusPM7Ftmlmpm3wQKgJfdfSOx4ZrbzSzdzM4Bvha32t8TGwaaYmYpZpYZnNYYH8gH8yBwp5kNDtqXbWaXNrPPTwM3m1nv4LjCjY3mbyU23t8sZnaFmWW7exTYFRTXH2oZd99A7HjELU3MWwvcDzwZ/D7Sg9/NdDOb2dx2Seen0JejZf+ZKPt/4ocTFgFDge3AncA34k4ljHcWsMjM9gIvAje5+4ag7leBfyE2LPFD4Kvuvj1Y7lvEDhbvIHYA9cD4tbtvBi4FfkzsVMbNwA9o3r+Fe4N2vG5m5cD7wXaa49+BYmAD8AbwLFAdN//nwL8FQ0f/2oz1XQisCH439wLT3b3qcAu5+9vunvCtKvA94DfEDk7vAtYTO2XzpYPUl2OQ6SEq0p7M7CrgGnc/p6Pb0pHM7J+IBfW5Hd0WSS7a0xdpB2aWa2YTgnP9Tyb2LaXxwVSRo05X5Iq0j3Tgt0A+saGTp4iNoYu0Kw3viIgkEQ3viIgkkU4/vNOvXz8fMmRIRzdDROSYsnTp0u3unnB7kU4f+kOGDKGwsLCjmyEickwxs8ZXrQMa3hERSSoKfRGRJKLQFxFJIp1+TF9Ewqm2tpbi4mKqqg579wg5hMzMTPLy8khLS2tWfYW+iHSI4uJiunfvzpAhQ2h401VpLnenrKyM4uJi8vPzm7WMhndEpENUVVXRt29fBX4rmBl9+/Y9om9LCn0R6TAK/NY70t9haEN/zrtFvLTsYHeQFRFJTqEN/d+/v5FXl+uRnyLSth599FFuvDH2DJwHH3yQxx5r/Ljhzk0HckVEDsLdcXcikab3j//xH/+xTbZTX19PSkpKm6zrcEK7py8i0hx33303w4cPZ/jw4fzqV7+iqKiIU045heuvv55Ro0axefNmHnnkEYYNG8a5557LO++8c2DZ2267jVmzZgEwefJkfvSjHzFmzBiGDRvGwoULASgqKmLixImMGjWKUaNG8e677wIwf/58zjvvPL71rW8xYsQIfvKTn3DvvfceWPctt9zCr3/96zbvr/b0RaTD3f7SClZu2dOm6ywY0INbv3bqIessXbqURx55hEWLFuHujB07lnPPPZc1a9bwyCOPcP/991NSUsKtt97K0qVL6dmzJ+eddx5nnHFGk+urq6tj8eLFvPLKK9x+++288cYb9O/fn7lz55KZmcnatWuZMWPGgfuJLV68mOXLl5Ofn09RURGXX345N910E9FolKeeeorFixe36e8EQh76elSAiBzK22+/zdSpU8nKygLg8ssvZ+HChQwePJhx48YBsGjRIiZPnkx2duyGld/85jf55JNPmlzf5ZdfDsCZZ55JUVERELsI7cYbb+Sjjz4iJSWlwbJjxow5cH79kCFD6Nu3Lx9++CFbt27ljDPOoG/fvm3e59CGvs4EEzl2HG6P/Gg52EOk9n8I7Nfc0yIzMjIASElJoa6uDoB77rmHnJwcli1bRjQaJTMz86Dbueaaa3j00Uf5/PPPufrqq5vdjyOhMX0RSVqTJk3ihRdeoKKign379vH8888zceLEBnXGjh3L/PnzKSsro7a2lmeeeeaItrF7925yc3OJRCI8/vjj1NfXH7Tu1KlTee2111iyZAlTpkxpUZ8OJ7R7+iIihzNq1CiuuuoqxowZA8T2tHv37t2gTm5uLrfddhvjx48nNzeXUaNGHTK4G7v++uv5+te/zjPPPMN5552XsHcfLz09nfPOO49evXodtbN5Ov0zckePHu0teYjKl+/5Cydmd+OBK848Cq0SkdZatWoVp5xySkc3o1OJRqOMGjWKZ555hqFDhzZ7uaZ+l2a21N1HN66r4R0RkU5g5cqVnHTSSVxwwQVHFPhHKtTDO538S4yIyAEFBQV8+umnR307od3TN3T6johIY6ENfRERSXTY0Dez35nZNjNbHlfWx8zmmtna4LV33LybzWydma0xsylx5Wea2cfBvF+b7qkqItLumrOn/yhwYaOymcA8dx8KzAveY2YFwHTg1GCZ+81s/3lHDwDXAUODn8brFBGRo+ywoe/uC4AdjYovBeYE03OAy+LKn3L3anffAKwDxphZLtDD3d/z2Dmij8Utc9Q4OpIrIm3vmmuuYeXKlYesc9VVV/Hss88mlBcVFfHEE08c8TYPtr4j1dIx/Rx3LwEIXvsH5QOBzXH1ioOygcF04/KjRoNHInK0PPTQQxQUFLRo2ZaGfltp6wO5TUWtH6K86ZWYXWdmhWZWWFpa2maNExGJ98tf/vLA7Yu///3vc/755wMwb948rrjiCl5//XXGjx/PqFGjmDZtGnv37gVit1Hef9Howw8/zLBhw5g8eTLXXnvtgQesACxYsICzzz6bE0444cBe+syZM1m4cCEjR47knnvuob6+nh/84AecddZZnHbaafz2t78FYvcFuvHGGykoKODiiy9m27ZtbdLnlp6nv9XMct29JBi62d+aYmBQXL08YEtQntdEeZPcfTYwG2JX5LawjSJyrHh1Jnz+cduu87gR8JW7Dlll0qRJ/Od//iff+973KCwspLq6mtraWt5++21GjBjBHXfcwRtvvEFWVha/+MUvuPvuu/npT396YPktW7bws5/9jA8++IDu3btz/vnnc/rppx+YX1JSwttvv83q1au55JJL+MY3vsFdd93FrFmzePnllwGYPXs2PXv2ZMmSJVRXVzNhwgS+/OUv8+GHH7JmzRo+/vhjtm7dSkFBQZvchK2le/ovAlcG01cCf4orn25mGWaWT+yA7eJgCKjczMYFZ+18J24ZEZEOceaZZ7J06VLKy8vJyMhg/PjxFBYWsnDhQrp06cLKlSuZMGECI0eOZM6cOWzcuLHB8osXL+bcc8+lT58+pKWlMW3atAbzL7vsMiKRCAUFBWzdurXJNrz++us89thjjBw5krFjx1JWVsbatWtZsGABM2bMICUlhQEDBhz4FtJah93TN7MngclAPzMrBm4F7gKeNrPvApuAaQDuvsLMngZWAnXADe6+/85E/0TsTKAuwKvBz1GlK3JFjhGH2SM/WtLS0hgyZAiPPPIIZ599NqeddhpvvfUW69evJz8/ny996Us8+eSTB13+cPcu23+r5UPVdXfuu+++hLtqvvLKK82+pfORaM7ZOzPcPdfd09w9z90fdvcyd7/A3YcGrzvi6t/p7ie6+8nu/mpceaG7Dw/m3eid/U5vIpIUJk2axKxZs5g0aRITJ07kwQcfZOTIkYwbN4533nmHdevWAVBRUZHw8JQxY8bwl7/8hZ07d1JXV8dzzz132O11796d8vLyA++nTJnCAw88QG1tLQCffPIJ+/btY9KkSTz11FPU19dTUlLCW2+91Sb9DfW9d0REDmfixInceeedjB8/nqysLDIzM5k4cSLZ2dk8+uijzJgxg+rqagDuuOMOhg0bdmDZgQMH8uMf/5ixY8cyYMAACgoK6Nmz5yG3d9ppp5Gamsrpp5/OVVddxU033URRURGjRo3C3cnOzuaFF15g6tSpvPnmm4wYMeLA83nbQmhvrXzhrxZwfJ+uzP5Owp1FRaQTCMutlffu3Uu3bt2oq6tj6tSpXH311UydOrVd26BbK4uItJPbbruNkSNHMnz4cPLz87nssqN+3WmraHhHRKQVZs2a1dFNOCKh3tPv3ANXItLZh5ePBUf6Owxt6OsmniKdW2ZmJmVlZQr+VnB3ysrKyMzMbPYyGt4RkQ6Rl5dHcXExutVK62RmZpKXl3f4igGFvoh0iLS0NPLz8zu6GUkntMM7IiKSKNShr6FCEZGGQhv6OowrIpIotKEvIiKJFPoiIklEoS8ikkRCHvo6kisiEi+0oa8LckVEEoU29EVEJJFCX0QkiSj0RUSSiEJfRCSJhDr0dRsGEZGGQhv6OntHRCRRaENfREQSKfRFRJKIQl9EJImEOvR1HFdEpKHQhr7pjvoiIglCG/oiIpKoVaFvZt83sxVmttzMnjSzTDPrY2ZzzWxt8No7rv7NZrbOzNaY2ZTWN19ERI5Ei0PfzAYC3wNGu/twIAWYDswE5rn7UGBe8B4zKwjmnwpcCNxvZimta76IiByJ1g7vpAJdzCwV6ApsAS4F5gTz5wCXBdOXAk+5e7W7bwDWAWNauf1Dcl2SKyLSQItD390/A2YBm4ASYLe7vw7kuHtJUKcE6B8sMhDYHLeK4qAsgZldZ2aFZlZYWlra0iaKiEgjrRne6U1s7z0fGABkmdkVh1qkibImd8Xdfba7j3b30dnZ2S1sX4sWExEJtdYM73wR2ODupe5eC/wROBvYama5AMHrtqB+MTAobvk8YsNBIiLSTloT+puAcWbW1cwMuABYBbwIXBnUuRL4UzD9IjDdzDLMLB8YCixuxfZFROQIpbZ0QXdfZGbPAh8AdcCHwGygG/C0mX2X2AfDtKD+CjN7GlgZ1L/B3etb2X4RETkCLQ59AHe/Fbi1UXE1sb3+purfCdzZmm0eCZ27IyLSUGivyNVxXBGRRKENfRERSaTQFxFJIgp9EZEkEurQ110YREQaCm/o65JcEZEE4Q19ERFJoNAXEUkiCn0RkSSi0BcRSSKhDn2dvCMi0lBoQ1/n7oiIJApt6IuISCKFvohIElHoi4gkkVCHvus+DCIiDYQ29HUXBhGRRKENfRERSaTQFxFJIgp9EZEkotAXEUkioQ19HccVEUkU2tAXEZFECn0RkSSi0BcRSSIKfRGRJBLq0NddGEREGgpt6JvuwyAikqBVoW9mvczsWTNbbWarzGy8mfUxs7lmtjZ47R1X/2YzW2dma8xsSuubLyIiR6K1e/r3Aq+5+xeA04FVwExgnrsPBeYF7zGzAmA6cCpwIXC/maW0cvsiInIEWhz6ZtYDmAQ8DODuNe6+C7gUmBNUmwNcFkxfCjzl7tXuvgFYB4xp6fZFROTItWZP/wSgFHjEzD40s4fMLAvIcfcSgOC1f1B/ILA5bvnioCyBmV1nZoVmVlhaWtriBroejS4i0kBrQj8VGAU84O5nAPsIhnIOoqkjq02msrvPdvfR7j46Ozu7RY3TYVwRkUStCf1ioNjdFwXvnyX2IbDVzHIBgtdtcfUHxS2fB2xpxfZFROQItTj03f1zYLOZnRwUXQCsBF4ErgzKrgT+FEy/CEw3swwzyweGAotbun0RETlyqa1c/n8DfzCzdOBT4H8R+yB52sy+C2wCpgG4+woze5rYB0MdcIO717dy+yIicgRaFfru/hEwuolZFxyk/p3Ana3Z5pHQFbkiIg2F+Ircjm6BiEjnE9rQFxGRRAp9EZEkotAXEUkiCn0RkSQS6tDX2TsiIg2FNvRNN2IQEUkQ2tAXEZFECn0RkSSi0BcRSSKhDn3dT19EpKHwhr6O44qIJAhv6IuISAKFvohIElHoi4gkkVCHvq7IFRFpKLShr+O4IiKJQhv6IiKSSKEvIpJEFPoiIklEoS8ikkRCHfo6eUdEpKHQhr7p9B0RkQShDX0REUmk0BcRSSIKfRGRJJLa0Q04Wm7Y+UuqyATGd3RTREQ6jdCG/sTKNzu6CSIinU6rh3fMLMXMPjSzl4P3fcxsrpmtDV57x9W92czWmdkaM5vS2m2LiMiRaYsx/ZuAVXHvZwLz3H0oMC94j5kVANOBU4ELgfvNLKUNti8iIs3UqtA3szzgYuChuOJLgTnB9Bzgsrjyp9y92t03AOuAMa3ZvoiIHJnW7un/CvghEI0ry3H3EoDgtX9QPhDYHFevOChLYGbXmVmhmRWWlpa2qGGr0oezPP20Fi0rIhJWLQ59M/sqsM3dlzZ3kSbKmrxTgrvPdvfR7j46Ozu7pS3E9BQVEZEGWnP2zgTgEjO7CMgEepjZ74GtZpbr7iVmlgtsC+oXA4Pils8DtrRi+4fkGNbgC4iIiLR4T9/db3b3PHcfQuwA7ZvufgXwInBlUO1K4E/B9IvAdDPLMLN8YCiwuMUtP1z7jtaKRUSOYUfjPP27gKfN7LvAJmAagLuvMLOngZVAHXCDu9cfhe0HDFP0i4g00Cah7+7zgfnBdBlwwUHq3Qnc2RbbPGybTGP6IiKN6d47IiJJJLSh7xreERFJENrQj50hqtAXEYkX2tDXnr6ISKIQhz6YMl9EpIHQhr6Gd0REEoU29N00vCMi0lhoQ7/pW/2IiCS30Ia+DuSKiCQKceij0BcRaSS0oa8DuSIiiUIb+rHhHRERiRfa0Mc0vCMi0lhoQ98x0F02RUQaCHXoa09fRKSh0IY+GtMXEUkQ2tD3uP+KiEhMaENfj0sUEUkU2tB3De6IiCQIbeiD7r4jItJYaEPfdUWuiEiC0IY+6OIsEZHGQhv6btrTFxFpLLShr/P0RUQShTb0HcN0GwYRkQbCHfoa3hERaSC0oS8iIolCG/ra0xcRSdTi0DezQWb2lpmtMrMVZnZTUN7HzOaa2drgtXfcMjeb2TozW2NmU9qiA4doIDp7R0Skodbs6dcB/+LupwDjgBvMrACYCcxz96HAvOA9wbzpwKnAhcD9ZpbSmsYfip6RKyKSqMWh7+4l7v5BMF0OrAIGApcCc4Jqc4DLgulLgafcvdrdNwDrgDEt3f7h6ZRNEZHG2mRM38yGAGcAi4Acdy+B2AcD0D+oNhDYHLdYcVDW1PquM7NCMyssLS1tUZs0pi8ikqjVoW9m3YDngH929z2HqtpEWZOp7O6z3X20u4/Ozs5uacsOtnoRkaTVqtA3szRigf8Hd/9jULzVzHKD+bnAtqC8GBgUt3gesKU12z9M4zBlvohIA605e8eAh4FV7n533KwXgSuD6SuBP8WVTzezDDPLB4YCi1u6/eZR6ouIxEttxbITgG8DH5vZR0HZj4G7gKfN7LvAJmAagLuvMLOngZXEzvy5wd3rW7H9Q7OIxvRFRBppcei7+9sc/DklFxxkmTuBO1u6zSOn0BcRiRfaK3K1py8ikijEod/RDRAR6XzCG/o6T19EJEF4Q98iup++iEgj4Q19ERFJENrQj11GICIi8UIb+qC7bIqINBbe0LcIOk9fRKSh0Ia+mfb0RUQaC23oQ0Sn6ouINBLe0NfjEkVEEoQ39HVxlohIgtCGvpkelygi0lhoQ//UXW/RlSqo3NnRTRER6TRCG/q9arfGJrauSJy58V3YtKh9GyQi0gm05iEqx4aq3Yllj3wl9npbE/NEREIstHv6zbLkIdi+rqNbISLSbkIb+nvS+scmood4IuOf/wUeOr99GiQi0gmENvSfHfYfsYnXZh66YlPDPyIiIRXa0K9Ny4pN7Pms4Yzqve3fGBGRTiK0od81M7PpGRv+0r4NERHpREIb+lldDhL6kfCfsCQicjChDf3U1LSmZ9TXtG9DjnV7S6FqT0e3QkTaSGhDPz3tIHv0Oze2b0OOdQ9d8LfrGkTkmBfa0LfMHn97s201VOyITb9+S8OKOSPar1FHS9l6qKs+OuvetRG2Lj866xaRdhfa0N9e6fyx/pzYm/vHwi/z4baeCfWitRU8MH89uytqKSotZ/aC9bjH7s7p7vz81VXsq67jvfVllO2txveUULyzgqUbd3DfvLWUllezY18NSzfu5KVlWxh6yyvc/Me/UlVbT1VtPXX1Ucr2VlO3bycs/i+qamoBqKqtZ9ueKgC27ak6sE3cD1xbsK+6jj1VteCOR6N/qwOwZwvU1cQ+0O4bBXf0hwWzEvq3eMOOA9up3bYWVr/SYP7+dgLw20nw3LVQG6vPJ683+/ftuz9j5ZrVDdu43/o38ZqKhmWv/xv89tyG62hq2Raoq49SXZd4fUbC+ss/P+y6Wt2mDQub9e3ywP+D1ojWg3uTfW/KwfpWVVvf5Lza+mirmteUzTsqiEYP/ztOaE+0HvaVHXz+4USjUFt50Nn1USdaHz30dT7N2K67H7ZOZU192/z/byZrq39oR8vo0aO9sLDwiJfbW13Hwz+7hptSnz8KrYJS70kq9fS22CmgS6NDOTOyFoBn6ibxVnQk3ayScu/KtJS/cH7KRwC8XD+OfCvh+fpz2Oa9mJX2IFu8H4uip9DfdnJeyrID24i6UUMqmRb7oJhafTv/lvb7A9sBeLTuy1yVmhjOm6LZbKcn5d6VFT6EPd6VmWlPAbA+mstr0bO4IfVF3o+ewpn2CWl26D+6omgOQyJbmVs/ipfqx7PMT+TvUubzpchSPowO5Zup8xOW+XntDCakrmSSxfo026fym+qLmBBZzgPp9wLwDzX/zO1pczjOdvJS/Tg2+HEsiX6Bx9PvopTeZESclLQ0sqpLea7PtXyydS+Gc3nKQoZFYqfjbvVepFHHfXVT6U4l56R8zJjIGrZ6L96JDmdFdDDnRv5Kru3g7rpv8ED6vbxSP4aLUhZT5t15IeMSpqe8RVblFgAeqvsKObaTr6W8z9213+AfUl/i9ehoRqRu5kM7la61ZVR4JkMin7MyOphl9gVm2P9wVuQTSrwPRdHjqMfY5Dl8K/VNAD6InsSoyN+u/l4U/QKbIoMYl7kRr9pDSbQXv6u7kG+lvMnS6DCmpCzh1Ejsw6KcLnyQdS5n1iyhW20s6J6tn0SXSD0n2yYK606km1Xx1ZT3m/x/9+ce07Gdn7LJc/ggehKz0+8BYHH0ZMZE1rAgfRKTahYA8P3cx/ikqBgjyk/THiffPifb/nYty+roII6zHSxOOZPa2ipGpmxgIKUH5r9XX8D4lJUAzKqdxr+mPXNgXrH3Y3O0P9WksY8MXqyfwIjIp5wTWc7IyPoGbf4oegLrfSCZvXLIiFYyae9rpAd/o3s9k25WldDP+fWnk2M7OSWyice4mO/wZ9ZE81jrAyn1XpwRWcvIyKf8e+23+Wna4wAH/g4+jQyhZ3QX26LdGWhl3F93yYF/L39fczN/SP/5ge3UkUoqdWxMzeeFqpFcHnmbD/0kLkl5D4C30s/luKoNnBLZBECNp/CYX8Q1kZeoTenC7VXT6We7SaOObHazxgfx9ZSFLIiO4Nspc0mnjmdTLyYjfxxT//56zFp2v2AzW+ruoxPKwxr6AAtWb2He43eRYzv5cqSQkyKxf9T31l3OGbaWSSkft2VTRUTa1Narl5Bz/LAWLdtpQt/MLgTuBVKAh9z9rkPVb03oQ+zr1ed7qoiY0b97Brsra3GH1BSDaD2Rz5aQXrmVHZG+5OSdRPVnf6X2o6fpdsoF7Br0RXrtWIZ3y4HyEnznJiyngN316XTZuYb0orewXRup79IP751PbVp30mt3Y0ULieScCqWrqbcUUrbGPlz88v/C1rxKdP2bRKp2AVCXN45ozzzKd++iJmsAPWwfnw2eSvelD9CrSwp1ad3ZkXM2fba9T2oEbNdGqlOy2Jvam57l68iwetJ2reeT4f+Hys8+5vSdcwHY03Uw3Ss2UX3iFDI+fYN9Q75Ifc/BZOzbQlrRfPam9qRnZTEA7+VdzdDMPUSOH0PalkK6de/J5tJdpGf1os5S6bL9Y2p2b6VXSjWZezdTk9aD8gETqOl/Or0qNxjFFfEAAAdLSURBVMLnH9N1e6yPlcMuYV+kB+mb36YiK4/yPiPI2b4Iry5nbc8JZKQ4x7GdnRV1DCv9HwBqUrqyMucS+g+fTI/3/oOuFZ+xrufZnLDnffb2PpXU+kqWZ40nIyONYSUvk7VvE6VpA1hz+o8Yse63eM9BRHZvJrNiC+k1u4im96C6z8mkVW0nddcG9hw3nrTtK8mMVmLRv529tbd3AQwYSbcVT1CV1os6S2NbziQyqz6nV7du7KiJ0D3vVBbbaUS3r2XSjueorq0lpaacjGgF6bV7qOsxiIqKCrrX7WTXsGlsr0lhaNET1AyZTOXOz/nr8d/m9Ohq0nasYVXuVCqjqQzb/jqZezaSmpZKtHoftWk9SRk4kq5b3qU+kk769pXUpfekNpIJkRQq0/uSUbmNrOqtlJz1I3KX/ILa9F5Eu+dS2r2AAZtfpqLXMCIVZViXXkTyJ1Ad6cLm0l1srzR6RyoYtudddgy+iPTeuezY8FcG7vmI6l4nsrfHSfTZ+BofHf8d+kW3k1m+ie5VJVSk9aK8/1mQnkVu2XtUZg0ie8ubeLfjiFbsZE9aP7L2baJk6AxyazayzzPYVxOl36ZXiXbPo/uOv7Kt10i6WC2W1ZdoTRVdqrfhlsq+zONIrdlNz+0fsPn0myj77FPMjL09TqSr7yOv7D2yd8f+nurSe1CaO5n6zD508Qq8qpztWUPpUrWN1D6DqC9dS5e8kVRv30DO7mXsqaym974NfHLaD0nf9xnp/YfSo7qEqs/XsLfrIHqk1JCenkHq5veo7Z5H6o61lHc/gfLycnr06Y9F6+hXvZmKrgNI2fkpWwdcQE7lenbljOPtLXBOzUIydm9gU9YIvtBlN5mbF7C36yDKU3rRp7KIzb3Hkt53MNRWkpEaocoyqKyo5AubnqCmaw7R9O7szhxI7549qdizg8zMTPZWVpNVXkRx5lAyUyPYjvVEck6hfsQ0Bo29vMXZ1ylC38xSgE+ALwHFwBJghruvPNgyrQ19EZFkdLDQb+8DuWOAde7+qbvXAE8Bl7ZzG0REklZ7h/5AYHPc++KgrAEzu87MCs2ssLS0tPFsERFpofYO/aYOQyeML7n7bHcf7e6js7Oz26FZIiLJob1DvxgYFPc+D9jSzm0QEUla7R36S4ChZpZvZunAdODFdm6DiEjSatdbTrp7nZndCPwPsVM2f+fuTTy5XEREjoZ2v8+wu78CvHLYiiIi0uZCe+8dERFJ1Olvw2BmpUBL74fcD9jehs05FqjPyUF9Tg6t6fNgd084/bHTh35rmFlhU1ekhZn6nBzU5+RwNPqs4R0RkSSi0BcRSSJhD/3ZHd2ADqA+Jwf1OTm0eZ9DPaYvIiINhX1PX0RE4ij0RUSSSChD38wuNLM1ZrbOzGZ2dHtaw8x+Z2bbzGx5XFkfM5trZmuD195x824O+r3GzKbElZ9pZh8H835tLX3wZjsws0Fm9paZrTKzFWZ2U1Ae2n6bWaaZLTazZUGfbw/KQ9tniD1Yycw+NLOXg/eh7i+AmRUF7f3IzAqDsvbr9/6ntYflh9g9fdYDJwDpwDKgoKPb1Yr+TAJGAcvjyn4JzAymZwK/CKYLgv5mAPnB7yElmLcYGE/s9tavAl/p6L4dos+5wKhgujuxp60VhLnfQfu6BdNpwCJgXJj7HLT1/wBPAC8nw9920N4ioF+jsnbrdxj39EP1dC53XwDsaFR8KTAnmJ4DXBZX/pS7V7v7BmAdMMbMcoEe7v6ex/5aHotbptNx9xJ3/yCYLgdWEXvYTmj77TF7g7dpwY8T4j6bWR5wMfBQXHFo+3sY7dbvMIZ+s57OdYzLcfcSiAUk0D8oP1jfBwbTjcs7PTMbApxBbM831P0Ohjo+ArYBc9097H3+FfBDIBpXFub+7ufA62a21MyuC8rard/tfpfNdtCsp3OF1MH6fkz+TsysG/Ac8M/uvucQQ5ah6Le71wMjzawX8LyZDT9E9WO6z2b2VWCbuy81s8nNWaSJsmOmv41McPctZtYfmGtmqw9Rt837HcY9/WR4OtfW4Osdweu2oPxgfS8OphuXd1pmlkYs8P/g7n8MikPfbwB33wXMBy4kvH2eAFxiZkXEhmDPN7PfE97+HuDuW4LXbcDzxIak263fYQz9ZHg614vAlcH0lcCf4sqnm1mGmeUDQ4HFwdfFcjMbFxzh/07cMp1O0MaHgVXufnfcrND228yygz18zKwL8EVgNSHts7vf7O557j6E2L/RN939CkLa3/3MLMvMuu+fBr4MLKc9+93RR7KPxg9wEbEzPtYDt3R0e1rZlyeBEqCW2Kf7d4G+wDxgbfDaJ67+LUG/1xB3NB8YHfxxrQd+Q3A1dmf8Ac4h9lX1r8BHwc9FYe43cBrwYdDn5cBPg/LQ9jmuvZP529k7oe4vsbMKlwU/K/bnU3v2W7dhEBFJImEc3hERkYNQ6IuIJBGFvohIElHoi4gkEYW+iEgSUeiLiCQRhb6ISBL5/1vwADlZ5/V7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def running_mean(vals, n=1):\n",
    "    assert n < len(vals)\n",
    "    cumvals = np.array(vals).cumsum()\n",
    "    return (cumvals[n:] - cumvals[:-n]) / n \n",
    "\n",
    "n = 5\n",
    "\n",
    "plt.plot(running_mean(mc_ordinary_epslengths, n), label=\"ordinary\")\n",
    "plt.plot(running_mean(mc_weighted_epslengths, n), label=\"weighted\")\n",
    "plt.title('Episode lengths MC')\n",
    "plt.legend()\n",
    "# plt.gca().set_ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting episode length ordinary: 16\n",
      "resulting episode length weighted: 15\n"
     ]
    }
   ],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_mc_ordinary)\n",
    "greedy_weighted = GreedyPolicy(Q_mc_weighted)\n",
    "\n",
    "ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "weighted_episode = sample_episode(env, greedy_weighted)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])}\")\n",
    "print(f\"resulting episode length weighted: {len(weighted_episode[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference\n",
    "\n",
    "To-Do: Check n-step implementation so we can drop the first sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: Off-policy TD control. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q\n",
    "            \n",
    "        s = env.reset()\n",
    "        a = behavior_policy.sample_action(s)\n",
    "        \n",
    "        while True:\n",
    "            # Take action\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # Sample action at from next state\n",
    "            a_prime = behavior_policy.sample_action(s_prime)\n",
    "            \n",
    "            # Update weight\n",
    "            W = (target_policy.get_probs([s_prime],[a_prime]))/(behavior_policy.get_probs([s_prime],[a_prime]))\n",
    "\n",
    "            # Update Q \n",
    "            Q[s][a] += alpha * W * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])    \n",
    "            \n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "            R += r\n",
    "            i += 1 \n",
    "            \n",
    "            if final_state:\n",
    "                break\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "    Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_sarsa_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, n=1, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    n-step SARSA algorithm: Off-policy TD control. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        n: number of steps\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q\n",
    "        \n",
    "        s = defaultdict(lambda: defaultdict(float))\n",
    "        a = defaultdict(lambda: defaultdict(float))\n",
    "        r = defaultdict(lambda: defaultdict(float))\n",
    "            \n",
    "        s[0] = env.reset()\n",
    "        a[0] = behavior_policy.sample_action(s[0])\n",
    "        \n",
    "        T = np.inf\n",
    "        t = 0\n",
    "        while True:\n",
    "            if t < T:\n",
    "                # Take action\n",
    "                s[t+1], r[t+1], final_state, _ = env.step(a[t])\n",
    "                R += r[t+1]\n",
    "                i += 1\n",
    "                \n",
    "                if final_state:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # Sample action from next state\n",
    "                    a[t+1] = behavior_policy.sample_action(s[t+1])\n",
    "            \n",
    "            tau = t - n + 1\n",
    "            \n",
    "            if tau >= 0:\n",
    "                # Collect states and actions included in ratio\n",
    "                last_step_rho = min([tau + n, T - 1])\n",
    "                first_step = tau + 1\n",
    "                states = [value for key, value in s.items() if key in range(first_step, last_step_rho+1)]\n",
    "                actions = [value for key, value in a.items() if key in range(first_step, last_step_rho+1)]\n",
    "                \n",
    "                # n-step importance sampling ratio\n",
    "                rho = np.prod([(target_policy.get_probs([state],[action]))/(behavior_policy.get_probs([state],[action])) for state, action in zip(states, actions)])\n",
    "                \n",
    "                # n-step return\n",
    "                last_step_G = min([tau + n, T])\n",
    "                G = np.sum([discount_factor**(i - tau - 1) * r[i] for i in range(first_step, last_step_G)])\n",
    "                if tau + n < T:\n",
    "                    G += discount_factor**n * Q[s[tau+n]][a[tau+n]]\n",
    "\n",
    "                # Update Q \n",
    "                Q[s[tau]][a[tau]] += alpha * rho * (G - Q[s[tau]][a[tau]])\n",
    "\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "                         \n",
    "            t += 1\n",
    "\n",
    "        stats.append((i, R))\n",
    "        \n",
    "    Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (100 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 3/100 [00:01<00:38,  2.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 100/100 [00:02<00:00, 46.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "np.random.seed(42)\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.05\n",
    "discount_factor = 1.0\n",
    "num_episodes = 100\n",
    "alpha=0.5\n",
    "Q = np.ones((env.nS, env.nA)) * -100\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# # the episode length is equal to the negative return. \n",
    "# print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "# Q_td_ordinary, td_ordinary_epsstats = sarsa_ordinary_importance_sampling(env, behavioral_policy, target_policy, \n",
    "#                                                                         num_episodes, discount_factor, alpha)\n",
    "\n",
    "n=4\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_td_nstep_ordinary, td_nstep_ordinary_epsstats = n_step_sarsa_ordinary_importance_sampling(env, behavioral_policy, target_policy, \n",
    "                                                                        num_episodes, n, discount_factor, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "n = 5\n",
    "\n",
    "rm = running_mean(td_nstep_ordinary_epsstats[0], n)\n",
    "\n",
    "# fig = go.Figure(go.Scatter(x=list(range(len(rm))), y=rm))\n",
    "# fig.show()\n",
    "\n",
    "plt.plot(rm, label=\"ordinary\")\n",
    "plt.title('Episode lengths TD')\n",
    "plt.legend()\n",
    "# plt.gca().set_ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting episode length ordinary: 17\n"
     ]
    }
   ],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_td_ordinary)\n",
    "\n",
    "ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: TD Weighted Importance Sampling (same as above but weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD weighted importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

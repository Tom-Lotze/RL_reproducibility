{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions used in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import time\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Windy gridworld\n",
    "Gives a reward of -1 for each step taken, while the final state is not reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m            WindyGridworldEnv\n",
       "\u001b[0;31mString form:\u001b[0m     <WindyGridworldEnv instance>\n",
       "\u001b[0;31mFile:\u001b[0m            ~/Desktop/GitHub/RL_reproducibility/windy_gridworld.py\n",
       "\u001b[0;31mSource:\u001b[0m         \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mWindyGridworldEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscreteEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'render.modes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ansi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_limit_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnew_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnew_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_limit_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel_multi_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mis_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_position\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Wind strength\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwinds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwinds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwinds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Calculate transition probabilities\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUP\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDOWN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLEFT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# We always start in state (3, 0)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0misd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0misd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel_multi_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindyGridworldEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0moutfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ansi'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# print(self.s)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" x \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" T \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" o \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mClass docstring:\u001b[0m\n",
       "Has the following members\n",
       "- nS: number of states\n",
       "- nA: number of actions\n",
       "- P: transitions (*)\n",
       "- isd: initial state distribution (**)\n",
       "\n",
       "(*) dictionary dict of dicts of lists, where\n",
       "  P[s][a] == [(probability, nextstate, reward, done), ...]\n",
       "(**) list or array of length nS\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "env = WindyGridworldEnv()\n",
    "env??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "### Target policy (choose greedy vs non-greedy)\n",
    "Greedy policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def get_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"   \n",
    "        # for state and action only:\n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        \n",
    "        if action in max_indices:\n",
    "            prob = 1/len(max_indices)\n",
    "        else:\n",
    "            prob = 0\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        best_action = np.random.choice([i for i, j in enumerate(self.Q[obs]) if j == np.max(self.Q[obs])])\n",
    "        \n",
    "        return best_action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def get_probs(self, state, action):\n",
    "        # for one state and action \n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        # all probs are equal, give all equal probabilities\n",
    "        if len(max_indices) == len(action_probs):\n",
    "            return 1/len(max_indices)\n",
    "            \n",
    "        if action in max_indices:\n",
    "            prob = (1-self.epsilon)/len(max_indices)\n",
    "        else:\n",
    "            prob = epsilon / (len(action_probs) - len(max_indices))\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        p = np.random.uniform()\n",
    "        if p > self.epsilon:\n",
    "            # choose one of the best actions\n",
    "            action = np.random.choice([i for i, j in enumerate(self.Q[obs]) if j == np.max(self.Q[obs])])\n",
    "        else:\n",
    "            # return a random action\n",
    "            action = np.random.randint(0,4)\n",
    "                \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioural policy\n",
    "Random policy from blackjack lab. \n",
    "TODO: experiment with behavioural policies to check which yield interesting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy(object):\n",
    "    \"\"\"\n",
    "    A behavioural policy\n",
    "    \"\"\"\n",
    "    def __init__(self, nS, nA):\n",
    "        self.probs = np.ones((nS, nA)) * 1/nA\n",
    "        \n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"        \n",
    "        probs = [self.probs[s,a] for s,a in zip(states, actions)]\n",
    "        \n",
    "        return probs\n",
    "\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            state: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        p_s = self.probs[state]\n",
    "        \n",
    "        return np.random.choice(range(0,self.probs.shape[1]), p=p_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = RandomPolicy(env.nS, env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function given an env and policy\n",
    "Function to sample an episode from the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n",
    "        state after the termination is not included in the list of states.\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    # get a starting state\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    \n",
    "    # keep looping until done, don's save the terminal state\n",
    "    while not d:\n",
    "        states.append(s)\n",
    "        a = policy.sample_action(s)\n",
    "        s, r, d, _ = env.step(a)\n",
    "        \n",
    "        # save                \n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(d)\n",
    "        \n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 1441\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for random policy\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, random_policy)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 8173\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for eps greedy policy\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = EpsilonGreedyPolicy(Q, epsilon=0.1)\n",
    "\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, bp)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Ordinary Importance Sampling (make it work for windy gridworld)\n",
    "Status: updated to update Q instead of V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qdefaultdict2array(Q, nA, nS):\n",
    "    Q_np = np.zeros((nS, nA))\n",
    "    for S in range(nS):\n",
    "        for A in range(nA):\n",
    "            Q_np[S][A] = Q[S][A]\n",
    "    return Q_np\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "#     Q = defaultdict(lambda: defaultdict(float))\n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    returns_count = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral function:\n",
    "#         behavior_policy = EpsilonGreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS), epsilon)\n",
    "        behavior_policy.Q = Q\n",
    "#         target_policy = GreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "        target_policy.Q = Q        \n",
    "    \n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save the episode length\n",
    "        episode_lens.append(len(states))\n",
    "        \n",
    "        # extract target and behavioral probabilities\n",
    "#         target_probs = target_policy.get_probs(states, actions)\n",
    "#         behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "#         target_probs = [target_policy.get_probs(states[t], actions[t]) for t in range(len(states))]\n",
    "#         behavioral_probs = [behavioral_policy.get_probs(states[t], actions[t]) for t in range(len(states))]\n",
    "            \n",
    "\n",
    "        G = 0        \n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)):\n",
    "#             print(i)\n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            returns_count[s][a] += 1 \n",
    "\n",
    "            # compute the ratio using the two probability lists\n",
    "#             ratio = np.prod([t/b for t, b in zip(target_probs[timestep:], behavioral_probs[timestep:])])\n",
    "            \n",
    "            # use every visit incremental method\n",
    "            Q[s][a] += 1/returns_count[s][a] * (W * G - Q[s][a])\n",
    "            \n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))\n",
    "            \n",
    "            if W == 0:\n",
    "                break\n",
    "    \n",
    "#     Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "    \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC: Weighted Importance Sampling\n",
    "\n",
    "##### (TODO: Eventually: merge the two functions into one with a weighted flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_weighted_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and weighted importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a matrix defaultdict for the Q function and the sum of weights C\n",
    "#     Q = defaultdict(lambda: defaultdict(float))\n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    C = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral function:\n",
    "#         behavior_policy = EpsilonGreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS), epsilon)\n",
    "        behavior_policy.Q = Q\n",
    "#         target_policy = GreedyPolicy(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "        target_policy.Q = Q\n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save episode lengths\n",
    "        episode_lens.append(len(states))\n",
    "        \n",
    "        # extract target and behavioral probabilities OLD\n",
    "#         target_probs = target_policy.get_probs(states, actions)\n",
    "#         behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "       \n",
    "        # print(Qdefaultdict2array(Q, env.nA, env.nS))\n",
    "#         print(target_probs)\n",
    "        \n",
    "        \n",
    "        # initialize the return and the weight\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)): \n",
    "#             print(i)\n",
    "            # extract info of current timestep from trajectory    \n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            # add W to the sum of weights C\n",
    "            C[s][a] += W\n",
    "            \n",
    "            # update Q function incrementally\n",
    "            Q[s][a] += W/C[s][a] * (G - Q[s][a])\n",
    "            \n",
    "            # update the weight\n",
    "            # W *= (target_probs[timestep])/(behavioral_probs[timestep])\n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))\n",
    "            \n",
    "            # break out of the loop if the weights are 0\n",
    "            if W == 0:\n",
    "                break\n",
    "    \n",
    "#     Q = Qdefaultdict2array(Q, env.nA, env.nS)     \n",
    "    \n",
    "    return Q, episode_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Plot the episode length over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (5000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:15<00:00, 313.85it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using weighted importance sampling (5000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:22<00:00, 221.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "np.random.seed(42)\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.1\n",
    "discount_factor = 1.0\n",
    "num_episodes = 5000\n",
    "Q = np.ones((env.nS, env.nA)) * -100\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_ordinary, mc_ordinary_epslengths = mc_ordinary_importance_sampling(env, behavioral_policy, target_policy, \n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)\n",
    "print(f\"Updating Q using weighted importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_weighted, mc_weighted_epslengths = mc_weighted_importance_sampling(env, behavioral_policy, target_policy,\n",
    "                                                                        num_episodes, discount_factor, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de3xV5Z3v8c9v5wrhDiEGghAVrBEUEbmIIGpbrLYqtkyhY6vHqjOjnjo9M22xTqtO9dR2GK21Ry2jVbRVx0u1atURUQregKBSuQpIgEiEEG6B3LN/54+9oDvZAUISkrD29/16xb32s5611vPE8N1rP+tm7o6IiCSHSEc3QERE2o9CX0QkiSj0RUSSiEJfRCSJKPRFRJKIQl9EJIko9KVTMbNXzezKNl7nbWb2+xYuW2RmX2zL9jRzu0PMzM0stb23LeGm0Jc2FwRlpZntjfv5TXOWdfevuPuco93GzuZofbiY2eTgw+OPjcpPD8rnx5WZmX3PzJab2T4zKzazZ8xsRFu3SzqO9iLkaPmau7/R0Y0QAEqBs82sr7uXBWVXAp80qncvcDFwLfAOkAJMDco+bqe2ylGmPX1pV2Z2lZm9Y2b3mdluM1ttZhfEzZ9vZtcE0yeZ2V+CetvN7L/j6p1tZkuCeUvM7Oy4efnBcuVmNhfo16gN48zsXTPbZWbLzGxyM9seMbOZZrbezMrM7Gkz6xPM2z8cc6WZbQrae0vcsl3MbI6Z7TSzVWb2QzMrDuY9DhwPvBR8K/ph3Gb//iDrG2NmhWa2x8y2mtndh2h6DfACMD1YNgX4O+APcesbCtwAzHD3N9292t0r3P0P7n5Xc34/cmxQ6EtHGAt8SiyMbwX+uD88G/kZ8DrQG8gD7gMI6v4Z+DXQF7gb+LOZ9Q2WewJYGqz/Z8T2agmWHRgsewfQB/hX4Dkzy25Gu78HXAacCwwAdgL/r1Gdc4CTgQuAn5rZKUH5rcAQ4ATgS8AV+xdw928Dm4h9O+rm7r9sxvruBe519x7AicDTh2n7Y8B3gukpwApgS9z8C4Bid198mPXIMU6hL0fLC8Ge9P6fa+PmbQN+5e617v7fwBpiQwiN1QKDgQHuXuXubwflFwNr3f1xd69z9yeB1cDXzOx44CzgJ8He6gLgpbh1XgG84u6vuHvU3ecChcBFzejTPwC3uHuxu1cDtwHfaHSw9XZ3r3T3ZcAy4PSg/O+A/+vuO929mNgHVnMcbH21wElm1s/d97r7+4daibu/C/Qxs5OJhf9jjar0BUqa2SY5hin05Wi5zN17xf38V9y8z7zhnf42EttzbuyHgAGLzWyFmV0dlA8Ilom3ERgYzNvp7vsazdtvMDAt/gOJ2N50bjP6NBh4Pm65VUA9kBNX5/O46QqgW1ybN8fNi58+lIOt77vAMGB1MLz11Was63HgRuA84PlG88po3u9AjnEKfekIA83M4t4fT8OhBgDc/XN3v9bdBxDby77fzE4K6g5uVP144DNie6u9zSyr0bz9NgOPN/pAymrmuPVm4CuNls1098+asWwJsSGq/QY1mn9Et7t197XuPgPoD/wCeLZRn5vyOHA9sW86FY3mzQPyzGz0kbRDjj0KfekI/YHvmVmamU0DTgFeaVzJzKaZ2f6g3EksGOuDusPM7Ftmlmpm3wQKgJfdfSOx4ZrbzSzdzM4Bvha32t8TGwaaYmYpZpYZnNYYH8gH8yBwp5kNDtqXbWaXNrPPTwM3m1nv4LjCjY3mbyU23t8sZnaFmWW7exTYFRTXH2oZd99A7HjELU3MWwvcDzwZ/D7Sg9/NdDOb2dx2Seen0JejZf+ZKPt/4ocTFgFDge3AncA34k4ljHcWsMjM9gIvAje5+4ag7leBfyE2LPFD4Kvuvj1Y7lvEDhbvIHYA9cD4tbtvBi4FfkzsVMbNwA9o3r+Fe4N2vG5m5cD7wXaa49+BYmAD8AbwLFAdN//nwL8FQ0f/2oz1XQisCH439wLT3b3qcAu5+9vunvCtKvA94DfEDk7vAtYTO2XzpYPUl2OQ6SEq0p7M7CrgGnc/p6Pb0pHM7J+IBfW5Hd0WSS7a0xdpB2aWa2YTgnP9Tyb2LaXxwVSRo05X5Iq0j3Tgt0A+saGTp4iNoYu0Kw3viIgkEQ3viIgkkU4/vNOvXz8fMmRIRzdDROSYsnTp0u3unnB7kU4f+kOGDKGwsLCjmyEickwxs8ZXrQMa3hERSSoKfRGRJKLQFxFJIp1+TF9Ewqm2tpbi4mKqqg579wg5hMzMTPLy8khLS2tWfYW+iHSI4uJiunfvzpAhQ2h401VpLnenrKyM4uJi8vPzm7WMhndEpENUVVXRt29fBX4rmBl9+/Y9om9LCn0R6TAK/NY70t9haEN/zrtFvLTsYHeQFRFJTqEN/d+/v5FXl+uRnyLSth599FFuvDH2DJwHH3yQxx5r/Ljhzk0HckVEDsLdcXcikab3j//xH/+xTbZTX19PSkpKm6zrcEK7py8i0hx33303w4cPZ/jw4fzqV7+iqKiIU045heuvv55Ro0axefNmHnnkEYYNG8a5557LO++8c2DZ2267jVmzZgEwefJkfvSjHzFmzBiGDRvGwoULASgqKmLixImMGjWKUaNG8e677wIwf/58zjvvPL71rW8xYsQIfvKTn3DvvfceWPctt9zCr3/96zbvr/b0RaTD3f7SClZu2dOm6ywY0INbv3bqIessXbqURx55hEWLFuHujB07lnPPPZc1a9bwyCOPcP/991NSUsKtt97K0qVL6dmzJ+eddx5nnHFGk+urq6tj8eLFvPLKK9x+++288cYb9O/fn7lz55KZmcnatWuZMWPGgfuJLV68mOXLl5Ofn09RURGXX345N910E9FolKeeeorFixe36e8EQh76elSAiBzK22+/zdSpU8nKygLg8ssvZ+HChQwePJhx48YBsGjRIiZPnkx2duyGld/85jf55JNPmlzf5ZdfDsCZZ55JUVERELsI7cYbb+Sjjz4iJSWlwbJjxow5cH79kCFD6Nu3Lx9++CFbt27ljDPOoG/fvm3e59CGvs4EEzl2HG6P/Gg52EOk9n8I7Nfc0yIzMjIASElJoa6uDoB77rmHnJwcli1bRjQaJTMz86Dbueaaa3j00Uf5/PPPufrqq5vdjyOhMX0RSVqTJk3ihRdeoKKign379vH8888zceLEBnXGjh3L/PnzKSsro7a2lmeeeeaItrF7925yc3OJRCI8/vjj1NfXH7Tu1KlTee2111iyZAlTpkxpUZ8OJ7R7+iIihzNq1CiuuuoqxowZA8T2tHv37t2gTm5uLrfddhvjx48nNzeXUaNGHTK4G7v++uv5+te/zjPPPMN5552XsHcfLz09nfPOO49evXodtbN5Ov0zckePHu0teYjKl+/5Cydmd+OBK848Cq0SkdZatWoVp5xySkc3o1OJRqOMGjWKZ555hqFDhzZ7uaZ+l2a21N1HN66r4R0RkU5g5cqVnHTSSVxwwQVHFPhHKtTDO538S4yIyAEFBQV8+umnR307od3TN3T6johIY6ENfRERSXTY0Dez35nZNjNbHlfWx8zmmtna4LV33LybzWydma0xsylx5Wea2cfBvF+b7qkqItLumrOn/yhwYaOymcA8dx8KzAveY2YFwHTg1GCZ+81s/3lHDwDXAUODn8brFBGRo+ywoe/uC4AdjYovBeYE03OAy+LKn3L3anffAKwDxphZLtDD3d/z2Dmij8Utc9Q4OpIrIm3vmmuuYeXKlYesc9VVV/Hss88mlBcVFfHEE08c8TYPtr4j1dIx/Rx3LwEIXvsH5QOBzXH1ioOygcF04/KjRoNHInK0PPTQQxQUFLRo2ZaGfltp6wO5TUWtH6K86ZWYXWdmhWZWWFpa2maNExGJ98tf/vLA7Yu///3vc/755wMwb948rrjiCl5//XXGjx/PqFGjmDZtGnv37gVit1Hef9Howw8/zLBhw5g8eTLXXnvtgQesACxYsICzzz6bE0444cBe+syZM1m4cCEjR47knnvuob6+nh/84AecddZZnHbaafz2t78FYvcFuvHGGykoKODiiy9m27ZtbdLnlp6nv9XMct29JBi62d+aYmBQXL08YEtQntdEeZPcfTYwG2JX5LawjSJyrHh1Jnz+cduu87gR8JW7Dlll0qRJ/Od//iff+973KCwspLq6mtraWt5++21GjBjBHXfcwRtvvEFWVha/+MUvuPvuu/npT396YPktW7bws5/9jA8++IDu3btz/vnnc/rppx+YX1JSwttvv83q1au55JJL+MY3vsFdd93FrFmzePnllwGYPXs2PXv2ZMmSJVRXVzNhwgS+/OUv8+GHH7JmzRo+/vhjtm7dSkFBQZvchK2le/ovAlcG01cCf4orn25mGWaWT+yA7eJgCKjczMYFZ+18J24ZEZEOceaZZ7J06VLKy8vJyMhg/PjxFBYWsnDhQrp06cLKlSuZMGECI0eOZM6cOWzcuLHB8osXL+bcc8+lT58+pKWlMW3atAbzL7vsMiKRCAUFBWzdurXJNrz++us89thjjBw5krFjx1JWVsbatWtZsGABM2bMICUlhQEDBhz4FtJah93TN7MngclAPzMrBm4F7gKeNrPvApuAaQDuvsLMngZWAnXADe6+/85E/0TsTKAuwKvBz1GlK3JFjhGH2SM/WtLS0hgyZAiPPPIIZ599NqeddhpvvfUW69evJz8/ny996Us8+eSTB13+cPcu23+r5UPVdXfuu+++hLtqvvLKK82+pfORaM7ZOzPcPdfd09w9z90fdvcyd7/A3YcGrzvi6t/p7ie6+8nu/mpceaG7Dw/m3eid/U5vIpIUJk2axKxZs5g0aRITJ07kwQcfZOTIkYwbN4533nmHdevWAVBRUZHw8JQxY8bwl7/8hZ07d1JXV8dzzz132O11796d8vLyA++nTJnCAw88QG1tLQCffPIJ+/btY9KkSTz11FPU19dTUlLCW2+91Sb9DfW9d0REDmfixInceeedjB8/nqysLDIzM5k4cSLZ2dk8+uijzJgxg+rqagDuuOMOhg0bdmDZgQMH8uMf/5ixY8cyYMAACgoK6Nmz5yG3d9ppp5Gamsrpp5/OVVddxU033URRURGjRo3C3cnOzuaFF15g6tSpvPnmm4wYMeLA83nbQmhvrXzhrxZwfJ+uzP5Owp1FRaQTCMutlffu3Uu3bt2oq6tj6tSpXH311UydOrVd26BbK4uItJPbbruNkSNHMnz4cPLz87nssqN+3WmraHhHRKQVZs2a1dFNOCKh3tPv3ANXItLZh5ePBUf6Owxt6OsmniKdW2ZmJmVlZQr+VnB3ysrKyMzMbPYyGt4RkQ6Rl5dHcXExutVK62RmZpKXl3f4igGFvoh0iLS0NPLz8zu6GUkntMM7IiKSKNShr6FCEZGGQhv6OowrIpIotKEvIiKJFPoiIklEoS8ikkRCHvo6kisiEi+0oa8LckVEEoU29EVEJJFCX0QkiSj0RUSSiEJfRCSJhDr0dRsGEZGGQhv6OntHRCRRaENfREQSKfRFRJKIQl9EJImEOvR1HFdEpKHQhr7pjvoiIglCG/oiIpKoVaFvZt83sxVmttzMnjSzTDPrY2ZzzWxt8No7rv7NZrbOzNaY2ZTWN19ERI5Ei0PfzAYC3wNGu/twIAWYDswE5rn7UGBe8B4zKwjmnwpcCNxvZimta76IiByJ1g7vpAJdzCwV6ApsAS4F5gTz5wCXBdOXAk+5e7W7bwDWAWNauf1Dcl2SKyLSQItD390/A2YBm4ASYLe7vw7kuHtJUKcE6B8sMhDYHLeK4qAsgZldZ2aFZlZYWlra0iaKiEgjrRne6U1s7z0fGABkmdkVh1qkibImd8Xdfba7j3b30dnZ2S1sX4sWExEJtdYM73wR2ODupe5eC/wROBvYama5AMHrtqB+MTAobvk8YsNBIiLSTloT+puAcWbW1cwMuABYBbwIXBnUuRL4UzD9IjDdzDLMLB8YCixuxfZFROQIpbZ0QXdfZGbPAh8AdcCHwGygG/C0mX2X2AfDtKD+CjN7GlgZ1L/B3etb2X4RETkCLQ59AHe/Fbi1UXE1sb3+purfCdzZmm0eCZ27IyLSUGivyNVxXBGRRKENfRERSaTQFxFJIgp9EZEkEurQ110YREQaCm/o65JcEZEE4Q19ERFJoNAXEUkiCn0RkSSi0BcRSSKhDn2dvCMi0lBoQ1/n7oiIJApt6IuISCKFvohIElHoi4gkkVCHvus+DCIiDYQ29HUXBhGRRKENfRERSaTQFxFJIgp9EZEkotAXEUkioQ19HccVEUkU2tAXEZFECn0RkSSi0BcRSSIKfRGRJBLq0NddGEREGgpt6JvuwyAikqBVoW9mvczsWTNbbWarzGy8mfUxs7lmtjZ47R1X/2YzW2dma8xsSuubLyIiR6K1e/r3Aq+5+xeA04FVwExgnrsPBeYF7zGzAmA6cCpwIXC/maW0cvsiInIEWhz6ZtYDmAQ8DODuNe6+C7gUmBNUmwNcFkxfCjzl7tXuvgFYB4xp6fZFROTItWZP/wSgFHjEzD40s4fMLAvIcfcSgOC1f1B/ILA5bvnioCyBmV1nZoVmVlhaWtriBroejS4i0kBrQj8VGAU84O5nAPsIhnIOoqkjq02msrvPdvfR7j46Ozu7RY3TYVwRkUStCf1ioNjdFwXvnyX2IbDVzHIBgtdtcfUHxS2fB2xpxfZFROQItTj03f1zYLOZnRwUXQCsBF4ErgzKrgT+FEy/CEw3swwzyweGAotbun0RETlyqa1c/n8DfzCzdOBT4H8R+yB52sy+C2wCpgG4+woze5rYB0MdcIO717dy+yIicgRaFfru/hEwuolZFxyk/p3Ana3Z5pHQFbkiIg2F+Ircjm6BiEjnE9rQFxGRRAp9EZEkotAXEUkiCn0RkSQS6tDX2TsiIg2FNvRNN2IQEUkQ2tAXEZFECn0RkSSi0BcRSSKhDn3dT19EpKHwhr6O44qIJAhv6IuISAKFvohIElHoi4gkkVCHvq7IFRFpKLShr+O4IiKJQhv6IiKSSKEvIpJEFPoiIklEoS8ikkRCHfo6eUdEpKHQhr7p9B0RkQShDX0REUmk0BcRSSIKfRGRJJLa0Q04Wm7Y+UuqyATGd3RTREQ6jdCG/sTKNzu6CSIinU6rh3fMLMXMPjSzl4P3fcxsrpmtDV57x9W92czWmdkaM5vS2m2LiMiRaYsx/ZuAVXHvZwLz3H0oMC94j5kVANOBU4ELgfvNLKUNti8iIs3UqtA3szzgYuChuOJLgTnB9Bzgsrjyp9y92t03AOuAMa3ZvoiIHJnW7un/CvghEI0ry3H3EoDgtX9QPhDYHFevOChLYGbXmVmhmRWWlpa2qGGr0oezPP20Fi0rIhJWLQ59M/sqsM3dlzZ3kSbKmrxTgrvPdvfR7j46Ozu7pS3E9BQVEZEGWnP2zgTgEjO7CMgEepjZ74GtZpbr7iVmlgtsC+oXA4Pils8DtrRi+4fkGNbgC4iIiLR4T9/db3b3PHcfQuwA7ZvufgXwInBlUO1K4E/B9IvAdDPLMLN8YCiwuMUtP1z7jtaKRUSOYUfjPP27gKfN7LvAJmAagLuvMLOngZVAHXCDu9cfhe0HDFP0i4g00Cah7+7zgfnBdBlwwUHq3Qnc2RbbPGybTGP6IiKN6d47IiJJJLSh7xreERFJENrQj50hqtAXEYkX2tDXnr6ISKIQhz6YMl9EpIHQhr6Gd0REEoU29N00vCMi0lhoQ7/pW/2IiCS30Ia+DuSKiCQKceij0BcRaSS0oa8DuSIiiUIb+rHhHRERiRfa0Mc0vCMi0lhoQ98x0F02RUQaCHXoa09fRKSh0IY+GtMXEUkQ2tD3uP+KiEhMaENfj0sUEUkU2tB3De6IiCQIbeiD7r4jItJYaEPfdUWuiEiC0IY+6OIsEZHGQhv6btrTFxFpLLShr/P0RUQShTb0HcN0GwYRkQbCHfoa3hERaSC0oS8iIolCG/ra0xcRSdTi0DezQWb2lpmtMrMVZnZTUN7HzOaa2drgtXfcMjeb2TozW2NmU9qiA4doIDp7R0Skodbs6dcB/+LupwDjgBvMrACYCcxz96HAvOA9wbzpwKnAhcD9ZpbSmsYfip6RKyKSqMWh7+4l7v5BMF0OrAIGApcCc4Jqc4DLgulLgafcvdrdNwDrgDEt3f7h6ZRNEZHG2mRM38yGAGcAi4Acdy+B2AcD0D+oNhDYHLdYcVDW1PquM7NCMyssLS1tUZs0pi8ikqjVoW9m3YDngH929z2HqtpEWZOp7O6z3X20u4/Ozs5uacsOtnoRkaTVqtA3szRigf8Hd/9jULzVzHKD+bnAtqC8GBgUt3gesKU12z9M4zBlvohIA605e8eAh4FV7n533KwXgSuD6SuBP8WVTzezDDPLB4YCi1u6/eZR6ouIxEttxbITgG8DH5vZR0HZj4G7gKfN7LvAJmAagLuvMLOngZXEzvy5wd3rW7H9Q7OIxvRFRBppcei7+9sc/DklFxxkmTuBO1u6zSOn0BcRiRfaK3K1py8ikijEod/RDRAR6XzCG/o6T19EJEF4Q98iup++iEgj4Q19ERFJENrQj11GICIi8UIb+qC7bIqINBbe0LcIOk9fRKSh0Ia+mfb0RUQaC23oQ0Sn6ouINBLe0NfjEkVEEoQ39HVxlohIgtCGvpkelygi0lhoQ//UXW/RlSqo3NnRTRER6TRCG/q9arfGJrauSJy58V3YtKh9GyQi0gm05iEqx4aq3Yllj3wl9npbE/NEREIstHv6zbLkIdi+rqNbISLSbkIb+nvS+scmood4IuOf/wUeOr99GiQi0gmENvSfHfYfsYnXZh66YlPDPyIiIRXa0K9Ny4pN7Pms4Yzqve3fGBGRTiK0od81M7PpGRv+0r4NERHpREIb+lldDhL6kfCfsCQicjChDf3U1LSmZ9TXtG9DjnV7S6FqT0e3QkTaSGhDPz3tIHv0Oze2b0OOdQ9d8LfrGkTkmBfa0LfMHn97s201VOyITb9+S8OKOSPar1FHS9l6qKs+OuvetRG2Lj866xaRdhfa0N9e6fyx/pzYm/vHwi/z4baeCfWitRU8MH89uytqKSotZ/aC9bjH7s7p7vz81VXsq67jvfVllO2txveUULyzgqUbd3DfvLWUllezY18NSzfu5KVlWxh6yyvc/Me/UlVbT1VtPXX1Ucr2VlO3bycs/i+qamoBqKqtZ9ueKgC27ak6sE3cD1xbsK+6jj1VteCOR6N/qwOwZwvU1cQ+0O4bBXf0hwWzEvq3eMOOA9up3bYWVr/SYP7+dgLw20nw3LVQG6vPJ683+/ftuz9j5ZrVDdu43/o38ZqKhmWv/xv89tyG62hq2Raoq49SXZd4fUbC+ss/P+y6Wt2mDQub9e3ywP+D1ojWg3uTfW/KwfpWVVvf5Lza+mirmteUzTsqiEYP/ztOaE+0HvaVHXz+4USjUFt50Nn1USdaHz30dT7N2K67H7ZOZU192/z/byZrq39oR8vo0aO9sLDwiJfbW13Hwz+7hptSnz8KrYJS70kq9fS22CmgS6NDOTOyFoBn6ibxVnQk3ayScu/KtJS/cH7KRwC8XD+OfCvh+fpz2Oa9mJX2IFu8H4uip9DfdnJeyrID24i6UUMqmRb7oJhafTv/lvb7A9sBeLTuy1yVmhjOm6LZbKcn5d6VFT6EPd6VmWlPAbA+mstr0bO4IfVF3o+ewpn2CWl26D+6omgOQyJbmVs/ipfqx7PMT+TvUubzpchSPowO5Zup8xOW+XntDCakrmSSxfo026fym+qLmBBZzgPp9wLwDzX/zO1pczjOdvJS/Tg2+HEsiX6Bx9PvopTeZESclLQ0sqpLea7PtXyydS+Gc3nKQoZFYqfjbvVepFHHfXVT6U4l56R8zJjIGrZ6L96JDmdFdDDnRv5Kru3g7rpv8ED6vbxSP4aLUhZT5t15IeMSpqe8RVblFgAeqvsKObaTr6W8z9213+AfUl/i9ehoRqRu5kM7la61ZVR4JkMin7MyOphl9gVm2P9wVuQTSrwPRdHjqMfY5Dl8K/VNAD6InsSoyN+u/l4U/QKbIoMYl7kRr9pDSbQXv6u7kG+lvMnS6DCmpCzh1Ejsw6KcLnyQdS5n1iyhW20s6J6tn0SXSD0n2yYK606km1Xx1ZT3m/x/9+ce07Gdn7LJc/ggehKz0+8BYHH0ZMZE1rAgfRKTahYA8P3cx/ikqBgjyk/THiffPifb/nYty+roII6zHSxOOZPa2ipGpmxgIKUH5r9XX8D4lJUAzKqdxr+mPXNgXrH3Y3O0P9WksY8MXqyfwIjIp5wTWc7IyPoGbf4oegLrfSCZvXLIiFYyae9rpAd/o3s9k25WldDP+fWnk2M7OSWyice4mO/wZ9ZE81jrAyn1XpwRWcvIyKf8e+23+Wna4wAH/g4+jQyhZ3QX26LdGWhl3F93yYF/L39fczN/SP/5ge3UkUoqdWxMzeeFqpFcHnmbD/0kLkl5D4C30s/luKoNnBLZBECNp/CYX8Q1kZeoTenC7VXT6We7SaOObHazxgfx9ZSFLIiO4Nspc0mnjmdTLyYjfxxT//56zFp2v2AzW+ruoxPKwxr6AAtWb2He43eRYzv5cqSQkyKxf9T31l3OGbaWSSkft2VTRUTa1Narl5Bz/LAWLdtpQt/MLgTuBVKAh9z9rkPVb03oQ+zr1ed7qoiY0b97Brsra3GH1BSDaD2Rz5aQXrmVHZG+5OSdRPVnf6X2o6fpdsoF7Br0RXrtWIZ3y4HyEnznJiyngN316XTZuYb0orewXRup79IP751PbVp30mt3Y0ULieScCqWrqbcUUrbGPlz88v/C1rxKdP2bRKp2AVCXN45ozzzKd++iJmsAPWwfnw2eSvelD9CrSwp1ad3ZkXM2fba9T2oEbNdGqlOy2Jvam57l68iwetJ2reeT4f+Hys8+5vSdcwHY03Uw3Ss2UX3iFDI+fYN9Q75Ifc/BZOzbQlrRfPam9qRnZTEA7+VdzdDMPUSOH0PalkK6de/J5tJdpGf1os5S6bL9Y2p2b6VXSjWZezdTk9aD8gETqOl/Or0qNxjFFfEAAAdLSURBVMLnH9N1e6yPlcMuYV+kB+mb36YiK4/yPiPI2b4Iry5nbc8JZKQ4x7GdnRV1DCv9HwBqUrqyMucS+g+fTI/3/oOuFZ+xrufZnLDnffb2PpXU+kqWZ40nIyONYSUvk7VvE6VpA1hz+o8Yse63eM9BRHZvJrNiC+k1u4im96C6z8mkVW0nddcG9hw3nrTtK8mMVmLRv529tbd3AQwYSbcVT1CV1os6S2NbziQyqz6nV7du7KiJ0D3vVBbbaUS3r2XSjueorq0lpaacjGgF6bV7qOsxiIqKCrrX7WTXsGlsr0lhaNET1AyZTOXOz/nr8d/m9Ohq0nasYVXuVCqjqQzb/jqZezaSmpZKtHoftWk9SRk4kq5b3qU+kk769pXUpfekNpIJkRQq0/uSUbmNrOqtlJz1I3KX/ILa9F5Eu+dS2r2AAZtfpqLXMCIVZViXXkTyJ1Ad6cLm0l1srzR6RyoYtudddgy+iPTeuezY8FcG7vmI6l4nsrfHSfTZ+BofHf8d+kW3k1m+ie5VJVSk9aK8/1mQnkVu2XtUZg0ie8ubeLfjiFbsZE9aP7L2baJk6AxyazayzzPYVxOl36ZXiXbPo/uOv7Kt10i6WC2W1ZdoTRVdqrfhlsq+zONIrdlNz+0fsPn0myj77FPMjL09TqSr7yOv7D2yd8f+nurSe1CaO5n6zD508Qq8qpztWUPpUrWN1D6DqC9dS5e8kVRv30DO7mXsqaym974NfHLaD0nf9xnp/YfSo7qEqs/XsLfrIHqk1JCenkHq5veo7Z5H6o61lHc/gfLycnr06Y9F6+hXvZmKrgNI2fkpWwdcQE7lenbljOPtLXBOzUIydm9gU9YIvtBlN5mbF7C36yDKU3rRp7KIzb3Hkt53MNRWkpEaocoyqKyo5AubnqCmaw7R9O7szhxI7549qdizg8zMTPZWVpNVXkRx5lAyUyPYjvVEck6hfsQ0Bo29vMXZ1ylC38xSgE+ALwHFwBJghruvPNgyrQ19EZFkdLDQb+8DuWOAde7+qbvXAE8Bl7ZzG0REklZ7h/5AYHPc++KgrAEzu87MCs2ssLS0tPFsERFpofYO/aYOQyeML7n7bHcf7e6js7Oz26FZIiLJob1DvxgYFPc+D9jSzm0QEUla7R36S4ChZpZvZunAdODFdm6DiEjSatdbTrp7nZndCPwPsVM2f+fuTTy5XEREjoZ2v8+wu78CvHLYiiIi0uZCe+8dERFJ1Olvw2BmpUBL74fcD9jehs05FqjPyUF9Tg6t6fNgd084/bHTh35rmFlhU1ekhZn6nBzU5+RwNPqs4R0RkSSi0BcRSSJhD/3ZHd2ADqA+Jwf1OTm0eZ9DPaYvIiINhX1PX0RE4ij0RUSSSChD38wuNLM1ZrbOzGZ2dHtaw8x+Z2bbzGx5XFkfM5trZmuD195x824O+r3GzKbElZ9pZh8H835tLX3wZjsws0Fm9paZrTKzFWZ2U1Ae2n6bWaaZLTazZUGfbw/KQ9tniD1Yycw+NLOXg/eh7i+AmRUF7f3IzAqDsvbr9/6ntYflh9g9fdYDJwDpwDKgoKPb1Yr+TAJGAcvjyn4JzAymZwK/CKYLgv5mAPnB7yElmLcYGE/s9tavAl/p6L4dos+5wKhgujuxp60VhLnfQfu6BdNpwCJgXJj7HLT1/wBPAC8nw9920N4ioF+jsnbrdxj39EP1dC53XwDsaFR8KTAnmJ4DXBZX/pS7V7v7BmAdMMbMcoEe7v6ex/5aHotbptNx9xJ3/yCYLgdWEXvYTmj77TF7g7dpwY8T4j6bWR5wMfBQXHFo+3sY7dbvMIZ+s57OdYzLcfcSiAUk0D8oP1jfBwbTjcs7PTMbApxBbM831P0Ohjo+ArYBc9097H3+FfBDIBpXFub+7ufA62a21MyuC8rard/tfpfNdtCsp3OF1MH6fkz+TsysG/Ac8M/uvucQQ5ah6Le71wMjzawX8LyZDT9E9WO6z2b2VWCbuy81s8nNWaSJsmOmv41McPctZtYfmGtmqw9Rt837HcY9/WR4OtfW4Osdweu2oPxgfS8OphuXd1pmlkYs8P/g7n8MikPfbwB33wXMBy4kvH2eAFxiZkXEhmDPN7PfE97+HuDuW4LXbcDzxIak263fYQz9ZHg614vAlcH0lcCf4sqnm1mGmeUDQ4HFwdfFcjMbFxzh/07cMp1O0MaHgVXufnfcrND228yygz18zKwL8EVgNSHts7vf7O557j6E2L/RN939CkLa3/3MLMvMuu+fBr4MLKc9+93RR7KPxg9wEbEzPtYDt3R0e1rZlyeBEqCW2Kf7d4G+wDxgbfDaJ67+LUG/1xB3NB8YHfxxrQd+Q3A1dmf8Ac4h9lX1r8BHwc9FYe43cBrwYdDn5cBPg/LQ9jmuvZP529k7oe4vsbMKlwU/K/bnU3v2W7dhEBFJImEc3hERkYNQ6IuIJBGFvohIElHoi4gkEYW+iEgSUeiLiCQRhb6ISBL5/1vwADlZ5/V7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def running_mean(vals, n=1):\n",
    "    assert n < len(vals)\n",
    "    cumvals = np.array(vals).cumsum()\n",
    "    return (cumvals[n:] - cumvals[:-n]) / n \n",
    "\n",
    "n = 5\n",
    "\n",
    "plt.plot(running_mean(mc_ordinary_epslengths, n), label=\"ordinary\")\n",
    "plt.plot(running_mean(mc_weighted_epslengths, n), label=\"weighted\")\n",
    "plt.title('Episode lengths MC')\n",
    "plt.legend()\n",
    "# plt.gca().set_ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting episode length ordinary: 16\n",
      "resulting episode length weighted: 15\n"
     ]
    }
   ],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_mc_ordinary)\n",
    "greedy_weighted = GreedyPolicy(Q_mc_weighted)\n",
    "\n",
    "ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "weighted_episode = sample_episode(env, greedy_weighted)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])}\")\n",
    "print(f\"resulting episode length weighted: {len(weighted_episode[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference\n",
    "\n",
    "To-Do: Check n-step implementation so we can drop the first sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: Off-policy TD control. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q\n",
    "            \n",
    "        s = env.reset()\n",
    "        a = behavior_policy.sample_action(s)\n",
    "        \n",
    "        while True:\n",
    "            # Take action\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # Sample action at from next state\n",
    "            a_prime = behavior_policy.sample_action(s_prime)\n",
    "            \n",
    "            # Update weight\n",
    "            W = (target_policy.get_probs([s_prime],[a_prime]))/(behavior_policy.get_probs([s_prime],[a_prime]))\n",
    "\n",
    "            # Update Q \n",
    "            Q[s][a] += alpha * W * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])    \n",
    "            \n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "            R += r\n",
    "            i += 1 \n",
    "            \n",
    "            if final_state:\n",
    "                break\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "    Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_sarsa_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, n=1, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    n-step SARSA algorithm: Off-policy TD control. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        n: number of steps\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    Q = np.ones((env.nS, env.nA)) * -100\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q\n",
    "        \n",
    "        s = defaultdict(lambda: defaultdict(float))\n",
    "        a = defaultdict(lambda: defaultdict(float))\n",
    "        r = defaultdict(lambda: defaultdict(float))\n",
    "            \n",
    "        s[0] = env.reset()\n",
    "        a[0] = behavior_policy.sample_action(s[0])\n",
    "        \n",
    "        T = np.inf\n",
    "        t = 0\n",
    "        while True:\n",
    "            if t < T:\n",
    "                # Take action\n",
    "                s[t+1], r[t+1], final_state, _ = env.step(a[t])\n",
    "                R += r[t+1]\n",
    "                i += 1\n",
    "                \n",
    "                if final_state:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # Sample action from next state\n",
    "                    a[t+1] = behavior_policy.sample_action(s[t+1])\n",
    "            \n",
    "            tau = t - n + 1\n",
    "            \n",
    "            if tau >= 0:\n",
    "                # Collect states and actions included in ratio\n",
    "                last_step_rho = min([tau + n, T - 1])\n",
    "                first_step = tau + 1\n",
    "                states = [value for key, value in s.items() if key in range(first_step, last_step_rho+1)]\n",
    "                actions = [value for key, value in a.items() if key in range(first_step, last_step_rho+1)]\n",
    "                \n",
    "                # n-step importance sampling ratio\n",
    "                rho = np.prod([(target_policy.get_probs([state],[action]))/(behavior_policy.get_probs([state],[action])) for state, action in zip(states, actions)])\n",
    "                \n",
    "                # n-step return\n",
    "                last_step_G = min([tau + n, T])\n",
    "                G = np.sum([discount_factor**(i - tau - 1) * r[i] for i in range(first_step, last_step_G)])\n",
    "                if tau + n < T:\n",
    "                    G += discount_factor**n * Q[s[tau+n]][a[tau+n]]\n",
    "\n",
    "                # Update Q \n",
    "                Q[s[tau]][a[tau]] += alpha * rho * (G - Q[s[tau]][a[tau]])\n",
    "\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "                         \n",
    "            t += 1\n",
    "\n",
    "        stats.append((i, R))\n",
    "        \n",
    "    Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (100 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 22.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "np.random.seed(42)\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.05\n",
    "discount_factor = 1.0\n",
    "num_episodes = 100\n",
    "alpha=0.5\n",
    "Q = np.ones((env.nS, env.nA)) * -100\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# # the episode length is equal to the negative return. \n",
    "# print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "# Q_td_ordinary, td_ordinary_epsstats = sarsa_ordinary_importance_sampling(env, behavioral_policy, target_policy, \n",
    "#                                                                         num_episodes, discount_factor, alpha)\n",
    "\n",
    "n=4\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_td_nstep_ordinary, td_nstep_ordinary_epsstats = n_step_sarsa_ordinary_importance_sampling(env, behavioral_policy, target_policy, \n",
    "                                                                        num_episodes, n, discount_factor, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-3687c5f9efd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_nstep_ordinary_epsstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "n = 5\n",
    "\n",
    "rm = running_mean(td_nstep_ordinary_epsstats[0], n)\n",
    "\n",
    "# fig = go.Figure(go.Scatter(x=list(range(len(rm))), y=rm))\n",
    "# fig.show()\n",
    "\n",
    "plt.plot(rm, label=\"ordinary\")\n",
    "plt.title('Episode lengths TD')\n",
    "plt.legend()\n",
    "# plt.gca().set_ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_td_ordinary)\n",
    "\n",
    "ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: TD Weighted Importance Sampling (same as above but weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD weighted importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

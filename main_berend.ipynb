{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions used in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import time\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Windy gridworld\n",
    "Gives a reward of -1 for each step taken, while the final state is not reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "from gridworld import GridworldEnv\n",
    "# env = WindyGridworldEnv()\n",
    "# env = GridworldEnv()\n",
    "# env??\n",
    "import gym\n",
    "env = gym.envs.make(\"FrozenLake-v0\")\n",
    "env.env.__init__(is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "### Target policy (choose greedy vs non-greedy)\n",
    "Greedy policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def get_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        Takes a state and an action and returns the probability of taking that action from \n",
    "        that state, under Q and a greedy policy\n",
    "        \"\"\"   \n",
    "        # for state and action only:\n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        \n",
    "        if action in max_indices:\n",
    "            prob = 1/len(max_indices)\n",
    "        else:\n",
    "            prob = 0\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "\n",
    "        best_actions = [i for i, j in enumerate([self.Q[obs][i] for i in range(4)]) \n",
    "                   if j == max([self.Q[obs][i] for i in range(4)])] \n",
    "\n",
    "        best_action = np.random.choice(best_actions)\n",
    "        \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def get_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        Takes a state and an action and returns the probability of taking that action from \n",
    "        that state, under Q and a epsilon greedy policy\n",
    "        \"\"\"\n",
    "\n",
    "        action_probs = self.Q[state]\n",
    "        max_indices = np.argwhere(action_probs == np.amax(action_probs))\n",
    "        \n",
    "        # all probs are equal, give all equal probabilities\n",
    "        if len(max_indices) == len(action_probs):\n",
    "            return 1/len(max_indices)\n",
    "            \n",
    "        if action in max_indices:\n",
    "            prob = (1-self.epsilon)/len(max_indices)\n",
    "        else:\n",
    "            prob = epsilon / (len(action_probs) - len(max_indices))\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "\n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "         \n",
    "        best_actions = [i for i, j in enumerate(self.Q[obs])\n",
    "                   if j == np.max(self.Q[obs])] \n",
    "\n",
    "        p = np.random.uniform()\n",
    "        if p > self.epsilon:\n",
    "            # choose one of the best actions\n",
    "            action = np.random.choice(best_actions)\n",
    "        else:\n",
    "            action = np.random.randint(0,4)\n",
    "                \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n",
    "        state after the termination is not included in the list of states.\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    # get a starting state\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    \n",
    "    # keep looping until done, don's save the terminal state\n",
    "    while not d:\n",
    "        states.append(s)\n",
    "        a = policy.sample_action(s)\n",
    "        s, r, d, _ = env.step(a)\n",
    "        \n",
    "        # save                \n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(d)\n",
    "        \n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 3\n"
     ]
    }
   ],
   "source": [
    "# check the length of episodes that are generated for eps greedy policy\n",
    "Q = np.zeros((env.env.nS, env.env.nA))\n",
    "bp = EpsilonGreedyPolicy(Q, epsilon=0.1)\n",
    "\n",
    "for episode in range(1):\n",
    "    trajectory_data = sample_episode(env, bp)\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1]\n",
      "[0, 2, 1]\n",
      "[0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(trajectory_data[0])\n",
    "print(trajectory_data[1])\n",
    "print(trajectory_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_importance_sampling(env, behavior_policy, target_policy, num_episodes, weighted=False, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode, epsilon=0.05, seed=42, \n",
    "                           analyse_states=[(33,2), (18,2), (19,3)]):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the Q function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        weighted: Boolean flag to use weighted or ordinary importance sampling.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from (state, action) -> value.\n",
    "    \"\"\"\n",
    "\n",
    "    # set the current Q to a large negative value\n",
    "    Q = np.zeros((env.env.nS, env.env.nA))\n",
    "    if weighted:\n",
    "        C = np.zeros((env.env.nS, env.env.nA))\n",
    "    else:\n",
    "        returns_count = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    episode_lens = []\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    analysis_values = dict((k,[]) for k in analyse_states)\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        # update behavioral and target policy\n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q        \n",
    "    \n",
    "        # sample episode with new behavioural function\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # save the episode length\n",
    "        episode_lens.append(len(states)) \n",
    "\n",
    "        G = 0        \n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for i, timestep in enumerate(range(len(states)-1, -1, -1)):\n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            if weighted:\n",
    "                # add W to the sum of weights C\n",
    "                C[s][a] += W\n",
    "                Q[s][a] += W/C[s][a] * (G - Q[s][a])\n",
    "            else:\n",
    "                returns_count[s][a] += 1 \n",
    "                # use every visit incremental method\n",
    "                Q[s][a] += 1/returns_count[s][a] * W * (G - Q[s][a])\n",
    "\n",
    "            W *= (target_policy.get_probs(s, a)) / (behavior_policy.get_probs(s, a))        \n",
    "\n",
    "            if W == 0:\n",
    "                break\n",
    "\n",
    "        # store state values to analyse\n",
    "#         for (s,a) in analyse_states:\n",
    "# #             print(Q[s][a])\n",
    "#             analysis_values[(s,a)].append(Q[s][a])\n",
    "            \n",
    "    return Q, episode_lens, analysis_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (10000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:06<00:00, 1590.54it/s]\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using weighted importance sampling (10000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1691.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "seed = 10\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.2\n",
    "gamma = 1.0\n",
    "num_episodes = 10000\n",
    "Q = np.ones((env.env.nS, env.env.nA)) * 0\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_ordinary, mc_ordinary_epslengths, ordinary_analysis = mc_importance_sampling(env,\n",
    "                                                               behavioral_policy, target_policy, \n",
    "                                                               num_episodes, weighted=False,discount_factor=gamma, \n",
    "                                                               epsilon=epsilon, seed=seed)\n",
    "print(f\"Updating Q using weighted importance sampling ({num_episodes} episodes)\")\n",
    "Q_mc_weighted, mc_weighted_epslengths, weighted_analysis = mc_importance_sampling(env,\n",
    "                                                               behavioral_policy, target_policy,\n",
    "                                                               num_episodes, weighted=True, discount_factor=gamma, \n",
    "                                                               epsilon=epsilon, seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting episode length ordinary: 6 [0, 4, 8, 9, 13, 14]\n",
      "resulting episode length weighted: 6 [0, 4, 8, 9, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_mc_ordinary)\n",
    "greedy_weighted = GreedyPolicy(Q_mc_weighted)\n",
    "\n",
    "ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "weighted_episode = sample_episode(env, greedy_weighted)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])} {ordinary_episode[0]}\")\n",
    "print(f\"resulting episode length weighted: {len(weighted_episode[0])} {weighted_episode[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94304778 0.9942765  0.         0.81928614]\n",
      " [0.96337175 0.         0.99618249 0.9971685 ]\n",
      " [0.99919754 0.99854133 0.42695316 0.18163325]\n",
      " [0.82847552 0.         0.85633143 0.        ]\n",
      " [0.95161341 0.99720574 0.         0.97326101]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.99950238 0.         0.99948104]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.97565001 0.         0.99907265 0.98658571]\n",
      " [0.9939011  0.99989172 0.         0.        ]\n",
      " [0.98670544 0.99465955 0.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.99791286 1.         0.99817477]\n",
      " [0.99789916 0.99789363 1.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# print(np.var(weighted_analysis[(19,3)]))\n",
    "# print(np.var(ordinary_analysis[(19,3)]))\n",
    "# Q_mc_weighted[19][3]\n",
    "print(Q_mc_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting episode lengths during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def running_mean(vals, n=1):\n",
    "    assert n < len(vals)\n",
    "    cumvals = np.array(vals).cumsum()\n",
    "    return (cumvals[n:] - cumvals[:-n]) / n \n",
    "\n",
    "# set smoothing factor\n",
    "n = 5\n",
    "\n",
    "plt.plot(running_mean(mc_ordinary_epslengths, n), label=\"ordinary\")\n",
    "plt.plot(running_mean(mc_weighted_epslengths, n), label=\"weighted\")\n",
    "# plt.hlines(num_episodes)\n",
    "plt.title('Episode lengths MC')\n",
    "# plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "# plt.gca().set_ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference\n",
    "\n",
    "### TO-DO: TD Ordinary Importance Sampling (make it work for gridworld)\n",
    "Copied from TD_lab. Currently on-policy, needs to be off-policy.\n",
    "\n",
    "Confused: do we need value functions instead of q-values? Do we even use importance weights in off-policy TD? Are there more off-policy TD methods besides SARSA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, policy, Q, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        # initial state is 3,0 in the grid (according to source code)\n",
    "        s = env.reset()\n",
    "        a = policy.sample_action(s)\n",
    "        final_state_reached = False\n",
    "        \n",
    "        while True:\n",
    "            # new actions\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # keep track of stats\n",
    "            R += r\n",
    "            i += 1    \n",
    "            \n",
    "            # sample action at state s_prime\n",
    "            a_prime = policy.sample_action(s_prime)\n",
    "\n",
    "            # update Q \n",
    "            Q[s][a] += alpha * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])    \n",
    "    \n",
    "            # update policy\n",
    "            policy.Q = Q\n",
    "            \n",
    "            # if final state, terminate loop\n",
    "            if final_state:\n",
    "                break\n",
    "        \n",
    "            # update current s and a for next iteration\n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: Off-policy TD control. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    Q = np.ones((env.env.nS, env.env.nA)) * 0\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q\n",
    "            \n",
    "        s = env.reset()\n",
    "        a = behavior_policy.sample_action(s)\n",
    "        \n",
    "        while True:\n",
    "            # Take action\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # Sample action at from next state\n",
    "            a_prime = behavior_policy.sample_action(s_prime)\n",
    "            \n",
    "            # Update weight\n",
    "            W = (target_policy.get_probs([s_prime],[a_prime]))/(behavior_policy.get_probs([s_prime],[a_prime]))\n",
    "\n",
    "            # Update Q \n",
    "            Q[s][a] += alpha * W * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])    \n",
    "            \n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "            R += r\n",
    "            i += 1 \n",
    "            \n",
    "            if final_state:\n",
    "                break\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "#     Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_sarsa_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, n=1, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    n-step SARSA algorithm: Off-policy TD control. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        n: number of steps\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    Q = np.zeros((env.env.nS, env.env.nA))\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q\n",
    "        \n",
    "        s = defaultdict(lambda: defaultdict(float))\n",
    "        a = defaultdict(lambda: defaultdict(float))\n",
    "        r = defaultdict(lambda: defaultdict(float))\n",
    "            \n",
    "        s[0] = env.reset()\n",
    "        a[0] = behavior_policy.sample_action(s[0])\n",
    "        \n",
    "        T = np.inf\n",
    "        t = 0\n",
    "        while True:\n",
    "            if t < T:\n",
    "                # Take action\n",
    "                s[t+1], r[t+1], final_state, _ = env.step(a[t])\n",
    "                R += r[t+1]\n",
    "                i += 1\n",
    "                \n",
    "                if final_state:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # Sample action from next state\n",
    "                    a[t+1] = behavior_policy.sample_action(s[t+1])\n",
    "            \n",
    "            tau = t - n + 1\n",
    "            \n",
    "            if tau >= 0:\n",
    "                # Collect states and actions included in ratio\n",
    "                last_step_rho = min([tau + n, T - 1])\n",
    "                first_step = tau + 1\n",
    "                states = [value for key, value in s.items() if key in range(first_step, last_step_rho+1)]\n",
    "                actions = [value for key, value in a.items() if key in range(first_step, last_step_rho+1)]\n",
    "                \n",
    "                # n-step importance sampling ratio\n",
    "                rho = np.prod([(target_policy.get_probs([state],[action]))/(behavior_policy.get_probs([state],[action])) for state, action in zip(states, actions)])\n",
    "                \n",
    "                # n-step return\n",
    "                last_step_G = min([tau + n, T])\n",
    "                G = np.sum([discount_factor**(i - tau - 1) * r[i] for i in range(first_step, last_step_G)])\n",
    "                if tau + n < T:\n",
    "                    G += discount_factor**n * Q[s[tau+n]][a[tau+n]]\n",
    "\n",
    "                # Update Q \n",
    "                Q[s[tau]][a[tau]] += alpha * rho * (G - Q[s[tau]][a[tau]])\n",
    "\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "                         \n",
    "            t += 1\n",
    "\n",
    "        stats.append((i, R))\n",
    "        \n",
    "    Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: TD Weighted Importance Sampling (same as above but weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD weighted importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sarsa_weighted_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: Off-policy TD control. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        target policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        behaviour policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "#     Q = np.ones((env.nS, env.nA)) * -100\n",
    "    Q = np.zeros((env.env.nS, env.env.nA))\n",
    "    C = np.zeros((env.env.nS, env.env.nA))\n",
    "    \n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        W = 1\n",
    "        \n",
    "        behavior_policy.Q = Q\n",
    "        target_policy.Q = Q\n",
    "            \n",
    "        s = env.reset()\n",
    "        a = behavior_policy.sample_action(s)\n",
    "        \n",
    "        while True:\n",
    "            # Take action\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # Sample action at from next state\n",
    "            a_prime = behavior_policy.sample_action(s_prime)\n",
    "            \n",
    "            W = (target_policy.get_probs(s_prime,a_prime))/(behavior_policy.get_probs(s_prime,a_prime))\n",
    "            \n",
    "            # Update weight and C\n",
    "            C[s][a] = W\n",
    "            \n",
    "            if W == 0:\n",
    "                break\n",
    "                   \n",
    "            Q[s][a] += W/C[s][a] * alpha * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])\n",
    "            \n",
    "#             if i % 987 == 0:\n",
    "#                 print(W, C[s][a])\n",
    "                \n",
    "            behavior_policy.Q[s][a] = Q[s][a]\n",
    "            target_policy.Q[s][a] = Q[s][a]\n",
    "            \n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "            R += r\n",
    "            i += 1 \n",
    "            \n",
    "            if final_state:\n",
    "                break\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "#     Q = Qdefaultdict2array(Q, env.nA, env.nS)\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Q using ordinary importance sampling (10000 episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2815/10000 [00:17<00:43, 165.87it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-ab2621e5a43b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m Q_sarsa_ordinary, sarsa_ordinary_epslengths = sarsa_ordinary_importance_sampling(env, \n\u001b[1;32m     15\u001b[0m                                                                \u001b[0mbehavioral_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                                                num_episodes, discount_factor=gamma)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Updating Q using weighted importance sampling ({num_episodes} episodes)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-188-42087d98c520>\u001b[0m in \u001b[0;36msarsa_ordinary_importance_sampling\u001b[0;34m(env, behavior_policy, target_policy, num_episodes, discount_factor, alpha)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Update weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma_prime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbehavior_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma_prime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# Update Q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-644b7cc17aa6>\u001b[0m in \u001b[0;36mget_probs\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# for state and action only:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmax_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl2020/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m_argwhere_dispatcher\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_argwhere_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reproducible\n",
    "seed = 10\n",
    "\n",
    "# set other parameters\n",
    "epsilon = 0.1\n",
    "gamma = 1.0\n",
    "num_episodes = 10000\n",
    "Q = np.zeros((env.env.nS, env.env.nA))\n",
    "behavioral_policy = EpsilonGreedyPolicy(Q, epsilon=epsilon)\n",
    "target_policy = GreedyPolicy(Q)\n",
    "\n",
    "# the episode length is equal to the negative return. \n",
    "print(f\"Updating Q using ordinary importance sampling ({num_episodes} episodes)\")\n",
    "Q_sarsa_ordinary, sarsa_ordinary_epslengths = sarsa_ordinary_importance_sampling(env, \n",
    "                                                               behavioral_policy, target_policy, \n",
    "                                                               num_episodes, discount_factor=gamma)\n",
    "\n",
    "print(f\"Updating Q using weighted importance sampling ({num_episodes} episodes)\")\n",
    "Q_sarsa_weighted, sarsa_weighted_epslengths = sarsa_weighted_importance_sampling(env, \n",
    "                                                               behavioral_policy, target_policy,\n",
    "                                                               num_episodes, discount_factor=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting episode length ordinary: 100\n",
      "resulting episode length weighted: 6\n"
     ]
    }
   ],
   "source": [
    "# check how long an episode takes under the found Q function\n",
    "greedy_ordinary = GreedyPolicy(Q_sarsa_ordinary)\n",
    "greedy_weighted = GreedyPolicy(Q_sarsa_weighted)\n",
    "\n",
    "ordinary_episode = sample_episode(env, greedy_ordinary)\n",
    "weighted_episode = sample_episode(env, greedy_weighted)\n",
    "\n",
    "print(f\"resulting episode length ordinary: {len(ordinary_episode[0])}\")\n",
    "print(f\"resulting episode length weighted: {len(weighted_episode[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.     , 1.     , 1.     , 1.     ],\n",
       "       [1.     , 0.     , 0.     , 0.     ],\n",
       "       [0.     , 0.     , 0.     , 0.     ],\n",
       "       [0.     , 0.     , 0.     , 0.     ],\n",
       "       [0.     , 1.     , 0.     , 0.     ],\n",
       "       [0.     , 0.     , 0.     , 0.     ],\n",
       "       [0.     , 0.     , 0.     , 0.     ],\n",
       "       [0.     , 0.     , 0.     , 0.     ],\n",
       "       [0.     , 0.     , 1.     , 0.     ],\n",
       "       [0.     , 1.     , 0.     , 0.     ],\n",
       "       [0.     , 0.46875, 0.     , 0.     ],\n",
       "       [0.     , 0.     , 0.     , 0.     ],\n",
       "       [0.     , 0.     , 0.     , 0.     ],\n",
       "       [0.     , 0.     , 1.     , 0.     ],\n",
       "       [0.     , 0.     , 1.     , 0.     ],\n",
       "       [0.     , 0.     , 0.     , 0.     ]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

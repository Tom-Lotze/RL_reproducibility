{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions used in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import time\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Windy gridworld\n",
    "Gives a reward of -1 for each step taken, while the final state is not reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from windy_gridworld import WindyGridworldEnv\n",
    "env = WindyGridworldEnv()\n",
    "env??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "### Target policy (choose greedy vs non-greedy)\n",
    "Greedy policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q):\n",
    "        self.Q = Q\n",
    "    \n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"   \n",
    "        \n",
    "        # Inefficient but kept same structure as below if we change policy later\n",
    "        probs = [1 if a == np.argmax(self.Q[s]) else 0 for s,a in zip(states, actions)]\n",
    "        \n",
    "        return probs\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "\n",
    "        # find out what the max action is\n",
    "        best_action = np.argmax(self.Q[obs])\n",
    "        \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "#### OLD GET PROBS, uses code from sample action to get the probabilities of specific states\n",
    "#     def get_probs(self, obs):\n",
    "#         # find out what the max action is\n",
    "#         max_index = np.argmax(self.Q[obs])\n",
    "        \n",
    "#         # create equal probabilities for each action\n",
    "#         probs = np.zeros(self.Q[obs].shape) + (self.epsilon/(self.Q[obs].size))\n",
    "        \n",
    "#         # add (1-epsilon) to the max action\n",
    "#         probs[max_index] += 1-self.epsilon\n",
    "        \n",
    "#         return probs\n",
    "    \n",
    "    def get_probs(self, states, actions):\n",
    "#         breakpoint()\n",
    "        # loop over the state action lists and compute probabilities according to eps greedy\n",
    "        probs = [1-self.epsilon if a == np.argmax(Q[s]) else self.epsilon/self.Q.shape[0] for s, a in zip(states, actions)]\n",
    "                \n",
    "        return probs\n",
    "        \n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        # find out what the max action is\n",
    "        max_index = np.argmax(self.Q[obs])\n",
    "        \n",
    "        # create equal probabilities for each action\n",
    "        probs = np.zeros(self.Q[obs].shape) + (self.epsilon/(self.Q[obs].size))\n",
    "        \n",
    "        # add (1-epsilon) to the max action\n",
    "        probs[max_index] += 1-self.epsilon\n",
    "        \n",
    "        # possible actions to choose from\n",
    "        possible_actions = np.arange(0,self.Q[obs].size)\n",
    "        \n",
    "        # sample\n",
    "        action = np.random.choice(possible_actions, p=probs)        \n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioural policy\n",
    "Random policy in blackjack lab. \n",
    "TODO: experiment with behavioural policies to check which yield interesting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehaviouralPolicy(object):\n",
    "    \"\"\"\n",
    "    A behavioural policy\n",
    "    \"\"\"\n",
    "    def __init__(self, nS, nA):\n",
    "        self.probs = np.ones((nS, nA)) * 1/nA\n",
    "        \n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains \n",
    "        a probability of perfoming action in given state for every corresponding state action pair. \n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"        \n",
    "        probs = [self.probs[s,a] for s,a in zip(states, actions)]\n",
    "        \n",
    "        return probs\n",
    "\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            state: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        p_s = self.probs[state]\n",
    "        \n",
    "        return np.random.choice(range(0,self.probs.shape[1]), p=p_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = BehaviouralPolicy(env.nS, env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function given an env and policy\n",
    "Function to sample an episode from the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n",
    "        Hint: Do not include the state after the termination in the list of states.\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    # get a starting state\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    \n",
    "    # keep looping until done, don's save the terminal state\n",
    "    while not d:\n",
    "        states.append(s)\n",
    "        a = policy.sample_action(s)\n",
    "        s, r, d, _ = env.step(a)\n",
    "        \n",
    "        # save                \n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(d)\n",
    "        \n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of episode 0: 11938\n",
      "length of episode 1: 3126\n",
      "length of episode 2: 5623\n",
      "length of episode 3: 10277\n",
      "length of episode 4: 5629\n",
      "length of episode 5: 1338\n",
      "length of episode 6: 8800\n",
      "length of episode 7: 230\n",
      "length of episode 8: 9465\n",
      "length of episode 9: 49732\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "    trajectory_data = sample_episode(env, bp)\n",
    "#     print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "    print(f\"length of episode {episode}: {len(trajectory_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: MC Ordinary Importance Sampling (make it work for windy gridworld)\n",
    "Status: copied from MC_lab, not adapted to windy gridworld.\n",
    "TODO: \n",
    "- make it work for Q values instead of V.\n",
    "- update target policy's q values to make sure it learns something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "def mc_ordinary_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and weighted importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "    V = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0, leave=False):\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # extract target and behavioral probabilities\n",
    "        target_probs = target_policy.get_probs(states, actions)\n",
    "        behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "\n",
    "        G = 0        \n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for timestep in range(len(states)-1, -1, -1):\n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            returns_count[s] += 1 \n",
    "\n",
    "            # compute the ratio using the two probability lists\n",
    "            ratio = np.prod([t/b for t, b in zip(target_probs[timestep:], behavioral_probs[timestep:])])\n",
    "\n",
    "            # use every visit incremental method\n",
    "            V[s] += 1/returns_count[s] * (ratio * G - V[s])\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = BehaviouralPolicy(env.nS, env.nA)\n",
    "gp = GreedyPolicy(Q)\n",
    "V_10k = mc_ordinary_importance_sampling(env, bp, gp, num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC: Weighted Importance Sampling\n",
    "\n",
    "##### Eventually: merge the two functions into one with a weighted flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_weighted_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a matrix defaultdict for the Q function and the sum of weights C\n",
    "    Q = defaultdict(lambda: defaultdict(float))\n",
    "    C = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    # sample episodes\n",
    "    for i in tqdm(range(num_episodes), position=0):\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        \n",
    "        # extract target and behavioral probabilities\n",
    "        target_probs = target_policy.get_probs(states, actions)\n",
    "        behavioral_probs = behavior_policy.get_probs(states, actions)\n",
    "        \n",
    "        # initialize the return and the weight\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        # loop backwards over the trajectory\n",
    "        for timestep in range(len(states)-1, -1, -1):            \n",
    "            # extract info of current timestep from trajectory    \n",
    "            s = states[timestep]\n",
    "            r = rewards[timestep]\n",
    "            a = actions[timestep]\n",
    "            G = discount_factor * G + r\n",
    "            \n",
    "            # add W to the sum of weights C\n",
    "            C[s][a] += W\n",
    "            \n",
    "            # update Q function incrementally\n",
    "            Q[s][a] += W/C[s][a] * (G - Q[s][a])\n",
    "            \n",
    "            # update the weight\n",
    "            W *= (target_probs[timestep])/(behavioral_probs[timestep])\n",
    "            \n",
    "            # break out of the loop if the weights are 0\n",
    "            if W == 0:\n",
    "                break\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 167/200 [01:26<00:16,  1.99it/s]"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((env.nS, env.nA))\n",
    "bp = BehaviouralPolicy(env.nS, env.nA)\n",
    "gp = EpsilonGreedyPolicy(Q, epsilon=0.05)\n",
    "Q_10k = mc_weighted_importance_sampling(env, bp, gp, num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{48: defaultdict(<class 'float'>, {3: -1.0, 2: -2.000642325501259, 1: -4.002854680610205, 0: -19.0}), 49: defaultdict(<class 'float'>, {3: -2.000822964998614, 1: -3.0050551499095186, 2: -4.00639147195039, 0: -4.008814595529758}), 39: defaultdict(<class 'float'>, {2: -3.003115499827479, 0: -5.00721123932285, 1: -4.00291029592987, 3: -6.003252788508598}), 29: defaultdict(<class 'float'>, {2: -4.005657483013832, 0: -6.052356753597637, 3: -7.00265352493537, 1: -5.0211249232987525}), 19: defaultdict(<class 'float'>, {2: -5.004622224704382, 1: -6.027715380599329, 0: -7.9451030142943875, 3: -9.999897973881591}), 9: defaultdict(<class 'float'>, {2: -6.014115826560658, 0: -7.323384872990588, 1: -7.4944565491904145, 3: -8.792924894080755}), 8: defaultdict(<class 'float'>, {1: -7.336405429629597, 2: -8.059080903503988, 0: -8.04298373058723, 3: -11.524115212807995}), 7: defaultdict(<class 'float'>, {1: -8.098994939035897, 3: -12.003369604903918, 2: -9.02814251420866, 0: -9.00592288370136}), 6: defaultdict(<class 'float'>, {1: -10.005696809219149, 3: -12.000511776869063, 0: -15.055085500724076, 2: -11.005714274903008}), 5: defaultdict(<class 'float'>, {1: -11.00347052254705, 2: -13.021590622742346, 3: -20.76887200709321, 0: -24.789920390650337}), 4: defaultdict(<class 'float'>, {1: -25.97548240317065, 0: -17.820235969084226, 3: -22.002849003206165, 2: -26.997030901800215}), 3: defaultdict(<class 'float'>, {1: -18.123926451614608, 3: -20.00000047521298, 0: -23.822362252416344, 2: -20.18764647863942}), 2: defaultdict(<class 'float'>, {1: -19.0102117722028, 0: -28.00000000006184, 2: -27.00000168396445, 3: -34.0}), 1: defaultdict(<class 'float'>, {1: -25.980908160635032, 0: -23.0, 3: -26.0, 2: -28.0}), 11: defaultdict(<class 'float'>, {0: -27.00000000000923, 3: -49.87045316272709, 1: -44.00001844366036, 2: -34.00000002783664}), 10: defaultdict(<class 'float'>, {1: -47.00043019723569, 3: -42.98090816061583, 2: -35.04507733359314, 0: -45.0}), 12: defaultdict(<class 'float'>, {3: -44.99999996909217, 0: -40.99995739498717, 1: -26.000002159177175, 2: -31.000143607463784}), 22: defaultdict(<class 'float'>, {0: -36.0, 2: -36.999986220176794, 3: -39.0, 1: -39.9999998616451}), 21: defaultdict(<class 'float'>, {1: -38.000016326397336, 2: -42.011143388224305, 0: -28.00000000000923, 3: -60.0}), 20: defaultdict(<class 'float'>, {1: -29.000000000028887, 0: -36.00000236097316, 2: -34.0, 3: -37.000002368549644}), 0: defaultdict(<class 'float'>, {2: -44.000000000000064, 0: -45.79166675990568, 3: -25.0, 1: -24.0}), 15: defaultdict(<class 'float'>, {1: -71.0, 0: -19.000000354518917, 3: -45.00000000126614, 2: -72.0}), 35: defaultdict(<class 'float'>, {0: -35.0, 3: -36.0, 1: -34.0, 2: -75.0}), 44: defaultdict(<class 'float'>, {1: -35.00284900285819, 0: -21.0, 2: -36.00286532924634, 3: -49.0}), 53: defaultdict(<class 'float'>, {1: -22.0, 2: -23.0, 0: -80.0, 3: -60.0}), 52: defaultdict(<class 'float'>, {1: -24.0, 0: -51.0, 2: -46.00966025710569, 3: -79.00000000000001}), 42: defaultdict(<class 'float'>, {2: -56.000007678873004, 3: -51.00001632674747, 1: -41.00000006997076, 0: -49.316563813823656}), 41: defaultdict(<class 'float'>, {1: -41.000000000000256, 2: -30.000000000000004, 3: -36.00015945112468, 0: -47.000000018315774}), 51: defaultdict(<class 'float'>, {0: -42.19194929070913, 3: -27.000016326397336, 2: -55.000000000000064, 1: -25.0}), 32: defaultdict(<class 'float'>, {2: -42.000000069970845, 3: -50.928128008345816, 1: -35.9999840719229, 0: -37.02148106274731}), 31: defaultdict(<class 'float'>, {1: -46.0, 0: -48.021483167482394, 3: -41.0000000000008, 2: -39.21759997588413}), 13: defaultdict(<class 'float'>, {1: -25.000471454630166, 0: -34.0, 3: -51.0, 2: -30.000000000019654}), 23: defaultdict(<class 'float'>, {0: -38.99994552622016, 3: -42.0, 1: -35.0, 2: -31.00003476774977}), 43: defaultdict(<class 'float'>, {0: -61.813348324555854, 2: -39.77105061018254, 3: -40.000000069990094, 1: -38.0}), 40: defaultdict(<class 'float'>, {1: -31.00000002783637, 3: -73.00284900250398, 2: -44.00000000000417, 0: -33.02148106274731}), 50: defaultdict(<class 'float'>, {0: -45.00000000000411, 3: -50.002849002849054, 2: -49.00266787307622, 1: -26.000016326397336}), 30: defaultdict(<class 'float'>, {2: -32.02148106274731, 1: -40.00000000000176, 3: -31.00286525955124, 0: -30.00000000000923}), 33: defaultdict(<class 'float'>, {0: -35.0, 2: -48.000000000001215, 3: -28.000143126075447, 1: -25.0}), 60: defaultdict(<class 'float'>, {0: -57.00000168396445, 3: -64.0, 1: -48.000000107614774, 2: -58.00000168396445}), 62: defaultdict(<class 'float'>, {0: -52.0, 2: -59.0390549998766, 3: -58.0, 1: -45.00966025710569}), 28: defaultdict(<class 'float'>, {0: -9.002849078308651, 1: -6.001738927604297, 2: -7.0, 3: -14.000093058390389}), 61: defaultdict(<class 'float'>, {3: -62.0, 0: -49.61950029390474, 1: -53.00000000577359, 2: -54.00000000577359}), 18: defaultdict(<class 'float'>, {1: -8.99980455383367, 2: -12.003318779908875, 3: -17.822362134468037, 0: -11.794785177581915}), 14: defaultdict(<class 'float'>, {3: -32.0, 2: -33.002849002849004, 1: -60.0, 0: -58.870466321243526}), 38: defaultdict(<class 'float'>, {1: -5.009190662695922, 2: -6.015013606182088, 3: -13.002238670774636, 0: -14.118867303259796}), 58: defaultdict(<class 'float'>, {0: -22.000143126075447, 1: -3.0068293586948185, 2: -3.001721123859003, 3: -2.0}), 59: defaultdict(<class 'float'>, {3: -3.0012319717638523, 0: -3.0091680765245727, 1: -4.015297051228746, 2: -4.004968089400551}), 16: defaultdict(<class 'float'>, {1: -64.0, 0: -67.0, 2: -136.0}), 25: defaultdict(<class 'float'>, {1: -65.0, 3: -103.0, 0: -35.0, 2: -36.0}), 34: defaultdict(<class 'float'>, {1: -37.0, 0: -58.0, 3: -57.000471454630166, 2: -62.0}), 54: defaultdict(<class 'float'>, {0: -59.0, 2: -142.00284935610608, 1: -67.0, 3: -160.0}), 63: defaultdict(<class 'float'>, {1: -60.0, 3: -42.0, 2: -43.002849002849004, 0: -91.0}), 24: defaultdict(<class 'float'>, {3: -67.0, 2: -77.0, 1: -20.000000354518917, 0: -76.0}), 27: defaultdict(<class 'float'>, {0: -20.0, 1: -12.0, 2: -17.0, 3: -40.0}), 26: defaultdict(<class 'float'>, {3: -33.0, 2: -137.0}), 47: defaultdict(<class 'float'>, {2: -1.0, 1: -8.0, 0: -152.0}), 69: defaultdict(<class 'float'>, {0: -5.403451574355077, 1: -4.119336185126436, 2: -5.717044078696589, 3: -3.0021469471478364}), 17: defaultdict(<class 'float'>, {1: -13.000093058390389, 0: -16.0, 2: -20.0, 3: -70.0}), 68: defaultdict(<class 'float'>, {0: -2.0009541920326943, 1: -4.0000184745678276, 2: -3.0014347142411117, 3: -3.0}), 45: defaultdict(<class 'float'>, {3: -65.0, 2: -66.0, 1: -143.0, 0: -147.0}), 36: defaultdict(<class 'float'>, {0: -142.0}), 57: defaultdict(<class 'float'>, {2: -2.0, 1: -122.0})}\n"
     ]
    }
   ],
   "source": [
    "print(dict(Q_10k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference\n",
    "\n",
    "### TO-DO: TD Ordinary Importance Sampling (make it work for gridworld)\n",
    "Copied from TD_lab. Currently on-policy, needs to be off-policy.\n",
    "\n",
    "Confused: do we need value functions instead of q-values? Do we even use importance weights in off-policy TD? Are there more off-policy TD methods besides SARSA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, policy, Q, num_episodes, discount_factor=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        Q: Q value function, numpy array Q[s,a] -> state-action value.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a numpy array Q[s,a] -> state-action value.\n",
    "        stats is a list of tuples giving the episode lengths and returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = []\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        i = 0\n",
    "        R = 0\n",
    "        \n",
    "        # initial state is 3,0 in the grid (according to source code)\n",
    "        s = env.reset()\n",
    "        a = policy.sample_action(s)\n",
    "        final_state_reached = False\n",
    "        \n",
    "        while True:\n",
    "            # new actions\n",
    "            s_prime, r, final_state, _ = env.step(a)\n",
    "            \n",
    "            # keep track of stats\n",
    "            R += r\n",
    "            i += 1    \n",
    "            \n",
    "            # sample action at state s_prime\n",
    "            a_prime = policy.sample_action(s_prime)\n",
    "\n",
    "            # update Q \n",
    "            Q[s][a] += alpha * (r + discount_factor * Q[s_prime][a_prime] - Q[s][a])    \n",
    "    \n",
    "            # update policy\n",
    "            policy.Q = Q\n",
    "            \n",
    "            # if final state, terminate loop\n",
    "            if final_state:\n",
    "                break\n",
    "        \n",
    "            # update current s and a for next iteration\n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            \n",
    "        stats.append((i, R))\n",
    "        \n",
    "    episode_lengths, episode_returns = zip(*stats)\n",
    "    return Q, (episode_lengths, episode_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO: TD Weighted Importance Sampling (same as above but weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD weighted importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
